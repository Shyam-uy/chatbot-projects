{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libs\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing tokens\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-*****l10YXaAEyABSsabT3BlbkFJ5pbn5epZ64m06ubz######\"\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading NBC file\n",
    "pdfreader = PdfReader('MACHINE LEARNING(R17A0534) - Notes.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "\n",
    "raw_text = ''\n",
    "\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artition the instance space. In this \\nsection, we see how the  probabilistic models use the idea of probability to classify new entities.  \\n \\nProbabilistic models see features and target variables as '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[20000:20200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split the text using Character Text Split such that it should not increase token size\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MACHINE LEARNING   \\n[R17A0534 ] \\nLECTURE NOTES  \\n \\nB.TECH IV YEAR – I SEM(R17 ) \\n(2020 -21) \\n \\n \\n \\n \\n \\n \\nDEPARTMENT OF \\nCOMPUTER SCIENCE AND ENGINEERING  \\nMALLA REDDY COLLEGE OF ENGINEERING & \\nTECHNOLOGY  \\n(Autonomous Institution – UGC, Govt. of India)  \\nRecognized under 2(f) and  12 (B) of UGC ACT 1956  \\n(Affiliated to JNTUH, Hyderabad, Approved by AICTE - Accredited by NBA & NAAC – ‘A’ Grade - ISO 9001:2015 Certified)  \\nMaisammaguda, Dhulapally (Post Via. Hakimpet), Secunderabad – 500100, Telangana State, India  \\n \\nIV Year B. T ech. CSE –II Sem                        L   T/P/D   C  \\n  4   1/ - / -   3  \\n(R17A0534) Machine Learning  \\nObjectives:  \\n\\uf0b7 Acquire theoretical Knowledge on setting hypothesis for pattern recognition .',\n",
       " '4   1/ - / -   3  \\n(R17A0534) Machine Learning  \\nObjectives:  \\n\\uf0b7 Acquire theoretical Knowledge on setting hypothesis for pattern recognition . \\n\\uf0b7 Apply suitable machine learning techniques for data handling and  to gain knowledge  from it . \\n\\uf0b7 Evaluate the performance of algorithms and to provide solution for various real  world \\napplications . \\n \\nUNIT I:  \\nIntroduction to Machine Learning  \\nIntroduction  ,Components of Learning , Learning Models , Geometric Models, Probabilis tic \\nModels, Logic Models, G roupin g and Grading, D esign ing a Learning System, Types of \\nLearning, Supervised, Unsupervised,  Reinforcement , Perspectives and  Issues, Version Spaces, \\nPAC Learning, VC Dimension.   \\n \\nUNIT II:  \\nSupervised and Unsupervised Learning',\n",
       " 'Learning, Supervised, Unsupervised,  Reinforcement , Perspectives and  Issues, Version Spaces, \\nPAC Learning, VC Dimension.   \\n \\nUNIT II:  \\nSupervised and Unsupervised Learning  \\nDecision Trees: ID3, Classification and Regression Trees, Regression: Linear Regression, Multiple Linear \\nRegression, Logistic Regression, Neural Networks: Introduction, Perception, Multilayer Perception, \\nSupport Vector  Machines: Linear and Non -Linear, Kernel Functions, K  Nearest Neighbors .  \\nIntroduction to clustering,  K-means clustering, K -Mode  Clustering . \\n \\nUNIT III:  \\nEnsemble and Probabilistic Learning  \\nModel Combination Schemes, Voting, Error -Correcting Output Codes, B agging: Random  Forest Trees, \\nBoosting: Adaboost, Stacking .',\n",
       " 'UNIT III:  \\nEnsemble and Probabilistic Learning  \\nModel Combination Schemes, Voting, Error -Correcting Output Codes, B agging: Random  Forest Trees, \\nBoosting: Adaboost, Stacking . \\nGaussian mixture models  - The Expectation -Maximization (EM) Algorithm , Information Criteria , Nearest \\nneighbour methods  - Nearest Neighbour Smoothing , Efficient Distance Computations: the KD -Tree, \\nDistance Measures . \\n \\n \\nUNIT IV:  \\nReinforcement Learning  and Evaluating Hypotheses  \\nIntroduction, Learning Task, Q Learning, Non deterministic Rewards and actions, temporal -difference \\nlearning, Relationship to Dynamic Programming, A ctive reinf orcement learning, Generalizat ion in \\nreinfor cement learning.',\n",
       " 'learning, Relationship to Dynamic Programming, A ctive reinf orcement learning, Generalizat ion in \\nreinfor cement learning.  \\nMotivation, Basics of Sampling Theory: Error Estimation and Estimating Binomial Proportions, The \\nBinomial Distribution, Estimators, Bias, and Variance   \\n \\n \\nUNIT V:  \\nGenetic Algorithms:  Motivation, Genetic Algorithms : Representing Hypotheses , Genetic Operator, \\nFitness Function and Selection,  An Illustrative  Example, Hypothesis Space Search, Genetic \\nProgramming, Models of Evolution and Learning : Lamarkian Evolution, Baldwin Effect , Parallelizing \\nGenetic Algorithms . \\n TEXT BOOKS:  \\n \\n1. Ethem  Alpaydin, ”Introduction to Machine Learning”, MIT Press, Prentice Hall of India, 3rd \\nEdition2014.',\n",
       " 'Genetic Algorithms . \\n TEXT BOOKS:  \\n \\n1. Ethem  Alpaydin, ”Introduction to Machine Learning”, MIT Press, Prentice Hall of India, 3rd \\nEdition2014.  \\n2. Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar ” Foundations of Machine Learning”, MIT \\nPress,2012.  \\n3. Tom Mitchell, “Machine Learning”, McGraw Hill, 3rdEdition, 1997.  \\n4. MACHINE LEARNING - An Algorithmic Perspective, Second Edition , Stephen Marsland , 2015.  \\n \\nREFERENCE BOOKS:  \\n1. CharuC.Aggarwal,“DataClassificationAlgorithmsandApplications”,CRCPress,2014.  \\n2. Charu C. Aggarwal, “DATA CLUSTERING Algorithms and Applications”, CRC Press,  \\n       2014.  \\n3. Kevin P. Murphy ”Machine Learning: A Probabilistic Perspective”, The MIT Press, 2012  \\n4. Jiawei Han and  Micheline Kambers and JianPei, “Data Mining Concepts',\n",
       " '2014.  \\n3. Kevin P. Murphy ”Machine Learning: A Probabilistic Perspective”, The MIT Press, 2012  \\n4. Jiawei Han and  Micheline Kambers and JianPei, “Data Mining Concepts  \\n      andTechniques”,3rd edition, Morgan Kaufman Publications, 2012.  \\n \\n \\nOUTCOMES:  \\n1. Recognize the characteristics of Machine Learning techniques that enable to solve real world \\nproblems  \\n2. Recognize the char acteristics of machine learning strategies  \\n3. Apply various supervised learning methods to appropriate problems  \\n4. Identify and integrate more than one techniques to enhance the performance of learning  \\n5. Create probabilistic and unsupervised learning mode ls for handling unknown pattern  \\n6. Analyze the co -occurrence of data to find interesting frequent patterns',\n",
       " '5. Create probabilistic and unsupervised learning mode ls for handling unknown pattern  \\n6. Analyze the co -occurrence of data to find interesting frequent patterns  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n INDEX  \\nUNIT NO  TOPIC  PAGE NO  \\nI Introduction  1 \\nLearning Models  3 \\nDesigning a Learning System  7 \\nTypes of Learning  12 \\nPerspectives and Issues  13 \\nVersion Spaces  14 \\nPAC Learning  19 \\nVC Dimension  21 \\nII Decision Trees  23 \\nClassification and Regression Trees  27 \\nNeural Networks  37 \\nSupport Vector Machines  45 \\nIntroduction to clustering  49 \\nK-means clustering  52 \\nIII Model Combination Schemes  55 \\nVoting, Error -Correcting Output Codes  57 \\nBagging , Random Fo rest Trees  61 \\nBoosting , Adaboost  65 \\nGaussian mixture models  68 \\nEM Algorithms  69',\n",
       " 'III Model Combination Schemes  55 \\nVoting, Error -Correcting Output Codes  57 \\nBagging , Random Fo rest Trees  61 \\nBoosting , Adaboost  65 \\nGaussian mixture models  68 \\nEM Algorithms  69 \\nEfficient Distance Computations  73 IV Reinforcement Learning  78 \\nLearning Task  79 \\nQ Learning  82 \\nEvaluating Hypotheses  86 \\nBasics of Sampling Theory  88 \\nV Genetic Algorithms  92 \\nAn Illustrative Example  96 \\nHypothesis Space Search  98 \\nGenetic Programming  101 \\nModels of Evolution and Learning  104 \\nParallelizing Genetic Algorithms . 105 \\n 1 \\n UNIT I  \\nIntroduction to Machine Learning  \\n1. Introduction  \\n \\n1.1 What Is Machine Learning?  \\nMachine learning is programming computers to optimize a performance criterion using example',\n",
       " '1 \\n UNIT I  \\nIntroduction to Machine Learning  \\n1. Introduction  \\n \\n1.1 What Is Machine Learning?  \\nMachine learning is programming computers to optimize a performance criterion using example \\ndata or past experience. We have a model defined up to some parameters, and learning is the \\nexecution of a computer program to optimize the parameters of the model using the training data or \\npast experience. The model may be predictive to make predictions in the future, or descriptive to gain \\nknowledge from data, or both.  \\nArthur Samuel, an early American leader in the field of computer gaming and artificial intellige nce, \\ncoined the term “Machine Learning” in 1959 while at IBM. He defined machine learning as “the field of',\n",
       " 'coined the term “Machine Learning” in 1959 while at IBM. He defined machine learning as “the field of \\nstudy that gives computers the ability to learn without being explicitly programmed.” However, there is \\nno universally accepted definition for machin e learning. Different authors define the term differently.  \\n \\nDefinition of learning  \\nDefinition  \\nA computer program is said to learn from experience E with respect to some class of tasks T and \\nperformance measure P, if its performance at tasks T, as measured by P, improves with experience E.  \\n \\nExamples  \\ni) Handwriting recognition learning problem  \\n• Task T: Recognising and classifying handwritten words within images  \\n• Performance P: Percent of words correctly classified',\n",
       " 'Examples  \\ni) Handwriting recognition learning problem  \\n• Task T: Recognising and classifying handwritten words within images  \\n• Performance P: Percent of words correctly classified  \\n• Training experience E: A dataset of handw ritten words with given classifications  \\nii) A robot driving learning problem  \\n• Task T: Driving on highways using vision sensors  \\n• Performance measure P: Average distance traveled before an error  \\n• training  experience: A sequence of images and steering commands recorded while  \\n  observing a human driver  \\niii) A chess learning problem  \\n• Task T: Playing chess  \\n• Performance measure P: Percent of games won against opponents  \\n• Training experience E: Playing practi ce games against itself  \\nDefinition',\n",
       " '• Task T: Playing chess  \\n• Performance measure P: Percent of games won against opponents  \\n• Training experience E: Playing practi ce games against itself  \\nDefinition  \\nA computer program which learns from experience is called a machine learning program or \\nsimply a learning program. Such a program is sometimes also referred to as a learner.  \\n \\n1.2 Components of Learning  \\n Basic components o f learning process  \\nThe learning process, whether by a human or a machine, can be divided into four components, \\nnamely, data storage, abstraction, generalization and evaluation. Figure 1.1 illustrates the \\nvariouscomponents and the steps involved in the lear ning process.  \\n 2 \\n  \\n1. Data storage  \\nFacilities for storing and retrieving huge amounts of data are an important component of the',\n",
       " 'variouscomponents and the steps involved in the lear ning process.  \\n 2 \\n  \\n1. Data storage  \\nFacilities for storing and retrieving huge amounts of data are an important component of the \\nlearning process. Humans and computers alike utilize data storage as a foundation for advanced \\nreasoning.  \\n• In a human being, the data is stored in the brain and data is retrieved using electrochemical    signals.  \\n• Computers use hard disk drives, flash memory, random access memory and similar devices    to store \\ndata and use cables and other technology to retrieve data.  \\n \\n2. Abstract ion \\nThe second component of the learning process is known as abstraction.  \\nAbstraction is the process of extracting knowledge about stored data. This involves creating general',\n",
       " '2. Abstract ion \\nThe second component of the learning process is known as abstraction.  \\nAbstraction is the process of extracting knowledge about stored data. This involves creating general \\nconcepts about the data as a whole. The creation of knowledge involves application of known models \\nand creation of new models.  \\nThe process of fitting a model to a dataset is known as training. When the model has been trained, the \\ndata is transformed into an abstract form that summarizes the original information.  \\n \\n3. Generaliz ation  \\nThe third component of the learning process is known as generalisation.  \\nThe term generalization describes the process of turning the knowledge about stored data into a form',\n",
       " '3. Generaliz ation  \\nThe third component of the learning process is known as generalisation.  \\nThe term generalization describes the process of turning the knowledge about stored data into a form \\nthat can be utilized for future action. These actions are to be carried out o n tasks that are similar, but \\nnot identical, to those what have been seen before. In generalization, the goal is to discover those \\nproperties of the data that will be most relevant to future tasks.  \\n \\n4. Evaluation  \\nEvaluation is the last component of the lea rning process.  \\nIt is the process of giving feedback to the user to measure the utility of the learned knowledge. This \\nfeedback is then utilised to effect improvements in the whole learning process  \\n \\nApplications of machine learning',\n",
       " 'feedback is then utilised to effect improvements in the whole learning process  \\n \\nApplications of machine learning  \\nApplication of machine le arning methods to large databases is called data mining. In data \\nmining, a large volume of data is processed to construct a simple model with valuable use, for example, \\nhaving  \\nhigh predictive accuracy.  \\n \\nThe following is a list of some of the typical applic ations of machine learning.  \\n1. In retail business, machine learning is used to study consumer behaviour.  \\n2. In finance, banks analyze their past data to build models to use in credit applications, fraud \\ndetection, and the stock market.  \\n3. In manufacturing, learning  models are used for optimization, control, and troubleshooting.  \\n3',\n",
       " 'detection, and the stock market.  \\n3. In manufacturing, learning  models are used for optimization, control, and troubleshooting.  \\n3 \\n 4. In medicine, learning programs are used for medical diagnosis.  \\n5. In telecommunications, call patterns are analyzed for network optimization and maximizing the \\nquality of service.  \\n6. In science, la rge amounts of data in physics, astronomy, and biology can only be analyzed fast \\nenough by computers. The World Wide Web is huge; it is constantly growing and searching for \\nrelevant information cannot be done manually.  \\n7. In artificial intelligence, it is use d to teach a system to learn and adapt to changes so that the \\nsystem designer need not foresee and provide solutions for all possible situations.',\n",
       " '7. In artificial intelligence, it is use d to teach a system to learn and adapt to changes so that the \\nsystem designer need not foresee and provide solutions for all possible situations.  \\n8. It is used to find solutions to many problems in vision, speech recognition, and robotics.  \\n9. Machine learning methods are applied in the design of computer -controlled vehicles to steer \\ncorrectly when driving on a variety of roads.  \\n10. Machine learning methods have been used to develop programmes for playing games such as \\nchess, backgammon and Go.  \\n \\n1.3 Learning Models  \\nMachine learning is concerned with using the right features to build the right models that \\nachieve the right tasks.  The basic idea of Learning models has divided into three categories.',\n",
       " 'Machine learning is concerned with using the right features to build the right models that \\nachieve the right tasks.  The basic idea of Learning models has divided into three categories.  \\nFor a given problem, the collection of all possible outcomes represent s the  sample space or instance \\nspace . \\n \\n\\uf0b7 Using a Logical expression. ( Logical models ) \\n\\uf0b7 Using the Geometry of the instance space. ( Geometric models)  \\n\\uf0b7 Using Probability to classify the instance space. ( Probabilistic models ) \\n\\uf0b7 Grouping and Grading  \\n \\n1.3.1  Logical  models  \\nLogical models  use a logical expression to divide the instance space into segments and hence \\nconstruct grouping models. A  logical expression  is an expression that returns a Boolean value, i.e., a',\n",
       " 'Logical models  use a logical expression to divide the instance space into segments and hence \\nconstruct grouping models. A  logical expression  is an expression that returns a Boolean value, i.e., a \\nTrue or False outcome. Once the data is grouped usin g a logical expression, the data is divided into \\nhomogeneous groupings for the problem we are trying to solve.   For example, for a classiﬁcation \\nproblem, all the instances in the group belong to one class.  \\n \\nThere are mainly two kinds of logical models:  Tree models  and Rule models . \\n \\nRule models consist of a collection of implications or IF -THEN rules. For tree -based models, the ‘if -part’ \\ndeﬁnes a segment and the ‘then -part’ deﬁnes the behaviour of the model for this segment. Rule models \\nfollow the same reaso ning.',\n",
       " 'deﬁnes a segment and the ‘then -part’ deﬁnes the behaviour of the model for this segment. Rule models \\nfollow the same reaso ning.  \\n \\nLogical models and Concept learning  \\nTo understand logical models further, we need to understand the idea of  Concept Learning . \\nConcept Learning involves learning logical expressions or concepts from examples. The idea of Concept \\nLearning fits in well  with the idea of Machine learning, i.e., inferring a general function from specific \\ntraining examples. Concept learning forms the basis of both tree -based and rule -based models.   More \\nformally, Concept Learning involves acquiring the definition of a gener al category from a given set of \\npositive and negative training examples of the category. A Formal Definition for Concept Learning is',\n",
       " 'positive and negative training examples of the category. A Formal Definition for Concept Learning is \\n“The inferring of a Boolean -valued function from training examples of its input and output.”  In \\nconcept learning, we only l earn a description for the positive class and label everything that doesn’t \\nsatisfy that description as negative.  \\n 4 \\n  \\n \\n \\n \\n \\n \\n \\nThe following  example  explains this idea in more detail.  \\n \\n \\n \\nA Concept Learning  Task called “Enjoy Sport” as shown above is defined by a set of data from \\nsome example days. Each data is described by six attributes. The task is to learn to predict the value of \\nEnjoy Sport for an arbitrary day based on the values of its attribute values. The problem can be',\n",
       " 'Enjoy Sport for an arbitrary day based on the values of its attribute values. The problem can be \\nrepresented by a  series of hypotheses . Each hypothesis is described by a conjunction of constraints on \\nthe attribu tes. The training data represents a set of positive and negative examples of the target \\nfunction. In the example above, each hypothesis is a vector of six constraints, specifying the values of \\nthe six attributes –  Sky, AirTemp, Humidity, Wind, Water, and Forecast. The training phase involves \\nlearning the set of days (as a conjunction of attributes) for which Enjoy Sport = yes.  \\n \\nThus, the problem can be formulated as:  \\n \\n\\uf0b7 Given instances X   which represent a set of all possible days, each described by the attr ibutes:  \\no Sky – (values: Sunny, Cloudy, Rainy),',\n",
       " 'Thus, the problem can be formulated as:  \\n \\n\\uf0b7 Given instances X   which represent a set of all possible days, each described by the attr ibutes:  \\no Sky – (values: Sunny, Cloudy, Rainy),  \\no AirTemp – (values: Warm, Cold),  \\no Humidity – (values: Normal, High),  \\no Wind – (values: Strong, Weak),  \\no Water – (values: Warm, Cold),  \\no Forecast – (values: Same, Change).  \\n \\nTry to identify a function that can predict th e target variable Enjoy Sport as yes/no, i.e., 1 or 0.  \\n \\n1.3.2 Geometric models  \\nIn the previous section, we have seen that with logical models, such as decision trees, a logical \\nexpression is used to partition the instance space. Two instances are similar when they end up in the',\n",
       " 'expression is used to partition the instance space. Two instances are similar when they end up in the \\nsame logical segment. In this section, we consider models that define similarity by considering the \\ngeometry of the instance space.   In Geometric models, features could be described as points in two \\ndimensions ( x- and y-axis) or a three -dimensional space ( x, y, and z). Even when features are not \\n5 \\n intrinsically geometric, they could be modelled in a geometric manner (for example, temperature as a \\nfunction of time can be modelled in two axes). In geometric models, there are two ways we  could \\nimpose similarity.  \\n\\uf0b7 We could use geometric concepts like  lines or planes to segment (classify)  the instance space. \\nThese are called  Linear models .',\n",
       " 'impose similarity.  \\n\\uf0b7 We could use geometric concepts like  lines or planes to segment (classify)  the instance space. \\nThese are called  Linear models . \\n\\uf0b7 Alternatively, we can use the geometric notion of distance to represent similarity. In this case, if \\ntwo points are close together, they have similar values for features and thus can be classed as \\nsimilar. We call such models as  Distance -based models . \\n \\n \\nLinear models  \\nLinear models are relatively simple. In this case, the function is represented as a linear \\ncombination of its inputs. Thus, if  x1 and x2 are two scalars or vectors of the same dimension \\nand a and b are arbitrary scalars, then  ax1 + bx2 represents a linear combination of  x1 and x2. In the',\n",
       " 'and a and b are arbitrary scalars, then  ax1 + bx2 represents a linear combination of  x1 and x2. In the \\nsimplest case where  f(x) represents a straight line, we ha ve an equation of the form  f (x) \\n= mx + c where  c represents the intercept and  m represents the slope.  \\n \\n \\nLinear models are  parametric , which means that they have a ﬁxed  form with a small number of numeric \\nparameters that need to be learned from data. For example, in  f (x) = mx + c, m and c are the \\nparameters that we are trying to learn from the data. This technique is different from tree or rule \\nmodels, where the structu re of the model (e.g., which features to use in the tree, and where) is  not \\nﬁxed in advance .',\n",
       " 'models, where the structu re of the model (e.g., which features to use in the tree, and where) is  not \\nﬁxed in advance . \\n \\nLinear models are  stable , i.e., small variations in the t raining data have only a limited impact on the \\nlearned model. In contrast,  tree models tend to vary more with the training data , as the choice of a \\ndifferent split at the root of the tree typically means that the rest of the tree is different as well.   As a \\nresult of having relatively few parameters, Linear models have  low variance and high bias . This implies \\nthat Linear models are less likely to overfit  the training data  than some other models. However, they \\nare more likely to underfit. For example, if we want to learn the boundaries between countries based',\n",
       " 'are more likely to underfit. For example, if we want to learn the boundaries between countries based \\non labelled data, then linear models are not likely to give a good approximation.  \\n \\nDistance -based models  \\nDistance -based models  are the second class of Geometric models. Like Linear models, distance -\\nbased  models are based on the geometry of data. As the name implies, distance -based models work on \\nthe concept of distance.   In the context of Machine learning, the concept of distance is not based on \\nmerely the physical distance between two points. Instead, we  could think of the distance between two \\npoints considering the  mode of transport  between two points. Travelling between two cities by plane \\n6',\n",
       " 'points considering the  mode of transport  between two points. Travelling between two cities by plane \\n6 \\n covers less distance physically than by train because a plane is unrestricted. Similarly, in chess, the \\nconcept of distance depends on the piece used – for example, a Bishop can move diagonally.    Thus, \\ndepending on the entity and the mode of travel, the concept of distance can be experienced differently. \\nThe distance metrics commonly used are  Euclidean , Minkowski , Man hattan , and  Mahalanobis . \\n \\n \\nDistance is applied through the concept of  neighbours and exemplars . Neighbours are points in \\nproximity with respect to the distance measure expressed through exemplars. Exemplars are',\n",
       " 'Distance is applied through the concept of  neighbours and exemplars . Neighbours are points in \\nproximity with respect to the distance measure expressed through exemplars. Exemplars are \\neither  centroids  that ﬁnd  a centre of mass according to a chosen distance metric or  medoids  that ﬁnd \\nthe most centrally located data point. The most commonly used centroid is the arithmetic mean, which \\nminimises squared Euclidean distance to all other points.  \\n \\nNotes:  \\n\\uf0b7 The centroid  represents the geometric centre of a plane figure, i.e., the arithmetic mean \\nposition of all the points in the figure from the centroid point. This definition extends to any \\nobject in  n-dimensional space: its centroid is the mean position of all the points .',\n",
       " 'position of all the points in the figure from the centroid point. This definition extends to any \\nobject in  n-dimensional space: its centroid is the mean position of all the points . \\n\\uf0b7 Medoids  are similar in concept to means or centroids. Medoids are most commonly used on \\ndata when a mean or centroid cannot be defined. They are used in contexts where the centroid \\nis not representative of the dataset, such as in image data.  \\n \\nExamples of  distance -based models include the  nearest -neighbour  models, which use the training data \\nas exemplars – for example, in classification. The  K-means clustering  algorithm also uses exemplars to \\ncreate clusters of similar data points.  \\n \\n1.3.3 Probabilistic mod els',\n",
       " 'as exemplars – for example, in classification. The  K-means clustering  algorithm also uses exemplars to \\ncreate clusters of similar data points.  \\n \\n1.3.3 Probabilistic mod els \\nThe third family of machine learning algorithms is the probabilistic models. We have seen \\nbefore that the k -nearest neighbour algorithm uses the idea of distance (e.g., Euclidian distance) to \\nclassify entities, and logical models use a logical expressi on to partition the instance space. In this \\nsection, we see how the  probabilistic models use the idea of probability to classify new entities.  \\n \\nProbabilistic models see features and target variables as random variables. The process of modelling \\nrepresents and manipulates the level of uncertainty  with respect to these variables. There are two',\n",
       " 'represents and manipulates the level of uncertainty  with respect to these variables. There are two \\ntypes of probabilistic models:  Predictive and Generative . Predictive probability models use the idea of \\na conditional probability  distribution  P (Y |X) from which  Y can be predicted from  X.  Generative models \\nestimate the  joint distribution  P (Y, X).  Once we know the joint distribution for the generative models, \\nwe can derive any conditional or marginal distribution involving the same variables. Thus, the \\ngenerative model is capable of creating new data points and their labels, knowing the joint probability \\ndistribution. The joint distribution looks for a relationship between two variables. Once this relationship \\nis inferred, it is possible to infer new data points.',\n",
       " 'distribution. The joint distribution looks for a relationship between two variables. Once this relationship \\nis inferred, it is possible to infer new data points.  \\nNaïve Bayes  is an example of a probabilistic classifier.  \\n \\nWe can do this using the  Bayes rule  defined as  \\n7 \\n  \\n \\n \\nThe Naïve Bayes algorithm is based on the idea of  Conditional Probability.   Conditional probability is \\nbased on finding the  probability that somethi ng will happen,  given that something else  has already \\nhappened. The task of the algorithm then is to look at the evidence and to determine the likelihood of a \\nspecific class and assign a label accordingly to each entity.  \\n \\nSome broad categories of models:  \\nGeometric models  Probabilistic models  Logical models  \\nE.g. K -nearest neighbors, linear',\n",
       " 'specific class and assign a label accordingly to each entity.  \\n \\nSome broad categories of models:  \\nGeometric models  Probabilistic models  Logical models  \\nE.g. K -nearest neighbors, linear \\nregression, support vector \\nmachine, logistic regression, …  Naïve Bayes, Gaussian process \\nregression, conditional random \\nfield, …  Decision tree, random forest, …  \\n \\n1.3.4 Grouping and Grading  \\nGrading vs grouping is an orthogonal categorization to geometric -probabilistic -logical -compositional.  \\n\\uf0b7 Grouping models break the instance space up into groups or segments and in each segment \\napply a very simple method (such as majority  class).  \\no E.g. decision tree, KNN.  \\n\\uf0b7 Grading models form one global model over the instance space.  \\no E.g. Linear classifiers – Neural networks',\n",
       " 'apply a very simple method (such as majority  class).  \\no E.g. decision tree, KNN.  \\n\\uf0b7 Grading models form one global model over the instance space.  \\no E.g. Linear classifiers – Neural networks  \\n1.4 Designing a Learning System  \\nFor any learning system, we must be knowing the three elements — T (Task) , P (Performance \\nMeasure) , and  E (Training Experience) . At a high level, the process of learning system looks as below.  \\n \\nThe learning process starts with task T, performance measure P and training experience E and objective \\nare to find an unknown target function. The target function is an exact knowledge to be learned from the \\ntraining experience and its unknown. For example , in a case of credit approval, the learning system will',\n",
       " 'training experience and its unknown. For example , in a case of credit approval, the learning system will \\nhave customer application records as experience and task would be to classify whether the given \\ncustomer application is eligible for a loan. So in this case, the training examples can be represented as \\n8 \\n (x1,y1)(x2,y2)..(xn,yn) where X represents customer application details and y represents the status of \\ncredit approval.  \\nWith these details, what is that exact knowledge to be learned from the training experience?  \\nSo the target function to be learned in the credit approval learning system is a mapping function f:X →y. \\nThis function represents the exact knowledge defining the relationship between input variable X and \\noutput variable y.  \\nDesign of a learning system',\n",
       " 'This function represents the exact knowledge defining the relationship between input variable X and \\noutput variable y.  \\nDesign of a learning system  \\nJust now we looked into the learning proce ss and also understood the goal of the learning. When we \\nwant to design a learning system that follows the learning process, we need to consider a few design \\nchoices. The design choices will be to decide the following key components:  \\n1. Type of training exper ience  \\n2. Choosing the Target Function  \\n3. Choosing a representation for the Target Function  \\n4. Choosing an approximation algorithm for the Target Function  \\n5. The final Design  \\n \\nWe will look into the game - checkers learning problem and apply the above design choices. For a \\ncheckers learning problem, the three elements will be,',\n",
       " \"5. The final Design  \\n \\nWe will look into the game - checkers learning problem and apply the above design choices. For a \\ncheckers learning problem, the three elements will be,  \\n \\n1. Task T: To play checkers  \\n2. Performance measure P: Total percent of the game won in the tournament.  \\n3. Training experience E: A set of games played against itself  \\n \\n1.4.1 Type of training experience  \\nDuring the design of the checker's learning system, the type of training experience available for a \\nlearning system will have a significant effect on the success or failure of the learning.  \\n \\n1. Direct or Indirect training experience — In the case of direct training experience, an individual board \\nstates and correct move for each board state are given.\",\n",
       " '1. Direct or Indirect training experience — In the case of direct training experience, an individual board \\nstates and correct move for each board state are given.  \\nIn case of indirect training experience, the move sequences for a game and the final result (win, loss \\nor draw) are given for a number of games. How to assign credit or blame to individual moves is the \\ncredit assignment problem.  \\n2. Teacher or Not — Supervised — The training experience will be labeled, which means, all the board \\nstates will be labeled with the correct move. So the learning takes place in t he presence of a \\nsupervisor or a teacher.  \\nUnsupervised — The training experience will be unlabeled, which means, all the board states will not',\n",
       " 'supervisor or a teacher.  \\nUnsupervised — The training experience will be unlabeled, which means, all the board states will not \\nhave the moves. So the learner generates random games and plays against itself with no supervision \\nor teacher inv olvement.  9 \\n Semi -supervised — Learner generates game states and asks the teacher for help in finding the \\ncorrect move if the board state is confusing.  \\n3. Is the training experience good — Do the training examples represent the distribution of examples \\nover whic h the final system performance will be measured? Performance is best when training \\nexamples and test examples are from the same/a similar distribution.  \\n \\nThe checker player learns by playing against oneself. Its experience is indirect. It may not encounter',\n",
       " 'examples and test examples are from the same/a similar distribution.  \\n \\nThe checker player learns by playing against oneself. Its experience is indirect. It may not encounter \\nmoves that are common in human expert play. Once the proper training experience is available, the next \\ndesign step will be choosing the Target Function.  \\n \\n1.4.2 Choosing the Target Function  \\nWhen you are playing the checkers game, at any moment of time, you make a decision on \\nchoosing the best move from different possibilities. You think and apply the learning that you have \\ngained from the experience. Here the learning is, for a specific board, you move a checker such that your \\nboard state tends towards the w inning situation. Now the same learning has to be defined in terms of \\nthe target function.',\n",
       " 'board state tends towards the w inning situation. Now the same learning has to be defined in terms of \\nthe target function.  \\n \\nHere there are 2 considerations — direct and indirect experience.  \\n \\n\\uf0b7 During the direct experience , the checkers learning system, it needs only to learn how to choose \\nthe best move among some large search space. We need to find a target function that will help \\nus choose the best move among alternatives. Let us call this function ChooseMove and use the \\nnotation  ChooseMove : B →M  to indicate that this function accepts as input any board from the \\nset of legal board states B and produces as output some move from the set of legal moves M.  \\n\\uf0b7 When there is an indirect experience , it becomes difficult to learn su ch function. How about',\n",
       " 'set of legal board states B and produces as output some move from the set of legal moves M.  \\n\\uf0b7 When there is an indirect experience , it becomes difficult to learn su ch function. How about \\nassigning a real score to the board state.  \\n \\nSo the function be  V : B →R  indicating that this accepts as input any board from the set of legal board \\nstates B and produces an output a real score. This function assigns the higher score s to better board \\nstates.  \\n \\n \\nIf the system can successfully learn such a target function V, then it can easily use it to select the best \\nmove from any board position.  \\n10 \\n Let us therefore define the target value V(b) for an arbitrary board state b in B, as follows:  \\n1. if b is a final board state that is won, then V(b) = 100',\n",
       " 'move from any board position.  \\n10 \\n Let us therefore define the target value V(b) for an arbitrary board state b in B, as follows:  \\n1. if b is a final board state that is won, then V(b) = 100  \\n2. if b is a final board state that is lost, then V(b) = -100 \\n3. if b is a final board state that is drawn, then V(b) = 0  \\n4. if b is a not a final state in the game, then V (b) = V (b’), where b’ is the best final board state that can \\nbe achieved starting from b and playing optimally until the end of the game.  \\n \\nThe (4) is a recursive definition and to determine the value of V(b) for a particular board state, it \\nperforms the search ahead fo r the optimal line of play, all the way to the end of the game. So this',\n",
       " 'performs the search ahead fo r the optimal line of play, all the way to the end of the game. So this \\ndefinition is not efficiently computable by our checkers playing program, we say that it is a \\nnonoperational definition.  \\n \\nThe goal of learning, in this case, is to discover an operational description of V ; that is, a description \\nthat can be used by the checkers -playing program to evaluate states and select moves within realistic \\ntime bounds.  \\nIt may be very difficult in general to learn such an operational form of V perfectly. W e expect learning \\nalgorithms to acquire only some approximation to the target function ^V.  \\n \\n1.4.3 Choosing a representation for the Target Function  \\nNow that we have specified the ideal target function V, we must choose a representation that',\n",
       " '1.4.3 Choosing a representation for the Target Function  \\nNow that we have specified the ideal target function V, we must choose a representation that \\nthe learning pr ogram will use to describe the function ^V that it will learn. As with earlier design \\nchoices, we again have many options. We could, for example, allow the program to represent using a \\nlarge table with a distinct entry specifying the value for each distinc t board state. Or we could allow it to \\nrepresent using a collection of rules that match against features of the board state, or a quadratic \\npolynomial function of predefined board features, or an artificial  \\nneural network. In general, this choice of repres entation involves a crucial tradeoff. On one hand, we',\n",
       " 'polynomial function of predefined board features, or an artificial  \\nneural network. In general, this choice of repres entation involves a crucial tradeoff. On one hand, we \\nwish to pick a very expressive representation to allow representing as close an approximation as \\npossible to the ideal target function V.  \\n \\nOn the other hand, the more expressive the representation, the  more training data the program \\nwill require in order to choose among the alternative hypotheses it can represent. To keep the \\ndiscussion brief, let us choose a simple representation:  \\nfor any given board state, the function ^V will be calculated as a linear combination of the following \\nboard features:  \\n\\uf0b7 x1(b) — number of black pieces on board b  \\n\\uf0b7 x2(b) — number of red pieces on b',\n",
       " 'board features:  \\n\\uf0b7 x1(b) — number of black pieces on board b  \\n\\uf0b7 x2(b) — number of red pieces on b  \\n\\uf0b7 x3(b) — number of black kings on b  \\n\\uf0b7 x4(b) — number of red kings on b  \\n\\uf0b7 x5(b) — number of red pieces threat ened by black (i.e., which can be taken on black’s next turn)  \\n\\uf0b7 x6(b) — number of black pieces threatened by red  \\n \\n^V = w0 + w1 · x1(b) + w2 · x2(b) + w3 · x3(b) + w4 · x4(b) +w5 · x5(b) + w6 · x6(b)  \\n \\nWhere w0 through w6 are numerical coefficients or weights to be obtained by a learning algorithm.  \\nWeights w1 to w6 will determine the relative importance of different board features.  \\n 11 \\n Specification of the Machine Learning Problem at this time — Till now we worked on choosing the type',\n",
       " 'Weights w1 to w6 will determine the relative importance of different board features.  \\n 11 \\n Specification of the Machine Learning Problem at this time — Till now we worked on choosing the type \\nof training experience, choo sing the target function and its representation. The checkers learning task \\ncan be summarized as below.  \\n\\uf0b7 Task T : Play Checkers  \\n\\uf0b7 Performance Measure : % of games won in world tournament  \\n\\uf0b7 Training Experience E : opportunity to play against itself  \\n\\uf0b7 Target Functi on : V : Board → R  \\n\\uf0b7 Target Function Representation : ^V = w0 + w1 · x1(b) + w2 · x2(b) + w3 · x3(b) + w4 · x4(b) +w5 · \\nx5(b) + w6 · x6(b)  \\nThe first three items above correspond to the specification of the learning task,whereas  the final two',\n",
       " 'x5(b) + w6 · x6(b)  \\nThe first three items above correspond to the specification of the learning task,whereas  the final two \\nitems constitute design choices for the implementation of the learning program.  \\n \\n1.4.4 Choosing an approximation algorithm for the Target Function  \\nGenerating training data — \\nTo train our learning program, we need a set of training data, each  describing a specific board state b and \\nthe training value V_train (b) for b. Each training example is an ordered pair <b,V_train(b)>  \\nFor example, a training example may be <(x1 = 3, x2 = 0, x3 = 1, x4 = 0, x5 = 0, x6 = 0), +100\">. This is an \\nexample wher e black has won the game since x2 = 0 or red has no remaining pieces. However, such clean',\n",
       " 'example wher e black has won the game since x2 = 0 or red has no remaining pieces. However, such clean \\nvalues of V_train (b) can be obtained only for board value b that are clear win, loss or draw.  \\nIn above case, assigning a training value V_train (b) for the specific boards b that are clean win, loss or \\ndraw is direct as they are direct training experience. But in the case of indirect training experience, \\nassigning a training value V_train(b) for the intermediate boards is difficult. In such case, the training \\nvalues are updated using temporal difference learning.  Temporal difference (TD) learning is a concept \\ncentral to reinforcement learning, in which learning happens through the iterative correction of your \\nestimated returns towards a more accura te target return.',\n",
       " 'central to reinforcement learning, in which learning happens through the iterative correction of your \\nestimated returns towards a more accura te target return.  \\nLet Successor(b) denotes the next board state following b for which it is again the program’s turn to \\nmove. ^V is the learner’s current approximation to V. Using these information, assign the training value \\nof V_train(b) for any intermedi ate board state b as below :  \\nV_train(b) ← ^V(Successor(b))  \\n \\nAdjusting the weights  \\nNow its  time to define the learning algorithm for choosing the weights and best fit the set of \\ntraining examples. One common approach is to define the best hypothesis as that which minimizes the \\nsquared error E between the training values and the values predicted  by the hypothesis ^V.',\n",
       " 'training examples. One common approach is to define the best hypothesis as that which minimizes the \\nsquared error E between the training values and the values predicted  by the hypothesis ^V.  \\n \\n \\nThe learning algorithm should incrementally refine weights as more training examples become available \\nand it needs to be robust to errors in training data Least Mean Square (LMS) training rule is the one \\ntraining algorithm that wi ll adjust weights a small amount in the direction that reduces the error.  \\n \\nThe LMS algorithm is defined as follows:  \\n \\n12 \\n  \\n \\n1.4.5 Final Design for Checkers Learning system  \\nThe final design of our checkers learning system can be naturally described by four distinct \\nprogram modules that represent the central components in many learning systems.',\n",
       " 'The final design of our checkers learning system can be naturally described by four distinct \\nprogram modules that represent the central components in many learning systems.  \\n1. The performance System — Takes a new board as input and outputs a trace of the game it played \\nagainst itself.  \\n2. The Critic — Takes the trace of a game as an input and ou tputs a set of training examples of the \\ntarget function.  \\n3. The Generalizer — Takes training examples as input and outputs a hypothesis that estimates the \\ntarget function. Good generalization to new cases is crucial.  \\n4. The Experiment Generator — Takes the curre nt hypothesis (currently learned function) as input and \\noutputs a new problem (an initial board state) for the performance system to explore.',\n",
       " '4. The Experiment Generator — Takes the curre nt hypothesis (currently learned function) as input and \\noutputs a new problem (an initial board state) for the performance system to explore.  \\n \\n \\nFinal design of the checkers learning program.  \\n \\n1.5 Types of Learning  \\nIn general, machine learning algorithms can be classified into three types.  \\n\\uf0b7 Supervised learning  \\n\\uf0b7 Unsupervised learning  \\n\\uf0b7 Reinforcement learning  \\n \\n1.5.1 Supervised learning  \\nA training set of examples with the correct responses (targets) is provided and, based on this \\ntraining set, the algorithm generalises to respond correctly to all possible inputs. This is also called \\nlearning from exemplars. Supervised learning is the machine learning task of learning a function that',\n",
       " 'learning from exemplars. Supervised learning is the machine learning task of learning a function that \\nmaps an input to an output based on example input -output pairs.  \\n \\nIn supervise d learning, each example in the training set is a pair consisting of an input object \\n(typically a vector) and an output value. A supervised learning algorithm analyzes the training data and \\nproduces a function, which can be used for mapping new examples. I n the optimal case, the function \\nwill correctly determine the class labels for unseen instances. Both classification and regression \\n13 \\n problems are supervised learning problems. A wide range of supervised learning algorithms are \\navailable, each with its stren gths and weaknesses. There is no single learning algorithm that works best',\n",
       " 'available, each with its stren gths and weaknesses. There is no single learning algorithm that works best \\non all supervised learning problems.  \\n \\n \\nFigure 1.4: Supervised learning  \\n \\n \\n \\nRemarks  \\nA “supervised learning” is so called because the process of an algorithm  learning from the \\ntraining dataset can be thought of as a teacher supervising the learning process. We know the correct \\nanswers (that is, the correct outputs), the algorithm iteratively makes predictions on the training data \\nand is corrected by the teache r. Learning stops when the algorithm achieves an acceptable level of \\nperformance.  \\n \\nExample  \\nConsider the following data regarding patients entering a clinic. The data consists of the gender',\n",
       " 'performance.  \\n \\nExample  \\nConsider the following data regarding patients entering a clinic. The data consists of the gender \\nand age of the patients and each patient is labeled as “healthy” or  “sick”.  \\n \\n \\n \\n1.5.2 Unsupervised learning  \\nCorrect responses are not provided, but instead the algorithm tries to identify similarities \\nbetween the inputs so that inputs that have something in common are categorised  together. The \\nstatistical approach to unsupervised learning is  \\nknown as density estimation.  \\n \\nUnsupervised learning is a type of machine learning algorithm used to draw inferences from \\ndatasets consisting of input data without labeled responses. In unsuper vised learning algorithms, a',\n",
       " 'Unsupervised learning is a type of machine learning algorithm used to draw inferences from \\ndatasets consisting of input data without labeled responses. In unsuper vised learning algorithms, a \\nclassification or categorization is not included in the observations. There are no output values and so \\nthere is no estimation of functions. Since the examples given to the learner are unlabeled, the accuracy \\nof the structure t hat is output by the algorithm cannot be evaluated. The most common unsupervised \\nlearning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns \\n14 \\n or grouping in data.  \\n \\nExample  \\nConsider the following data regarding pa tients entering a clinic. The data consists of the gender \\nand age of the patients.',\n",
       " '14 \\n or grouping in data.  \\n \\nExample  \\nConsider the following data regarding pa tients entering a clinic. The data consists of the gender \\nand age of the patients.  \\n \\nBased on this data, can we infer anything regarding the patients entering the clinic?  \\n \\n1.5.3 Reinforcement learning  \\nThis is somewhere between supervised and unsupervised l earning. The algorithm gets told \\nwhen the answer is wrong, but does not get told how to correct it. It has to explore and try out different \\npossibilities until it works out how to get the answer right. Reinforcement learning is sometime called \\nlearning wit h a critic because of this monitor that scores the answer, but does not suggest \\nimprovements.',\n",
       " 'learning wit h a critic because of this monitor that scores the answer, but does not suggest \\nimprovements.  \\n \\nReinforcement learning is the problem of getting an agent to act in the world so as to maximize \\nits rewards. A learner (the program) is not told what actions to take as in most forms of machine \\nlearning, but instead must discover which actions yield the most reward by trying them. In the most \\ninteresting and challenging cases, actions may affect not only the immediate reward but also the next \\nsituations and, throu gh that, all subsequent rewards.  \\n \\nExample  \\nConsider teaching a dog a new trick: we cannot tell it what to do, but we can reward/punish it if \\nit does the right/wrong thing. It has to find out what it did that made it get the reward/punishment. We',\n",
       " 'it does the right/wrong thing. It has to find out what it did that made it get the reward/punishment. We \\ncan use a s imilar method to train computers to do many tasks, such as playing backgammon or chess, \\nscheduling jobs, and controlling robot limbs. Reinforcement learning is different from supervised \\nlearning. Supervised learning is learning from examples provided by a knowledgeable expert.  \\n \\n1.6 PERSPECTIVES AND ISSUES IN MACHINE LEARNING  \\n \\nPerspectives in Machine Learning  \\nOne useful perspective on machine learning is that it involves searching a very large space of \\npossible hypotheses to determine one that best fits the observed data and any prior knowledge held by \\nthe learner.',\n",
       " \"possible hypotheses to determine one that best fits the observed data and any prior knowledge held by \\nthe learner.  \\nFor example, consider the space of hypotheses that could in principle be output by the above checkers \\nlearner. This hypothesis space consists of all evaluation functions that can be represented by some \\nchoice of values for the weights wo through w6. The learner's task is thus to search through this vast \\nspace to locate the hypothesis that is most consistent with the available training examples. The LMS \\nalgorithm for fitting weights achieves this goa l by iteratively tuning the weights, adding a correction to \\neach weight each time the hypothesized evaluation function predicts a value that differs from the\",\n",
       " 'each weight each time the hypothesized evaluation function predicts a value that differs from the \\ntraining value. This algorithm works well when the hypothesis representation considered by the lea rner \\ndefines a continuously parameterized space of potential hypotheses.  \\n \\n15 \\n Many of the chapters in this book present algorithms that search a hypothesis space defined by \\nsome underlying representation (e.g., linear functions, logical descriptions, decision trees, artificial \\nneural networks). These different hypothesis representat ions are appropriate for learning different \\nkinds of target functions. For each of these hypothesis representations, the corresponding learning \\nalgorithm takes advantage of a different underlying structure to organize the search through the',\n",
       " 'kinds of target functions. For each of these hypothesis representations, the corresponding learning \\nalgorithm takes advantage of a different underlying structure to organize the search through the \\nhypothesis spac e.  \\n \\nThroughout this book we will return to this perspective of learning as a search problem in order \\nto characterize learning methods by their search strategies and by the underlying structure of the \\nsearch spaces they explore. We will also find this view point useful in formally analyzing the relationship \\nbetween the size of the hypothesis space to be searched, the number of training examples available, \\nand the confidence we can have that a hypothesis consistent with the training data will correctly \\ngenera lize to unseen examples.  \\n \\nIssues in Machine Learning',\n",
       " 'and the confidence we can have that a hypothesis consistent with the training data will correctly \\ngenera lize to unseen examples.  \\n \\nIssues in Machine Learning  \\nOur checkers example raises a number of generic questions about machine learning. The field of \\nmachine learning, and much of this book, is concerned with answering questions such as the following:  \\n \\n\\uf0b7 What algorithms exist for learning general target functions from specific training examples? In \\nwhat settings will particular algorithms converge to the desired function, given sufficient \\ntraining data? Which algorithms perform best for which types of problems and representations?  \\n\\uf0b7 How much training data is sufficient? What general bounds can be found to relate the',\n",
       " \"training data? Which algorithms perform best for which types of problems and representations?  \\n\\uf0b7 How much training data is sufficient? What general bounds can be found to relate the \\nconfidence in learned hypotheses to the amount of training experience and the character of the \\nlearner's hypothesis space?  \\n\\uf0b7 When and how can prior know ledge held by the learner guide the process of generalizing from \\nexamples? Can prior knowledge be helpful even when it is only approximately correct?  \\n\\uf0b7 What is the best strategy for choosing a useful next training experience, and how does the \\nchoice of this strategy alter the complexity of the learning problem?  \\n\\uf0b7 What is the best way to reduce the learning task to one or more function approximation\",\n",
       " 'choice of this strategy alter the complexity of the learning problem?  \\n\\uf0b7 What is the best way to reduce the learning task to one or more function approximation \\nproblems? Put another way, what specific functions should the system attempt to learn? Can \\nthis process itself be  automated?  \\n\\uf0b7 How can the learner automatically alter its representation to improve its ability to represent \\nand learn the target function?  \\n \\n1.7 Version Spaces  \\nDefinition (Version space). A concept is complete if it covers all positive examples.  \\n \\nA concept is consistent if it covers none of the negative examples. The version space is the set of all \\ncomplete and consistent concepts. This set is convex and is fully defined by its least and most general \\nelements.',\n",
       " 'complete and consistent concepts. This set is convex and is fully defined by its least and most general \\nelements.  \\n \\nThe key idea in the CANDIDATE -ELIMINA TION algorithm is to output a description of the set of all \\nhypotheses consistent with the training examples  \\n \\n1.7.1 Representation  \\nThe Candidate – Elimination  algorithm finds all describable hypotheses that are consistent with the 16 \\n observed training exampl es. In order to define this algorithm precisely, we begin with a few basic \\ndefinitions. First, let us say that a hypothesis is consistent with the training examples if it correctly \\nclassifies these examples.  \\n \\nDefinition: A hypothesis h is consistent with a set of training examples D if and only if h(x) = c(x) for \\neach example (x, c(x)) in D.',\n",
       " 'classifies these examples.  \\n \\nDefinition: A hypothesis h is consistent with a set of training examples D if and only if h(x) = c(x) for \\neach example (x, c(x)) in D. \\n \\n \\n \\nNote difference between definitions of consistent and satisfies  \\n\\uf0b7 An example x is said to satisfy hypothesis h when h(x) = 1, regardless of whether x is a positive \\nor negative example of the target concept.  \\n\\uf0b7 An example x is said to consistent with hypothesis h iff h(x) = c(x)  \\n \\nDefinition: version space - The version space, denoted V SH, D with respect to hypothesis space H and \\ntraining examples D, is th e subset of hypotheses from H consistent with the training examples in D  \\n \\n \\n \\n1.7.2 The LIST -THEN -ELIMINATION algorithm',\n",
       " 'training examples D, is th e subset of hypotheses from H consistent with the training examples in D  \\n \\n \\n \\n1.7.2 The LIST -THEN -ELIMINATION algorithm  \\nThe LIST -THEN -ELIMINATE algorithm first initializes the version space to contain all hypotheses in H \\nand then eliminates any hypothesis found inconsistent with any training example.  \\n \\n1. VersionSpace c a list containing every hypothesis in H  \\n2. For each training example, (x, c(x)) remove from VersionSpace any hypothesis h for which h(x) ≠ c(x)  \\n3. Output the list of hypotheses in VersionSpace  \\n \\n\\uf0b7 List-Then -Eliminate works in principle, so long as version space is finite.  \\n\\uf0b7 However, since it requires exhaustive enumeration of all hypotheses in practice it is not feasible.  \\n \\nA More Compact Representation for Version Spaces',\n",
       " \"\\uf0b7 However, since it requires exhaustive enumeration of all hypotheses in practice it is not feasible.  \\n \\nA More Compact Representation for Version Spaces  \\nThe version space is represen ted by its most general and least general members. These members form general \\nand specific boundary sets that delimit the version space within the partially ordered hypothesis space.  \\nDefinition: The general boundary G, with respect to hypothesis space H and training data D, is the set of \\nmaximally general members of H consistent with D \\n \\nG \\uf0ba{g \\uf0ce H | Consistent (g, D)\\uf0d9(\\uf0d8\\uf024g' \\uf0ce H)[(g' \\uf03e g) \\uf0d9 Consistent (g', D )]} \\ng \\n \\nDefinition: The specific boundary S, with respect to hypothesis space H and training data D, is the set of \\nminimally general (i.e., maximally specific) members of H consistent with D.\",\n",
       " \"g \\n \\nDefinition: The specific boundary S, with respect to hypothesis space H and training data D, is the set of \\nminimally general (i.e., maximally specific) members of H consistent with D. \\n \\nS \\uf0ba{s \\uf0ce H | Consistent (s, D)\\uf0d9(\\uf0d8\\uf024s' \\uf0ce H)[(s \\uf03e s') \\uf0d9 Consistent (s', D)]} \\ng \\n \\nTheorem: Version Space representation theorem  \\nTheorem: Let X be an arbitrary set of instances and Let H be a set of Boolean -valued hypotheses defined over X. \\nLet c: X →{O, 1} be an arbitrary target concept defined over X, and let D be an arbitrary set of training examples \\n17 \\n {(x, c(x))). For all X, H, c, and D su ch that S and G are well defined,  \\n \\nVS ={ h \\uf0ce H | (\\uf024s \\uf0ce S ) (\\uf024g \\uf0ce G ) ( g \\uf0b3 h \\uf0b3 s )} \\nH,D g g \\n \\nTo Prove:  \\n1. Every h satisfying the right hand side of the above expression is in  VS\",\n",
       " 'VS ={ h \\uf0ce H | (\\uf024s \\uf0ce S ) (\\uf024g \\uf0ce G ) ( g \\uf0b3 h \\uf0b3 s )} \\nH,D g g \\n \\nTo Prove:  \\n1. Every h satisfying the right hand side of the above expression is in  VS \\n                                                                                                                                            H, D  \\n2. Every member  of VS satisfies the right -hand side of the  expression  \\n                                         H, D \\n \\nSketch of proof:  \\n1. let g, h, s be arbitrary members of G, H, S respectively with g \\uf0b3g h \\uf0b3g s \\n\\uf0b7 By the definition of S, s must be satisfied by all positive examples in D. Because h \\uf0b3g s, \\nh must also be satisfied by all positive examples in D.  \\n\\uf0b7 By the definition  of G, g cannot  be satisfied  by any negative  example  in D, and because g \\uf0b3g h',\n",
       " 'h must also be satisfied by all positive examples in D.  \\n\\uf0b7 By the definition  of G, g cannot  be satisfied  by any negative  example  in D, and because g \\uf0b3g h \\nh cannot be satisfied by any negative example in D. Because h is satisfied by all positive \\nexamples in D and by no negative examples in D, h is consistent with D, and therefore h is a \\nmember of VSH,D. \\n2. It can be proven by assuming some h in VSH,D,that does n ot satisfy the right -hand side of \\nthe expression, then showing that this leads to an  inconsistency  \\n1.7.3 CANDIDATE -ELIMINATION Learning Algorithm  \\n \\nThe CANDIDATE -ELIMINTION algorithm computes the version space containing all hypotheses \\nfrom H that are consi stent with an observed sequence of training examples.',\n",
       " 'The CANDIDATE -ELIMINTION algorithm computes the version space containing all hypotheses \\nfrom H that are consi stent with an observed sequence of training examples.  \\n \\nInitialize G to the set of maximally general hypotheses in H Initialize S to the set of maximally specific \\nhypotheses in H For each training example d, do  \\n• If d is a positive  example  \\n• Remove from G any hypothesis inconsistent with  d \\n• For each hypothesis s in S that is not consistent with  d \\n• Remove s from  S \\n• Add to S all minimal generalizations h of s such  that \\n• h is consistent with d, and some member of G is more general than  h \\n• Remove  from  S any hypothesis  that is more  general  than  another  hypothesis  in S \\n \\n• If d is a negative  example',\n",
       " '• h is consistent with d, and some member of G is more general than  h \\n• Remove  from  S any hypothesis  that is more  general  than  another  hypothesis  in S \\n \\n• If d is a negative  example  \\n• Remove from S any hypothesis inconsistent with  d \\n• For each hypothesis g in G that is not consistent with  d \\n• Remove g from  G 18 \\n • Add to G all minimal specializations h of g such  that \\n• h is consistent with d, and some member of S is more specific than  h \\n• Remove from G any hypothesis that is less general than another hypothesis in  G \\nCANDIDATE - ELIMINTION algorithm using version spaces  \\n \\n1.7.4 An Illustrative Example  \\n \\n \\nExample  Sky AirTemp  Humidity  Wind  Water  Forecast  EnjoySport  \\n1 Sunny  Warm  Normal  Strong  Warm  Same  Yes',\n",
       " '1.7.4 An Illustrative Example  \\n \\n \\nExample  Sky AirTemp  Humidity  Wind  Water  Forecast  EnjoySport  \\n1 Sunny  Warm  Normal  Strong  Warm  Same  Yes \\n2 Sunny  Warm  High  Strong  Warm  Same  Yes \\n3 Rainy  Cold  High  Strong  Warm  Change  No \\n4 Sunny  Warm  High  Strong  Cool  Change  Yes \\n \\nCANDIDATE -ELIMINTION algorithm begins by initializing the version space to the set of all \\nhypotheses in H;  \\n \\nInitializing the G boundary set to contain the most general hypothesis in H  \\nG0  \\uf0b3?,  ?,  ?,  ?,  ?, ?\\uf0b3  \\n \\nInitializing the S boundary set to contain the most specific (least general) hypothesis  \\nS0  \\uf0b3\\uf0b3, \\uf0b3, \\uf0b3, \\uf0b3, \\uf0b3, \\uf0b3\\uf0b3  \\n \\n\\uf0b7 When the first training example is presented , the CANDIDATE -ELIMINTION algorithm checks the',\n",
       " 'S0  \\uf0b3\\uf0b3, \\uf0b3, \\uf0b3, \\uf0b3, \\uf0b3, \\uf0b3\\uf0b3  \\n \\n\\uf0b7 When the first training example is presented , the CANDIDATE -ELIMINTION algorithm checks the \\nS boundary and finds that it is overly specific and it fails to cover the positive example.  \\n\\uf0b7 The boundary is therefore revised by moving it to the least more general hypothesis that covers \\nthis new example  \\n\\uf0b7 No update of the G boundary is needed in response to this training example because Go \\ncorrectly covers this  example  \\n \\n \\n \\n \\n\\uf0b7 When the second training example is observed,  it has a similar effect of generalizing S  further to S 2, \\nleaving G again unchanged i.e., G 2 = G1 = G0 \\n \\n19 \\n  \\n \\n \\n\\uf0b7 Consider  the third  training  example . This negative  example  reveals  that the G boundary of',\n",
       " 'leaving G again unchanged i.e., G 2 = G1 = G0 \\n \\n19 \\n  \\n \\n \\n\\uf0b7 Consider  the third  training  example . This negative  example  reveals  that the G boundary of \\nthe version space is overly general, that is, the hypothesis in G incorrectly predicts that this \\nnew example is a positive  example.  \\n\\uf0b7 The hypothesis in the G boundary must therefore be specialized until it correctly classifies \\nthis new negative  example . \\n \\n \\n \\nGiven that there are six attributes that could be specified to specialize G 2, why are there only  three \\nnew hypotheses in G 3? \\nFor example, the hypothesis h = (?, ?, Normal, ?, ?, ?) is a minimal specialization of G2 that \\ncorrectly  labels  the new  example  as a negative  example,  but it is not included  in G3. The',\n",
       " 'For example, the hypothesis h = (?, ?, Normal, ?, ?, ?) is a minimal specialization of G2 that \\ncorrectly  labels  the new  example  as a negative  example,  but it is not included  in G3. The \\nreason this hypothesis is excluded is that it is inconsistent with the previously encountered \\npositive examples  \\n \\nConsider the fourth training  example . \\n \\n20 \\n  \\n \\n \\n\\uf0b7 This positive example further generalizes the S boundary of the version space. It also \\nresults in removing one member of the G boundary, because this member fails to cover \\nthe new positive  example  \\n \\nAfter processing these four examples, the boundary sets S 4 and G 4 delimit the version space of all \\nhypotheses consistent with the set of incrementally ob served training examples.',\n",
       " 'After processing these four examples, the boundary sets S 4 and G 4 delimit the version space of all \\nhypotheses consistent with the set of incrementally ob served training examples.  \\n \\n \\n \\n \\n \\n1.8 Probably approximately correct learning  \\n \\nIn computer science, computational learning theory (or just learning theory) is a subfield of \\nartificial intelligence devoted to studying the design and analysis of machine learni ng algorithms. In \\ncomputational learning theory, probably approximately correct learning (PAC learning) is a framework \\nfor mathematical analysis of machine learning algorithms. It was proposed in 1984 by Leslie Valiant.  \\n \\nIn this framework, the learner (tha t is, the algorithm) receives samples and must select a',\n",
       " 'for mathematical analysis of machine learning algorithms. It was proposed in 1984 by Leslie Valiant.  \\n \\nIn this framework, the learner (tha t is, the algorithm) receives samples and must select a \\nhypothesis from a certain class of hypotheses. The goal is that, with high probability (the “probably” \\npart), the selected hypothesis will have low generalization error (the “approximately correct” pa rt). In \\nthis section we first give an informal definition of PAC -learnability. After introducing a few nore notions, \\nwe give a more formal, mathematically oriented, definition of PAC -learnability. At the end, we mention \\none of the applications of PAC -learn ability.  \\n \\nPAC-learnability  \\nTo define PAC -learnability we require some specific terminology and related notations.  \\n21',\n",
       " 'one of the applications of PAC -learn ability.  \\n \\nPAC-learnability  \\nTo define PAC -learnability we require some specific terminology and related notations.  \\n21 \\n \\uf0b7 Let X be a set called the instance space which may be finite or infinite. For example, X may be \\nthe set of all points in a plane.  \\n\\uf0b7 A concept class C for X is a family of functions c : X \\uf0e0 {0; 1}. A member of C is called a concept. \\nA concept can also be thought of as a subset of X. If C is a subset of X, it defines a unique \\nfunction µ c : X \\uf0e0 {0; 1} as follows:  \\n \\n \\n \\n\\uf0b7 A hypothesis h is als o a function h : X \\uf0e0 {0; 1}. So, as in the case of concepts, a hypothesis can \\nalso be thought of as a subset of X. H will denote a set of hypotheses.',\n",
       " '\\uf0b7 A hypothesis h is als o a function h : X \\uf0e0 {0; 1}. So, as in the case of concepts, a hypothesis can \\nalso be thought of as a subset of X. H will denote a set of hypotheses.  \\n\\uf0b7 We assume that F is an arbitrary, but fixed, probability distribution over X.  \\n\\uf0b7 Training examples are obtain ed by taking random samples from X. We assume that the samples \\nare randomly generated from X according to the probability distribution F.  \\n \\nNow, we give below an informal definition of PAC -learnability.  \\n \\nDefinition (informal)  \\nLet X be an instance space, C a  concept class for X, h a hypothesis in C and F an arbitrary, but fixed, \\nprobability distribution. The concept class C is said to be PAC -learnable if there is an algorithm A which,',\n",
       " 'probability distribution. The concept class C is said to be PAC -learnable if there is an algorithm A which, \\nfor samples drawn with any probability distribution F and any concept c Є C , will with high probability \\nproduce a hypothesis h Є C whose error is small.  \\n \\nExamples  \\n \\nTo illustrate the definition of PAC -learnability, let us consider some concrete examples . \\n \\n \\nFigure : An axis -aligned rectangle in the Euclidean plane  \\n \\nExample  \\n22 \\n \\uf0b7 Let the instance space be the set X of all points in the Euclidean plane. Each point is represented \\nby its coordinates (x; y). So, the dimension or length of the instances is 2.  \\n\\uf0b7 Let the concept class C be the set of all “axis -aligned rectangles” in the pl ane; that is, the set of',\n",
       " 'by its coordinates (x; y). So, the dimension or length of the instances is 2.  \\n\\uf0b7 Let the concept class C be the set of all “axis -aligned rectangles” in the pl ane; that is, the set of \\nall rectangles whose sides are parallel to the coordinate axes in the plane (see Figure).  \\n\\uf0b7 Since an axis -aligned rectangle can be defined by a set of inequalities of the following form \\nhaving four parameters  \\n \\na ≤ x ≤ b,    c ≤ y ≤ d \\n \\nthe size of a concept is 4.  \\n\\uf0b7 We take the set H of all hypotheses to be equal to the set C of concepts, H = C.  \\n \\nGiven a set of sample points labeled positive or negative, let L be the algorithm which outputs the \\nhypothesis defined by the axis -aligned rect angle which gives the tightest fit to the positive examples',\n",
       " 'hypothesis defined by the axis -aligned rect angle which gives the tightest fit to the positive examples \\n(that is, that rectangle with the smallest area that includes all of the positive examples and none of the \\nnegative examples) (see Figure bleow).  \\n \\n \\nFigure :  Axis -aligned rectangle which gives the tightest fit to the positive examples  \\n \\nIt can be shown that, in the notations introduced above, the concept class C is PAC -learnable by the \\nalgorithm L using the hypothesis space H of all axis -aligned rectangles.  \\n \\n1.9 Vapnik -Chervonenkis (VC) dimension  \\nThe concepts of Vapnik -Chervonenkis dimension (VC dimension) and probably approximate \\ncorrect (PAC) learning are two important concepts in the mathematical theory of learnability and hence',\n",
       " 'The concepts of Vapnik -Chervonenkis dimension (VC dimension) and probably approximate \\ncorrect (PAC) learning are two important concepts in the mathematical theory of learnability and hence \\nare mathematically oriented. T he former is a measure of the capacity (complexity, expressive power, \\nrichness, or flexibility) of a space of functions that can be learned by a classification algorithm. It was \\noriginally defined by Vladimir Vapnik and Alexey Chervonenkis in 1971. The lat ter is a framework for the \\nmathematical analysis of learning algorithms. The goal is to check whether the probability for a selected \\nhypothesis to be approximately correct is very high. The notion of PAC  \\nlearning was proposed by Leslie Valiant in 1984.  \\n \\nV-C dimension',\n",
       " 'hypothesis to be approximately correct is very high. The notion of PAC  \\nlearning was proposed by Leslie Valiant in 1984.  \\n \\nV-C dimension  \\nLet H be the hypothesis space for some machine learning problem. The Vapnik -Chervonenkis dimension \\nof H, also called the VC dimension of H, and denoted by V C(H), is a measure of the complexity (or, \\ncapacity, expressive power, richness, or flex ibility) of the space H. To define the VC dimension we \\nrequire the notion of the shattering of a set of instances.  \\n23 \\n  \\nShattering of a set  \\nLet D be a dataset containing N examples for a binary classification problem with class labels 0 and 1. \\nLet H be a hypot hesis space for the problem. Each hypothesis h in H partitions D into two disjoint \\nsubsets as follows:',\n",
       " 'Let H be a hypot hesis space for the problem. Each hypothesis h in H partitions D into two disjoint \\nsubsets as follows:  \\n \\n \\nSuch a partition of S is called a “dichotomy” in D. It can be shown that there are 2N possible dichotomies \\nin D. To each dichotomy of D there is a unique assignment of the labels “1” and “0” to the elements of \\nD. Conversely, if S is any subset of D then, S defines a unique hypothesis h as follows:  \\n \\n \\nThus to specify a hypothesis h, we need only specify the set {x Є D |  h(x) = 1}. Figure 3.1 shows al l \\npossible dichotomies of D if D has three elements. In the figure, we have shown only one of the two sets \\nin a dichotomy, namely the set {x Є D |  h(x) = 1}.The circles and ellipses represent such sets.  \\n \\n \\n \\n \\nDefinition',\n",
       " 'in a dichotomy, namely the set {x Є D |  h(x) = 1}.The circles and ellipses represent such sets.  \\n \\n \\n \\n \\nDefinition  \\nA set of examples D is said to be shattered by a hypothesis space H if and only if for every dichotomy of \\nD there exists some hypothesis in H consistent with the dichotomy of D.  \\n \\nThe following example illustrates the concept of Vapnik -Chervonenkis dimension.  \\n \\nExample  \\n \\nIn figure ,  we see that an axis -aligned rectangle can shatter four points in two dimensions. Then  VC(H), \\nwhen H is the hypothesis class of axis -aligned rectangles in two dimensions, is four. In calculating the VC \\ndimension, it is enough that we find four points that  can be shattered; it is not necessary that we be \\nable to shatter any four points in two dimensions.  \\n \\n24',\n",
       " 'dimension, it is enough that we find four points that  can be shattered; it is not necessary that we be \\nable to shatter any four points in two dimensions.  \\n \\n24 \\n  \\nFig: An axis -aligned rectangle can shattered four points. Only rectangle covering two points are shown.  \\n \\nVC dimension may seem pessimistic. It tells us that using a rectangle as our hypothesis class, we can \\nlearn only datasets containing four points and not more.  \\n \\n \\n \\n \\n \\n \\n \\nUnit II  \\nSupervised and Unsupervised Learning  \\n \\nTopics: Decision Trees: ID3, Classification and Regression Trees, Regression: Linear Regres sion, \\nMultiple Linear Regression, Logistic Regression, Neural Networks: Introduction, Perception, \\nMultilayer Perception, Support Vector Machines: Linear and Non -Linear, Kernel Functions, K',\n",
       " 'Multiple Linear Regression, Logistic Regression, Neural Networks: Introduction, Perception, \\nMultilayer Perception, Support Vector Machines: Linear and Non -Linear, Kernel Functions, K \\nNearest Neighbors. Introduction to clustering, K -means clustering, K-Mode Clustering.  \\n \\n2.1. Decision Tree  \\nIntroduction  Decision Trees are a type of Supervised Machine Learning (that is you explain what \\nthe input is and what the corresponding output is in the training data) where the data is continuously \\nsplit according to a certain parameter. The tree can be explained by two entities, namely decision \\nnodes and leaves . The leaves are the decisions or the final outcomes. And the decision nodes are where \\nthe data is split.',\n",
       " 'nodes and leaves . The leaves are the decisions or the final outcomes. And the decision nodes are where \\nthe data is split.  \\n \\nAn example of a decision tree can be explained using above binary tree. Let’s say you want to predict \\nwhether a person is fit given their information like age, eating habit, and physical activity, etc. The \\n25 \\n decision nodes here are questions like ‘What’s the age?’, ‘Does he exercise?’, and ‘Does he eat a lot of \\npizzas’? And the leaves, which are outcomes lik e either ‘fit’, or ‘unfit’. In this case this was a binary \\nclassification problem (a yes no type problem). There are two main types of Decision Trees:  \\n1. Classification trees  (Yes/No types)  \\nWhat we have seen above is an example of classification tree, where t he outcome was a variable like',\n",
       " '1. Classification trees  (Yes/No types)  \\nWhat we have seen above is an example of classification tree, where t he outcome was a variable like \\n‘fit’ or ‘unfit’. Here the decision variable is  Categorical . \\n \\n2. Regression trees  (Continuous data types)  \\nHere the decision or the outcome variable is  Continuous , e.g. a number like 123.   Working  Now that we \\nknow what a Decision  Tree is, we’ll see how it works internally. There are many algorithms out there \\nwhich construct Decision Trees, but one of the best is called as  ID3 Algorithm . ID3 Stands for  Iterative \\nDichotomiser 3 . Before discussing the ID3 algorithm, we’ll go through few definitions.  Entropy  Entropy, \\nalso called as Shannon Entropy is denoted by H(S) for a finite set S, is the measure of the amount of',\n",
       " 'also called as Shannon Entropy is denoted by H(S) for a finite set S, is the measure of the amount of \\nuncertainty or randomness in data.  \\n \\nIntuitively, it tells us about the predictability of a certain event. Example, cons ider a coin toss whose \\nprobability of heads is 0.5 and probability of tails is 0.5. Here the entropy is the highest possible, since \\nthere’s no way of determining what the outcome might be. Alternatively, consider a coin which has \\nheads on both the sides, t he entropy of such an event can be predicted perfectly since we know \\nbeforehand that it’ll always be heads. In other words, this event has  no randomness  hence it’s entropy \\nis zero. In particular, lower values imply less uncertainty while higher values impl y high',\n",
       " 'beforehand that it’ll always be heads. In other words, this event has  no randomness  hence it’s entropy \\nis zero. In particular, lower values imply less uncertainty while higher values impl y high \\nuncertainty.  Information Gain  Information gain is also called as Kullback -Leibler divergence denoted by \\nIG(S,A) for a set S is the effective change in entropy after deciding on a particular attribute A. It \\nmeasures the relative change in entropy wit h respect to the independent variables  \\n \\n\\uf028\\uf029\\uf028\\uf029\\uf028\\uf029ASHSHASIG , , \\uf02d\\uf03d\\n \\nAlternatively,  \\n \\n \\n \\nwhere IG(S, A) is the information gain by applying feature A. H(S) is the Entropy of the entire set, while \\nthe second term calculates the Entropy after applying the feature A, where P(x) is the probability of',\n",
       " 'the second term calculates the Entropy after applying the feature A, where P(x) is the probability of \\neven t x. Let’s understand this with the help of an example Consider a piece of data collected over the \\ncourse of 14 days where the features are Outlook, Temperature, Humidity, Wind and the outcome \\nvariable is whether Golf was played on the day. Now, our job is  to build a predictive model which takes \\nin above 4 parameters and predicts whether Golf will be played on the day. We’ll build a decision tree \\nto do that using  ID3 algorithm.  \\n \\nDay Outlook  Temperature  Humidity  Wind  Play Golf  \\nD1 Sunny  Hot High  Weak  No \\nD2 Sunny  Hot High  Strong  No \\nD3 Overcast  Hot High  Weak  Yes \\n\\uf028\\uf029\\uf028\\uf029\\uf028\\uf029\\uf028\\uf029\\uf0e5\\n\\uf03d\\uf02a\\uf02d\\uf03dn\\nixHxP SHASIG\\n0,26 \\n D4 Rain  Mild  High  Weak  Yes',\n",
       " 'D1 Sunny  Hot High  Weak  No \\nD2 Sunny  Hot High  Strong  No \\nD3 Overcast  Hot High  Weak  Yes \\n\\uf028\\uf029\\uf028\\uf029\\uf028\\uf029\\uf028\\uf029\\uf0e5\\n\\uf03d\\uf02a\\uf02d\\uf03dn\\nixHxP SHASIG\\n0,26 \\n D4 Rain  Mild  High  Weak  Yes \\nD5 Rain  Cool  Normal  Weak  Yes \\nD6 Rain  Cool  Normal  Strong  No \\nD7 Overcast  Cool  Normal  Strong  Yes \\nD8 Sunny  Mild  High  Weak  No \\nD9 Sunny  Cool  Normal  Weak  Yes \\nD10 Rain  Mild  Normal  Weak  Yes \\nD11 Sunny  Mild  Normal  Strong  Yes \\nD12 Overcast  Mild  High  Strong  Yes \\nD13 Overcast  Hot Normal  Weak  Yes \\nD14 Rain  Mild  High  Strong  No \\n \\n2.1.1 ID3  \\n ID3 Algorithm will perform following tasks recursively  \\n \\n1. Create root node for the tree  \\n2. If all examples are positive, return leaf node „positive‟  \\n3. Else if all examples are negative, return leaf node „negative‟',\n",
       " '1. Create root node for the tree  \\n2. If all examples are positive, return leaf node „positive‟  \\n3. Else if all examples are negative, return leaf node „negative‟  \\n4. Calculate the entropy of current state H(S)  \\n5. For each attribute, calculate the entropy with respect to the attribute „x‟ denoted by H(S, x)  \\n6. Select the attribute which has maximum value of IG(S, x)  \\n7. Remove the attribute that offers highest IG from the set of attributes  \\n8. Repeat until we run out of all attributes, or the decision tree has all leaf nodes.  \\n \\nNow we‟ll go ahead and grow the decision tree.  The initial step is to calculate H(S), the Entropy of the current state. \\nIn the above example, we can see in total there are 5 No‟s and 9 Yes‟s.  \\n \\nYes No Total  \\n9 5 14',\n",
       " 'In the above example, we can see in total there are 5 No‟s and 9 Yes‟s.  \\n \\nYes No Total  \\n9 5 14 \\n \\n \\nwhere  „x‟ are the possible values for an attribute. Here,  attribute „Wind‟ takes two possible values in the sample \\ndata, hence x = {Weak, Strong} we‟ll have to calculate:  \\n \\n \\n \\nAmongst all the 14 examples we have  8 places where the wind is weak and 6 where the wind is Strong . \\n \\nWind = Weak  Wind = Strong  Total  \\n8 6 14 \\n27 \\n  \\nNow out of the 8 Weak examples, 6 of them were „Yes‟ for Play Golf and 2 of them were „No‟ for „Play Golf‟. So, \\nwe have,  \\n \\n \\nSimilarly, out of 6 Strong examples, we have  3 examples where the outcome was „Yes‟ for Play Golf and 3 \\nwhere we had „No‟ for Play Golf .',\n",
       " 'we have,  \\n \\n \\nSimilarly, out of 6 Strong examples, we have  3 examples where the outcome was „Yes‟ for Play Golf and 3 \\nwhere we had „No‟ for Play Golf . \\n \\n \\n \\nRemember, here half items belong to one class while other half belong to other. Hence we have perfect randomness. \\nNow we have all the pieces required to calculate the Information Ga in, \\n \\n \\n \\nWhich tells us the Information Gain by considering „Wind‟ as the feature and give us information gain of  0.048 . \\nNow we must similarly calculate the Information Gain for all the features.  \\n \\n \\nWe can clearly see that IG(S, Outlook) has the highest information gain of 0.246,  hence  we chose Outlook \\nattribute  as the root node . At this point, the decision tree looks like.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n28',\n",
       " 'attribute  as the root node . At this point, the decision tree looks like.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n28 \\n Here we observe that whenever the outlook is Overcast, Play Golf is always ‘Yes’, it’s no coincidence by \\nany chance, the simple tree resulted because of  the highest information gain is given by the attribute \\nOutlook . Now how do we proceed from this point? We can simply apply  recursion , you might want to \\nlook at the algorithm steps described earlier. Now that we’ve used Outlo ok, we’ve got three of them \\nremaining Humidity, Temperature, and Wind. And, we had three possible values of Outlook: Sunny, \\nOvercast, Rain. Where the Overcast node already ended up having leaf node ‘Yes’, so we’re left with \\ntwo subtrees to compute: Sunny a nd Rain.',\n",
       " 'Overcast, Rain. Where the Overcast node already ended up having leaf node ‘Yes’, so we’re left with \\ntwo subtrees to compute: Sunny a nd Rain.  \\n \\nTable where the value of Outlook is Sunny looks like:  \\nTemperature  Humidity  Wind  Play Golf  \\nHot High  Weak  No \\nHot High  Strong  No \\nMild  High  Weak  No \\nCool  Normal  Weak  Yes \\nMild  Normal  Strong  Yes \\n \\n \\nAs we can see the  highest Information Gain is given by Humidity . Proceeding in the same way with  \\n \\nwill give us Wind as the one with highest information gain. The final Decision Tree looks something like \\nthis. The final Decision Tree looks something like this.  \\n \\n \\n \\n2.1.2. Classification and Regression Trees  \\n2.1.2.1. Classification Trees',\n",
       " 'this. The final Decision Tree looks something like this.  \\n \\n \\n \\n2.1.2. Classification and Regression Trees  \\n2.1.2.1. Classification Trees  \\nA classification tree is an algorithm where the target variable is fixed or categorical. The \\nalgorithm is then used to identify the “class” within which a target variable would most likely fall.  \\nAn example of a classific ation -type problem would be determining who will or will not subscribe to a \\ndigital platform; or who will or will not graduate from high school.  \\nThese are examples of simple binary classifications where the categorical dependent variable can \\nassume only on e of two, mutually exclusive values. In other cases, you might have to predict among a',\n",
       " 'These are examples of simple binary classifications where the categorical dependent variable can \\nassume only on e of two, mutually exclusive values. In other cases, you might have to predict among a \\nnumber of different variables. For instance, you may have to predict which type of smartphone a \\nconsumer may decide to purchase.  \\nIn such cases, there are multiple values  for the categorical dependent variable. Here’s what a classic \\nclassification tree looks like  \\n29 \\n  \\n2.1.2.2. Regression Trees  \\nA regression tree refers to an algorithm where the target variable is and the algorithm is used to \\npredict it’s value. As an example o f a regression type problem, you may want to predict the selling \\nprices of a residential house, which is a continuous dependent variable.',\n",
       " 'predict it’s value. As an example o f a regression type problem, you may want to predict the selling \\nprices of a residential house, which is a continuous dependent variable.  \\nThis will depend on both continuous factors like square footage as well as categorical factors like the \\nstyle of home,  area in which the property is located and so on.  \\n \\n \\n \\nWhen to use Classification and Regression Trees  \\nClassification trees are used when the dataset needs to be split into classes which belong to the \\nresponse variable. In many cases, the classes Yes or No.  \\nIn other words, they are just two and mutually exclusive. In some cases, there may be more than two \\nclasses in which case a variant of the classification tree algorithm is used.',\n",
       " 'In other words, they are just two and mutually exclusive. In some cases, there may be more than two \\nclasses in which case a variant of the classification tree algorithm is used.  \\nRegression trees, on the other hand, are used when the response variable is c ontinuous. For instance, if \\nthe response variable is something like the price of a property or the temperature of the day, a \\nregression tree is used.  \\nIn other words, regression trees are used for prediction -type problems while classification trees are \\nused  for classification -type problems.  \\n \\nHow Classification and Regression Trees Work  \\nA classification tree splits the dataset based on the homogeneity of data. Say, for instance, \\nthere are two variables; income and age; which determine whether or not a consume r will buy a',\n",
       " 'A classification tree splits the dataset based on the homogeneity of data. Say, for instance, \\nthere are two variables; income and age; which determine whether or not a consume r will buy a \\nparticular kind of phone.  \\nIf the training data shows that 95% of people who are older than 30 bought the phone, the data gets \\nsplit there and age becomes a top node in the tree. This split makes the data “95% pure”. Measures of \\nimpurity like e ntropy or Gini index are used to quantify the homogeneity of the data when it comes to \\nclassification trees.  \\n30 \\n In a regression tree, a regression model is fit to the target variable using each of the independent \\nvariables. After this, the data is split at se veral points for each independent variable.',\n",
       " '30 \\n In a regression tree, a regression model is fit to the target variable using each of the independent \\nvariables. After this, the data is split at se veral points for each independent variable.  \\nAt each such point, the error between the predicted values and actual values is squared to get “A Sum \\nof Squared Errors” (SSE). The SSE is compared across the variables and the variable or point which has \\nthe low est SSE is chosen as the split point. This process is continued recursively.  \\n \\n \\nAdvantages of Classification and Regression Trees  \\nThe purpose of the analysis conducted by any classification or regression tree is to create a set of if -else \\nconditions that allow for the accurate prediction or classification of a case.  \\n(i) The Results are Simplistic',\n",
       " 'conditions that allow for the accurate prediction or classification of a case.  \\n(i) The Results are Simplistic  \\nThe interpretation of results summarized in classification or regression trees is usually fairly simple. The \\nsimplicity of results helps in the following ways.  \\n\\uf0b7 It allows for the rapid classification of new observations. That’s because it is much simpler to \\nevaluate just one or two logical conditions than to compute scores using complex nonlinear \\nequations for each group.  \\n\\uf0b7 It can often result in a simpler model which  explains why the observations are either classified \\nor predicted in a certain way. For instance, business problems are much easier to explain with \\nif-then statements than with complex nonlinear equations.',\n",
       " 'or predicted in a certain way. For instance, business problems are much easier to explain with \\nif-then statements than with complex nonlinear equations.  \\n(ii) Classification and Regression Trees are Nonpa rametric & Nonlinear  \\nThe results from classification and regression trees can be summarized in simplistic if -then conditions. \\nThis negates the need for the following implicit assumptions.  \\n\\uf0b7 The predictor variables and the dependent variable are linear.  \\n\\uf0b7 The p redictor variables and the dependent variable follow some specific nonlinear link function.  \\n\\uf0b7 The predictor variables and the dependent variable are monotonic.  \\nSince there is no need for such implicit assumptions, classification and regression tree methods a re well',\n",
       " '\\uf0b7 The predictor variables and the dependent variable are monotonic.  \\nSince there is no need for such implicit assumptions, classification and regression tree methods a re well \\nsuited to data mining. This is because there is very little knowledge or assumptions that can be made \\nbeforehand about how the different variables are related.  \\nAs a result, classification and regression trees can actually reveal relationships betwe en these variables \\nthat would not have been possible using other techniques.  \\n(iii) Classification and Regression Trees Implicitly Perform Feature Selection  \\nFeature selection or variable screening is an important part of analytics. When we use decision tree s,',\n",
       " '(iii) Classification and Regression Trees Implicitly Perform Feature Selection  \\nFeature selection or variable screening is an important part of analytics. When we use decision tree s, \\nthe top few nodes on which the tree is split are the most important variables within the set. As a result, \\nfeature selection gets performed automatically and we don’t need to do it again.  \\nLimitations of Classification and Regression Trees  \\n31 \\n Classification  and regression tree tutorials, as well as classification and regression tree ppts, exist in \\nabundance. This is a testament to the popularity of these decision trees and how frequently they are \\nused. However, these decision trees are not without their disa dvantages.',\n",
       " 'abundance. This is a testament to the popularity of these decision trees and how frequently they are \\nused. However, these decision trees are not without their disa dvantages.  \\nThere are many classification and regression trees examples where the use of a decision tree has not \\nled to the optimal result. Here are some of the limitations of classification and regression trees.  \\n(i) Overfitting  \\nOverfitting occurs when the tree takes into account a lot of noise that exists in the data and \\ncomes up with an inaccurate result.  \\n(ii) High variance  \\nIn this case, a small variance in the data can lead to a very high variance in the prediction, \\nthereby affecting the stability of the outcome.  \\n(iii) Low bias',\n",
       " '(ii) High variance  \\nIn this case, a small variance in the data can lead to a very high variance in the prediction, \\nthereby affecting the stability of the outcome.  \\n(iii) Low bias  \\nA decision tree that is very complex usually has a low bias. This makes it very difficult for the \\nmodel to incorporate any new data.  \\n \\nWhat is a CART in Machine Learning?  \\nA Classification and Regression Tree (CART) is a predictive algorithm used in  machine learning. It \\nexplains how a target variable’s values can be predicted based on other values.  \\nIt is a decision tree where each fork is a split in a predictor variable and each node at the end has a \\nprediction for the target variabl e. \\nThe CART algorithm is an important  decision tree algorithm  that lies at the foundation of machine',\n",
       " 'prediction for the target variabl e. \\nThe CART algorithm is an important  decision tree algorithm  that lies at the foundation of machine \\nlearning. Moreover, it is also the basis for other powerful machine learning algorithms like bagged \\ndecision trees, random forest and boosted decision tree s. \\nSumming up  \\nThe Classification and regression tree (CART) methodology is one of the oldest and most fundamental \\nalgorithms. It is used to predict outcomes based on certain predictor variables.  \\nThey are excellent for data mining tasks because they require  very little data pre -processing. Decision \\ntree models are easy to understand and implement which gives them a strong advantage when \\ncompared to other analytical models.  \\n \\n2.2. Regression  \\nRegression Analysis in Machine learning',\n",
       " 'tree models are easy to understand and implement which gives them a strong advantage when \\ncompared to other analytical models.  \\n \\n2.2. Regression  \\nRegression Analysis in Machine learning  \\nRegression analysis is a stat istical method to model the relationship between a dependent \\n(target) and independent (predictor) variables with one or more independent variables. More \\nspecifically, Regression analysis helps us to understand how the value of the dependent variable is \\nchanging corresponding to an independent variable when other independent variables are held fixed. It \\npredicts continuous/real values such as  temperature, age, salary, price,  etc. \\n \\nWe can understand the concept of regression analysis using the below example:',\n",
       " 'predicts continuous/real values such as  temperature, age, salary, price,  etc. \\n \\nWe can understand the concept of regression analysis using the below example:  \\n \\nExample:  Suppose there is a marketing company A, who does various advertisement every year and get \\nsales on that. The below list shows the advertisement made by the company in the last 5 years and the \\ncorresponding sales:  32 \\n  \\n \\nNow, the company wants to do t he advertisement of $200 in the year 2019  and wants to know the \\nprediction about the sales for this year . So to solve such type of prediction problems in machine \\nlearning, we need regression analysis.  \\nRegression is a  supervised learning technique  which hel ps in finding the correlation between variables',\n",
       " 'learning, we need regression analysis.  \\nRegression is a  supervised learning technique  which hel ps in finding the correlation between variables \\nand enables us to predict the continuous output variable based on the one or more predictor variables. \\nIt is mainly used for  prediction, forecasting, time series modeling, and determining the causal -effect \\nrelationship between variables . \\nIn Regression, we plot a graph between the variables which best fits the given datapoints, using this \\nplot, the machine learning model can make predictions about the data. In simple words,  \"Regression \\nshows a line or curve tha t passes through all the datapoints on target -predictor graph in such a way',\n",
       " 'shows a line or curve tha t passes through all the datapoints on target -predictor graph in such a way \\nthat the vertical distance between the datapoints and the regression line is minimum.\"  The distance \\nbetween datapoints and line tells whether a model has captured a strong relation ship or not.  \\n \\nSome examples of regression can be as:  \\no Prediction of rain using temperature and other factors  \\no Determining Market trends  \\no Prediction of road accidents due to rash driving.  \\n \\nTerminologies Related to the Regression Analysis:  \\no Dependent Variable:  The main factor in Regression analysis which we want to predict or \\nunderstand is called the dependent variable. It is also called  target variable .',\n",
       " 'o Dependent Variable:  The main factor in Regression analysis which we want to predict or \\nunderstand is called the dependent variable. It is also called  target variable . \\no Independent Variable:  The factors which affect the dependent variables or which are used to \\npredict the valu es of the dependent variables are called independent variable, also called as \\na predictor . \\no Outliers:  Outlier is an observation which contains either very low value or very high value in \\ncomparison to other observed values. An outlier may hamper the result,  so it should be \\navoided.  \\no Multicollinearity:  If the independent variables are highly correlated with each other than other \\nvariables, then such condition is called Multicollinearity. It should not be present in the dataset,',\n",
       " 'variables, then such condition is called Multicollinearity. It should not be present in the dataset, \\nbecause it creates problem while  ranking the most affecting variable.  \\no Underfitting and Overfitting:  If our algorithm works well with the training dataset but not well \\nwith test dataset, then such problem is called  Overfitting . And if our algorithm does not \\nperform well even with training  dataset, then such problem is called  underfitting . \\n \\n33 \\n Why do we use Regression Analysis?  \\nAs mentioned above, Regression analysis helps in the prediction of a continuous variable. There are \\nvarious scenarios in the real world where we need some future predic tions such as weather condition, \\nsales prediction, marketing trends, etc., for such case we need some technology which can make',\n",
       " 'various scenarios in the real world where we need some future predic tions such as weather condition, \\nsales prediction, marketing trends, etc., for such case we need some technology which can make \\npredictions more accurately. So for such case we need Regression analysis which is a statistical method \\nand used in machine lear ning and data science. Below are some other reasons for using Regression \\nanalysis:  \\no Regression estimates the relationship between the target and the independent variable.  \\no It is used to find the trends in data.  \\no It helps to predict real/continuous values.  \\no By performing the regression, we can confidently determine the  most important factor, the \\nleast important factor, and how each factor is affecting the other factors . \\n \\nTypes of Regression',\n",
       " 'o By performing the regression, we can confidently determine the  most important factor, the \\nleast important factor, and how each factor is affecting the other factors . \\n \\nTypes of Regression  \\nThere are various types of regressions which are used in data science an d machine learning. Each type \\nhas its own importance on different scenarios, but at the core, all the regression methods analyze the \\neffect of the independent variable on dependent variables. Here we are discussing some important \\ntypes of regression which are given below:  \\no Linear Regression  \\no Logistic Regression  \\no Polynomial Regression  \\no Support Vector Regression  \\no Decision Tree Regression  \\no Random Forest Regression  \\no Ridge Regression  \\no Lasso Regression  \\n \\n \\n \\n2.2.1. Linear Regression:',\n",
       " 'o Polynomial Regression  \\no Support Vector Regression  \\no Decision Tree Regression  \\no Random Forest Regression  \\no Ridge Regression  \\no Lasso Regression  \\n \\n \\n \\n2.2.1. Linear Regression:  \\no Linear regression is a statistical regression method which is used for predictive analysis.  \\no It is one of the very simple and easy algorithms which works on regression and shows the relationship \\nbetween the continuous variables.  \\no It is used for solving the regression problem in machine learni ng. \\no Linear regression shows the linear relationship between the independent variable (X -axis) and the \\ndependent variable (Y -axis), hence called linear regression.  \\n34 \\n o If there is only one input variable (x), then such linear regression is called  simple linear regression . And if',\n",
       " 'dependent variable (Y -axis), hence called linear regression.  \\n34 \\n o If there is only one input variable (x), then such linear regression is called  simple linear regression . And if \\nthere is more than one input variable, then such linear regression is called  multiple linear regression . \\no The relationship between variables in the linear regression model can be explained using the below image. \\nHere we are predicting the  salary of an employee on the basis of  the year of experience . \\n \\nBelow is the mathematical equation for Linear regression:  \\nY= aX+b   \\n \\nHere, Y = dependent variables (target variables),  \\nX= Independent variables (predictor variables),  \\na and b are the linear co efficients  \\n \\nSome popular applications of linear regression are:',\n",
       " 'Here, Y = dependent variables (target variables),  \\nX= Independent variables (predictor variables),  \\na and b are the linear co efficients  \\n \\nSome popular applications of linear regression are:  \\no Analyzing trends and sales estimates  \\no Salary forecasting  \\no Real estate prediction  \\no Arriving at ETAs in traffic.  \\n2.2.2. Logistic Regression:  \\no Logistic regression is another supervised learning algor ithm which is used to solve the classification \\nproblems. In  classification problems , we have dependent variables in a binary or discrete format such as 0 \\nor 1. \\no Logistic regression algorithm works with the categorical variable such as 0 or 1, Yes or No, True or False, \\nSpam or not spam, etc.  \\no It is a predictive analysis algorithm which works on the concept of probability.',\n",
       " 'Spam or not spam, etc.  \\no It is a predictive analysis algorithm which works on the concept of probability.  \\no Logistic regression is a type of regression, but it is different from the linear regression algorithm in the \\nterm how they are used.  \\no Logistic regression uses  sigmoid function  or logistic function which is a complex cost function. This \\nsigmoid function is used to model the data in logistic regression. The function can be represented as:  \\n35 \\n  \\no f(x)= Output between the 0 and 1 value.  \\no x= input t o the function  \\no e= base of natural logarithm.  \\nWhen we provide the input values (data) to the function, it gives the S -curve as follows:  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\no It uses the concept of threshold levels, values above the threshold level are rounded up to 1, and values',\n",
       " 'o It uses the concept of threshold levels, values above the threshold level are rounded up to 1, and values \\nbelow  the threshold level are rounded up to 0.  \\nThere are three types of logistic regression:  \\no Binary(0/1, pass/fail)  \\no Multi(cats, dogs, lions)  \\no Ordinal(low, medium, high)  \\nLinear Regression in Machine Learning  \\nLinear regression is one of the easiest and most popula r Machine Learning algorithms. It is a statistical method that \\nis used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables such \\nas sales, salary, age, product price,  etc. \\nLinear regression algorithm shows a  linear relationship between a dependent (y) and one or more independent (y)',\n",
       " 'as sales, salary, age, product price,  etc. \\nLinear regression algorithm shows a  linear relationship between a dependent (y) and one or more independent (y) \\nvariables, hence called as linear regression. Since linear regression shows the linear relationship, which means it \\nfinds how the value of the dependent variable is changing accor ding to the value of the independent variable.  \\nThe linear regression model provides a sloped straight line representing the relationship between the variables. \\nConsider the below image:  \\n36 \\n  \\nMathematically, we can represent a linear regression as:  \\n    \\n   y= a 0+a1x+ ε \\n \\nHere,  \\nY= Dependent Variable (Target Variable)  \\nX= Independent Variable (predictor Variable)  \\na0= intercept of the line (Gives an additional degree of freedom)',\n",
       " 'y= a 0+a1x+ ε \\n \\nHere,  \\nY= Dependent Variable (Target Variable)  \\nX= Independent Variable (predictor Variable)  \\na0= intercept of the line (Gives an additional degree of freedom)  \\na1 = Linear regression coefficient (scale factor to each input value).  \\nε = random error  \\nThe values for x and y variables are training datasets for Linear Regression model representation.  \\n \\nTypes of Linear Regression  \\n \\nLinear regression can be further divided into two types of the algorithm:  \\no Simple Linear Regression:  \\nIf a single independent variable is used to predict the value of a numerical dependent variable, then such a \\nLinear Regression algorithm is called Simple Linear Regression.  \\no Multiple Linear regression:',\n",
       " 'Linear Regression algorithm is called Simple Linear Regression.  \\no Multiple Linear regression:  \\nIf more than one independent variable is used to predict the value of a numeric al dependent variable, then \\nsuch a Linear Regression algorithm is called Multiple Linear Regression.  \\nLinear Regression Line:  \\nA linear line showing the relationship between the dependent and independent variables is called a  regression line . \\nA regression li ne can show two types of relationship:  \\no Positive Linear Relationship:  \\nIf the dependent variable increases on the Y -axis and independent variable increases on X -axis, then such a \\nrelationship is termed as a Positive linear relationship.  \\n37 \\n  \\no Negative Linear Rel ationship:',\n",
       " 'relationship is termed as a Positive linear relationship.  \\n37 \\n  \\no Negative Linear Rel ationship:  \\nIf the dependent variable decreases on the Y -axis and independent variable increases on the X -axis, then \\nsuch a relationship is called a negative linear relationship.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFinding the best fit \\nline:  \\n \\nWhen working with linear regression, our main goal is to find the best fit line that means the error between \\npredicted values and actual values should be minimized. The best fit line will have the least error.  \\nThe different values for weights or the coefficient of lines (a 0, a1) gives a different line of regression, so we \\nneed to calculate the best values for a 0 and a 1 to find the best fit line, so to calculate this we use cost function.',\n",
       " 'need to calculate the best values for a 0 and a 1 to find the best fit line, so to calculate this we use cost function.  \\nCost function - \\no The different values for weights or coefficient of lines (a 0, a1) gives the diff erent line of regression, and the \\ncost function is used to estimate the values of the coefficient for the best fit line.  \\no Cost function optimizes the regression coefficients or weights. It measures how a linear regression model \\nis performing.  \\no We can use the  cost function to find the accuracy of the  mapping function , which maps the input variable \\nto the output variable. This mapping function is also known as  Hypothesis function . \\nFor Linear Regression, we use the  Mean Squared Error (MSE)  cost function, which i s the average of',\n",
       " 'to the output variable. This mapping function is also known as  Hypothesis function . \\nFor Linear Regression, we use the  Mean Squared Error (MSE)  cost function, which i s the average of \\nsquared error occurred between the predicted values and actual values. It can be written as:  \\n \\n38 \\n For the above linear equation, MSE can be calculated as:  \\n \\n \\n \\nWhere,  \\nN=Total number of observation  \\nYi = Actual value  \\n(a1x i+a0)= Predicted value.  \\n \\nResiduals:  The distance between the actual value and predicted values is called residual. If the observed points are \\nfar from the regression line, then the residual will be high, and so cost function will high. If the scatter points are \\nclose to the regres sion line, then the residual will be small and hence the cost function.',\n",
       " 'close to the regres sion line, then the residual will be small and hence the cost function.  \\n \\nGradient Descent:  \\no Gradient descent is used to minimize the MSE by calculating the gradient of the cost function.  \\no A regression model uses gradient descent to update the coefficients of  the line by reducing the cost \\nfunction.  \\no It is done by a random selection of values of coefficient and then iteratively update the values to reach the \\nminimum cost function.  \\nModel Performance:  \\nThe Goodness of fit determines how the line of regression fits the set of observations. The process of \\nfinding the best model out of various models is called  optimization . It can be achieved by below method:  \\n \\n1. R-squared method:',\n",
       " 'finding the best model out of various models is called  optimization . It can be achieved by below method:  \\n \\n1. R-squared method:  \\no R-squared is a statistical method that determines the goodness of fit.  \\no It measures the st rength of the relationship between the dependent and independent variables on a scale of \\n0-100%.  \\no The high value of R -square determines the less difference between the predicted values and actual values \\nand hence represents a good model.  \\no It is also called a  coefficient of determination,  or coefficient of multiple determination  for multiple \\nregression.  \\no It can be calculated from the below formula:  \\n \\nAssumptions of Linear Regression  \\nBelow are some important assumptions of Linear Regression. These are some formal checks while building a',\n",
       " 'o It can be calculated from the below formula:  \\n \\nAssumptions of Linear Regression  \\nBelow are some important assumptions of Linear Regression. These are some formal checks while building a \\nLinear Regression model, which ensures to get the best possible result from the given dataset.  \\n \\no Linear relationship between the features and target:  \\nLinear regression assumes the linear relationship between the dependent and independent variables.  \\no Small or no multicollinearity between the features:  \\nMulticollinearity means high -correlation between the independent variables. Due to multicollinearity, it \\nmay difficult to find the true relationship between the predictors and targe t variables. Or we can say, it is \\n39',\n",
       " 'may difficult to find the true relationship between the predictors and targe t variables. Or we can say, it is \\n39 \\n difficult to determine which predictor variable is affecting the target variable and which is not. So, the \\nmodel assumes either little or no multicollinearity between the features or independent variables.  \\no Homoscedasticity  Assumption:  \\nHomoscedasticity is a situation when the error term is the same for all the values of independent variables. \\nWith homoscedasticity, there should be no clear pattern distribution of data in the scatter plot.  \\no Normal distribution of error terms:  \\nLinear regression assumes that the error term should follow the normal distribution pattern. If error terms',\n",
       " 'o Normal distribution of error terms:  \\nLinear regression assumes that the error term should follow the normal distribution pattern. If error terms \\nare not normally distributed, then confidence intervals will become either too wide or too narrow, which \\nmay cause difficulties in finding coefficie nts. \\nIt can be checked using the  q-q plot . If the plot shows a straight line without any deviation, which means \\nthe error is normally distributed.  \\no No autocorrelations:  \\nThe linear regression model assumes no autocorrelation in error terms. If there will be any correlation in \\nthe error term, then it will drastically reduce the accuracy of the model. Autocorrelation usually occurs if \\nthere is a dependency between residual errors.  \\nSimple Linear Regression in Machine Learning',\n",
       " 'there is a dependency between residual errors.  \\nSimple Linear Regression in Machine Learning  \\nSimple Linear Regression is a type of Regression algorithms that models the relationship between a dependent \\nvariable and a single independent variable. The relationship shown by a Simple Linear Regression model is linear or \\na sloped straight line, hence it is called Simple Linear Regressio n. \\nThe key point in Simple Linear Regression is that the  dependent variable must be a continuous/real value . \\nHowever, the independent variable can be measured on continuous or categorical values.  \\nSimple Linear regression algorithm has mainly two objectives : \\no Model the relationship between the two variables.  Such as the relationship between Income and \\nexpenditure, experience and Salary, etc.',\n",
       " 'o Model the relationship between the two variables.  Such as the relationship between Income and \\nexpenditure, experience and Salary, etc.  \\no Forecasting new observations.  Such as Weather forecasting according to temperature, Revenue of a \\ncompany according to t he investments in a year, etc.  \\nSimple Linear Regression Model:  \\nThe Simple Linear Regression model can be represented using the below equation:  \\ny= a 0+a1x+ ε  \\n \\n \\nWhere,  \\na0= It is the intercept of the Regression line (can be obtained putting x=0)  \\na1= It is the  slope of the regression line, which tells whether the line is increasing or decreasing.  \\nε = The error term. (For a good model it will be negligible)  \\n \\n2.2.3. Multiple Linear Regressions',\n",
       " 'ε = The error term. (For a good model it will be negligible)  \\n \\n2.2.3. Multiple Linear Regressions  \\nIn the previous topic, we have learned about Simple Linear Regression,  where a single \\nIndependent/Predictor(X) variable is used to model the response variable (Y). But there may be various cases in \\nwhich the response variable is affected by more than one predictor variable; for such cases, the Multiple Linear \\nRegression algo rithm is used.  40 \\n  \\nMoreover, Multiple Linear Regression is an extension of Simple Linear regression as it takes more than one \\npredictor variable to predict the response variable.  \\n \\n We can define it as:  \\n“Multiple Linear Regression is one of the important regre ssion algorithms which models the linear relationship',\n",
       " 'predictor variable to predict the response variable.  \\n \\n We can define it as:  \\n“Multiple Linear Regression is one of the important regre ssion algorithms which models the linear relationship \\nbetween a single dependent continuous variable and more than one independent variable. ” \\n \\nExample:  \\nPrediction of CO 2 emission based on engine size and number of cylinders in a car.  \\n \\nSome key points about  MLR:  \\no For MLR, the dependent or target variable(Y) must be the continuous/real, but the predictor or independent \\nvariable may be of continuous or categorical form.  \\no Each feature variable must model the linear relationship with the dependent variable.  \\no MLR tries to fit a regression line through a multidimensional space of data -points.  \\nMLR equation:',\n",
       " 'o Each feature variable must model the linear relationship with the dependent variable.  \\no MLR tries to fit a regression line through a multidimensional space of data -points.  \\nMLR equation:  \\nIn Multiple Linear Regression, the target variable(Y) is a linear combination of multiple predictor variables \\nx1, x2, x3, ...,x n. Since it is an enhancement of Simple Linear Regression, so the same is applied for the multiple \\nlinear regression equation, the equation becomes:  \\nY= b<sub>0</sub>+b<sub>1</sub>x<sub>1</sub>+  b<sub>2</sub>x<sub>2</sub>+  b<sub>3</sub>x<sub>\\n3</sub>+......  bnxn        ...............  (a)  \\nWhere,  \\nY= Output/Response variable  \\nb0, b1, b2, b3 , bn....= Coefficients of the model.  \\nx1, x2, x3, x4,...= Various Independent/feature variable',\n",
       " 'Where,  \\nY= Output/Response variable  \\nb0, b1, b2, b3 , bn....= Coefficients of the model.  \\nx1, x2, x3, x4,...= Various Independent/feature variable  \\nAssumptions for Multiple Linear Regression:  \\no A linear relationship  should exist between the Target and predictor v ariables.  \\no The regression residuals must be  normally distributed . \\no MLR assumes little or  no multicollinearity  (correlation between the independent variable) in data.  \\n2.3. Neural Networks (ANN - Artificial Neural Network ) \\n \\n2.3.1. Introduction  \\nThe term \" Artifi cial Neural Network \" is derived from Biological neural networks that develop the structure \\nof a human brain. Similar to the human brain that has neurons interconnected to one another, artificial neural',\n",
       " 'of a human brain. Similar to the human brain that has neurons interconnected to one another, artificial neural \\nnetworks also have neurons that are interconnected to one another in various layers of the networks. These neurons \\nare known as nodes.  \\n 41 \\n  \\n \\nThe given figure illustrates the typical diagram of Biological Neural Network.  \\n \\nThe typical Artificial Neural Network looks something like the given figure.  \\n \\n \\nDendrites from Biological Neural Network represent inputs in Artificial Neural Networks, cell nucleus represents \\nNodes, synapse represents Weights, and Axon represents Output.  \\n \\nRelationship between Biological neural network and artificial neural network:  \\n \\nBiological  Neural Network  Artificial Neural Network  \\nDendrites  Inputs  \\nCell nucleus  Nodes',\n",
       " 'Relationship between Biological neural network and artificial neural network:  \\n \\nBiological  Neural Network  Artificial Neural Network  \\nDendrites  Inputs  \\nCell nucleus  Nodes  \\nSynapse  Weights  \\nAxon  Output  \\n \\nAn Artificial Neural Network  in the field of  Artificial intelligence  where it attempts to mimic the network of \\nneurons makes up a human brain so  that computers will have an option to understand things and make decisions in \\na human -like manner. The artificial neural network is designed by programming computers to behave simply like \\ninterconnected brain cells.  \\n \\nThere are around 1000 billion neurons in the human brain. Each neuron has an association point somewhere in the',\n",
       " 'interconnected brain cells.  \\n \\nThere are around 1000 billion neurons in the human brain. Each neuron has an association point somewhere in the \\nrange of 1,000 and 100,000. In the human brain, data is stored in such a manner as to be distributed, and we can \\n42 \\n extract more than one piece of this data when necessary from our memo ry parallelly. We can say that the human \\nbrain is made up of incredibly amazing parallel processors.  \\n \\n \\nWe can understand the artificial neural network with an example, consider an example of a digital logic gate that \\ntakes an input and gives an output. \"OR \" gate, which takes two inputs. If one or both the inputs are \"On,\" then we \\nget \"On\" in output. If both the inputs are \"Off,\" then we get \"Off\" in output. Here the output depends upon input.',\n",
       " 'get \"On\" in output. If both the inputs are \"Off,\" then we get \"Off\" in output. Here the output depends upon input. \\nOur brain does not perform the same task. The outputs to inputs r elationship keep changing because of the neurons \\nin our brain, which are \"learning.\"  \\n \\nThe architecture of an artificial neural network:  \\n \\n \\n \\nInput Layer:  \\nAs the name suggests, it accepts inputs in several different formats provided by the programmer.  \\n \\nHidden Layer:  \\nThe hidden layer presents in -between input and output layers. It performs all the calculations to find \\nhidden features and patterns.  \\n \\nOutput Layer:  \\nThe input goes through a series of transformations using the hidden layer, which finally resul ts in output \\nthat is conveyed using this layer.',\n",
       " 'hidden features and patterns.  \\n \\nOutput Layer:  \\nThe input goes through a series of transformations using the hidden layer, which finally resul ts in output \\nthat is conveyed using this layer.  \\n \\nThe artificial neural network takes input and computes the weighted sum of the inputs and includes a bias. This \\ncomputation is represented in the form of a transfer function.  \\n \\n \\n \\nIt determines weighted total  is passed as an input to an activation function to produce the output. Activation \\nfunctions choose whether a node should fire or not. Only those who are fired make it to the output layer. There are \\ndistinctive activation functions available that can be ap plied upon the sort of task we are performing.  \\n \\nAdvantages of Artificial Neural Network (ANN)',\n",
       " \"distinctive activation functions available that can be ap plied upon the sort of task we are performing.  \\n \\nAdvantages of Artificial Neural Network (ANN)  \\n \\nParallel processing capability:  \\nArtificial neural networks have a numerical value that can perform more than one task simultaneously.  \\n \\nStoring data on the entire  network:  \\nData that is used in traditional programming is stored on the whole network, not on a database. The \\ndisappearance of a couple of pieces of data in one place doesn't prevent the network from working.  \\n \\nCapability to work with incomplete knowledge:  \\nAfter ANN training, the information may produce output even with inadequate data. The loss of \\n43 \\n performance here relies upon the significance of missing data.  \\n \\nHaving a memory distribution:\",\n",
       " \"After ANN training, the information may produce output even with inadequate data. The loss of \\n43 \\n performance here relies upon the significance of missing data.  \\n \\nHaving a memory distribution:  \\nFor ANN is to be able to adapt, it is important to determine the examples and to encourage the network \\naccording to the desired output by demonstrating these examples to the network. The succession of the network is \\ndirectly proportional to the chosen instances, and if the event can't appear to the network in all its as pects, it can \\nproduce false output.  \\n \\nHaving fault tolerance:  \\nExtortion of one or more cells of ANN does not prohibit it from generating output, and this feature makes \\nthe network fault -tolerance.  \\n \\nDisadvantages of Artificial Neural Network:\",\n",
       " 'Extortion of one or more cells of ANN does not prohibit it from generating output, and this feature makes \\nthe network fault -tolerance.  \\n \\nDisadvantages of Artificial Neural Network:  \\n \\nAssurance of p roper network structure:  \\nThere is no particular guideline for determining the structure of artificial neural networks. The appropriate \\nnetwork structure is accomplished through experience, trial, and error.  \\n \\nUnrecognized behavior of the network:  \\nIt is the most significant issue of ANN. When ANN produces a testing solution, it does not provide insight \\nconcerning why and how. It decreases trust in the network.  \\n \\nHardware dependence:  \\nArtificial neural networks need processors with parallel processing power, as per their structure. Therefore,',\n",
       " \"concerning why and how. It decreases trust in the network.  \\n \\nHardware dependence:  \\nArtificial neural networks need processors with parallel processing power, as per their structure. Therefore, \\nthe realization of the equipment is dependent.  \\n \\nDifficulty of showing the issue to the network:  \\nANNs can work with numerical data. Problems must be converted into numerical values before being \\nintroduced to ANN. The presenta tion mechanism to be resolved here will directly impact the performance of the \\nnetwork. It relies on the user's abilities.  \\n \\nThe duration of the network is unknown:  \\nThe network is reduced to a specific value of the error, and this value does not give us opt imum results.\",\n",
       " \"network. It relies on the user's abilities.  \\n \\nThe duration of the network is unknown:  \\nThe network is reduced to a specific value of the error, and this value does not give us opt imum results.  \\n“Science artificial neural networks that have steeped into the world in the mid -20th century are exponentially \\ndeveloping. In the present time, we have investigated the pros of artificial neural networks and the issues \\nencountered in the cour se of their utilization. It should not be overlooked that the cons of ANN networks, which are \\na flourishing science branch, are eliminated individually, and their pros are increasing day by day. It means that \\nartificial neural networks will turn into an ir replaceable part of our lives progressively important. ”\",\n",
       " 'artificial neural networks will turn into an ir replaceable part of our lives progressively important. ” \\n \\nHow do artificial neural networks work?  \\nArtificial Neural Network can be best represented as a weighted directed graph, where the artificial \\nneurons form the nodes. The association between the neuron s outputs and neuron inputs can be viewed as the \\ndirected edges with weights. The Artificial Neural Network receives the input signal from the external source in the \\nform of a pattern and image in the form of a vector. These inputs are then mathematically assigned by the notations \\nx(n) for every n number of inputs.  \\n 44 \\n  \\n \\nAfterward, each of the input is multiplied by its corresponding weights ( these  weights are the details',\n",
       " \"x(n) for every n number of inputs.  \\n 44 \\n  \\n \\nAfterward, each of the input is multiplied by its corresponding weights ( these  weights are the details \\nutilized by the artificial neural networks to solve a specific problem ). In general terms, these weights normally \\nrepresent the strength of the interconnection between neurons inside the artificial neural network. All the weighted  \\ninputs are summarized inside the computing unit.  \\n \\nIf the weighted sum is equal to zero, then bias is added to make the output non -zero or something else to \\nscale up to the system's response. Bias has the same input, and weight equals to 1. Here the total of weighted inputs \\ncan be in the range of 0 to positive infinity. Here, to keep the response in the limits of the desired value, a certain\",\n",
       " 'can be in the range of 0 to positive infinity. Here, to keep the response in the limits of the desired value, a certain \\nmaximum value is benchmarked, and the total of weighted inputs is passed through the activation function.  \\nThe activat ion function refers to the set of transfer functions used to achieve the desired output. There is a \\ndifferent kind of the activation function, but primarily either linear or non -linear sets of functions. Some of the \\ncommonly used sets of activation functio ns are the Binary, linear, and Tan hyperbolic sigmoidal activation \\nfunctions. Let us take a look at each of them in details:  \\n \\nBinary:  \\nIn binary activation function, the output is either a one or a 0. Here, to accomplish this, there is a threshold',\n",
       " 'functions. Let us take a look at each of them in details:  \\n \\nBinary:  \\nIn binary activation function, the output is either a one or a 0. Here, to accomplish this, there is a threshold \\nvalue set  up. If the net weighted input of neurons is more than 1, then the final output of the activation function is \\nreturned as one or else the output is returned as 0.  \\n \\nSigmoidal Hyperbolic:  \\nThe Sigmoidal Hyperbola function is generally seen as an \" S\" shaped cu rve. Here the tan hyperbolic \\nfunction is used to approximate output from the actual net input. The function is defined as:  \\nF(x) = (1/1 + exp( -????x))  \\nWhere ???? is considered the Steepness parameter.  \\n \\nTypes of Artificial Neural Network:',\n",
       " 'F(x) = (1/1 + exp( -????x))  \\nWhere ???? is considered the Steepness parameter.  \\n \\nTypes of Artificial Neural Network:  \\nThere are various t ypes of Artificial Neural Networks (ANN) depending upon the human brain neuron and \\nnetwork functions, an artificial neural network similarly performs tasks. The majority of the artificial neural \\nnetworks will have some similarities with a more complex biol ogical partner and are very effective at their expected \\ntasks. For example, segmentation or classification.  \\n \\nFeedback ANN:  \\nIn this type of ANN, the output returns into the network to accomplish the best -evolved results internally. \\nAs per the  University of Massachusetts , Lowell Centre for Atmospheric Research. The feedback networks feed',\n",
       " 'As per the  University of Massachusetts , Lowell Centre for Atmospheric Research. The feedback networks feed \\ninformation back into itself and are well suited to solve optimization issues. The Internal system error corrections \\nutilize feedback ANNs.  \\n \\nFeed -Forward ANN:  \\nA feed -forward network is a basic neural network comprising of an input layer, an output layer, and at least \\none layer of a neuron. Through assessment of its output by reviewing its input, the intensity of the network can be \\nnoticed based on group behavior of the associa ted neurons, and the output is decided. The primary advantage of this \\nnetwork is that it figures out how to evaluate and recognize input patterns.  \\n \\n45 \\n Prerequisite',\n",
       " 'network is that it figures out how to evaluate and recognize input patterns.  \\n \\n45 \\n Prerequisite  \\nNo specific expertise is needed as a prerequisite before starting this tutorial.  \\n \\nAudience  \\nOur Artificial Neural Network Tutorial is developed for beginners as well as professionals, to help them \\nunderstand the basic concept of ANNs.  \\n \\n2.3.2. PERCEPTRONS  \\nOne type of ANN system is based on a unit called a perceptron, illustrated in below Figure: A \\nperceptron takes a vector of real -valued inputs, calculates a linear combination of these inputs, then \\noutputs a 1 if the result is greater than some threshold and -1 otherwise. More precisely, given inputs xl \\nthrough xn the output o(x l, . . . , xn) computed by the perceptron is  \\n \\n \\n \\n \\n \\nwhere each',\n",
       " 'through xn the output o(x l, . . . , xn) computed by the perceptron is  \\n \\n \\n \\n \\n \\nwhere each\\nwi  is a real -valued constant, or weight, that determines the contribution of input \\nxi  to \\nthe perceptron output. Notice the quantity \\n) (0w\\uf02d   is a threshold that the weighted combination of \\ninputs \\nxw xw n n\\uf02b\\uf02b....11  must surpass in order for the perceptron to output a 1.  \\n \\nTo simplify notation, we imagine an additional constant input\\n10\\uf03dx , allowing us to write the above \\ninequ ality as\\n00\\uf03e\\uf0e5\\uf03dn\\ni iixw , or in vector form as \\noxw\\uf03e\\uf0ae\\uf0ae\\n. . For brevity, we will sometimes write the \\nperceptron function as  \\n \\n  \\n \\nLearning a perceptron involves choosing values for the weights \\nw w n,....,0 Therefore, the space H of',\n",
       " 'oxw\\uf03e\\uf0ae\\uf0ae\\n. . For brevity, we will sometimes write the \\nperceptron function as  \\n \\n  \\n \\nLearning a perceptron involves choosing values for the weights \\nw w n,....,0 Therefore, the space H of  \\ncandidate hypotheses considered in perceptron learning is the set of all possible real -valued weight \\nvectors.  \\n \\nRepresentational Power of Perceptrons:  \\n \\nWe can view the perceptron as representing a hyperplane  decision surface in the n --dimensional space \\nof instances (i.e., points). The perceptron outputs a 1 for instances lying on one side of the hyperplane \\n46 \\n and outputs a -1 for instances lying on the other side, as illustrated in Figure below The equation for this \\ndecision hyperplane is \\n0.\\uf03d\\uf0ae\\uf0ae\\nxw . Of course, some sets of positive and negative examples cannot be',\n",
       " 'decision hyperplane is \\n0.\\uf03d\\uf0ae\\uf0ae\\nxw . Of course, some sets of positive and negative examples cannot be \\nseparated by any hyperplane. Those that can be separated are called linearly separable sets of \\nexamples.  \\n \\n \\nThe decision surface represented by a two -input perceptron. (a) A set of training examples and \\nthe decision surface of a perceptron that classifies them correctly. (b) A set of training examples that is \\nnot linearly separable (i.e., that cannot be correctly classified by any s traight line). xl and x2 are the \\nPerceptron inputs. Positive examples are indicated by \"+\", negative by \"-\". The inputs are fed to multiple \\nunits, and the outputs of these units are then input to a second, final stage. One way is to represent the',\n",
       " 'units, and the outputs of these units are then input to a second, final stage. One way is to represent the \\nBoolean f unction in disjunctive normal form (i.e., as the disjunction (OR) of a set of conjunctions (ANDs) \\nof the inputs and their negations). Note that the input to an AND perceptron can be negated simply by \\nchanging the sign of the corresponding input weight. Bec ause networks of threshold units can represent \\na rich variety of functions and because single units alone cannot, we will generally be interested in \\nlearning multilayer networks of threshold units.  \\n \\nThe Perceptron Training Rule  \\nAlthough we are interested i n learning networks of many interconnected units, let us begin by',\n",
       " 'learning multilayer networks of threshold units.  \\n \\nThe Perceptron Training Rule  \\nAlthough we are interested i n learning networks of many interconnected units, let us begin by \\nunderstanding how to learn the weights for a single perceptron. Here the precise learning problem is to \\ndetermine a weight vector that causes the perceptron to produce the correct \\n1\\uf0b1  output for each of the \\ngiven training examples.  \\nSeveral algorithms are known to solve this learning problem. Here we consider two: the \\nperceptron rule and the delta rule. These two algorithms are guaranteed to converge to somewhat \\ndifferent acceptable hypotheses, under somewhat different conditions. They are important to ANNs \\nbecause they provide the basis for learning networks of many units.',\n",
       " 'different acceptable hypotheses, under somewhat different conditions. They are important to ANNs \\nbecause they provide the basis for learning networks of many units.  \\nOne way to learn an acceptable weight vector is to begin with random weights, then iteratively \\napply the perceptron to each training example, modifying the perceptron weights whenever it \\nmisclassifies an example. This process is repeated, iterating through the training examples as many \\ntimes as needed until  \\nthe perceptron classifies all training examples correctly. Weights are modified at each step according to \\nthe perceptron training rule,  which revises the weight wi associated with input xi according to the rule  \\n \\n \\nHere t is the target output for the current training example, o is the output generated by the \\n47',\n",
       " 'Here t is the target output for the current training example, o is the output generated by the \\n47 \\n perceptron, and \\n\\uf068 is a positive constant called the learning rate. The role of the learning rate is to \\nmoderate the degree to which weights are changed at each step. It is usually set to some small value \\n(e.g., 0.1) and is sometimes made to decay as the number of weight -tuning iterations increases.  \\nWhy should this update rule converge toward successful weight values? To get an intuitive feel, \\nconsider some specific cases. Suppose the training example is cor rectly classified already by the \\nperceptron. In this case, (t - o) is zero, making \\nwi\\uf044  zero, so that no weights are updated. Suppose the',\n",
       " 'perceptron. In this case, (t - o) is zero, making \\nwi\\uf044  zero, so that no weights are updated. Suppose the \\nperceptron outputs a -1, when the target output is +1. To make the perceptron output a+1 instead of -1 \\nin this case, the weights must be altered to increase the value of \\n\\uf0ae\\uf0ae\\nxw.  For example, if xi>0, then \\nincreasing wi will bring the perceptron closer to correctly classifying this example. Notice the training \\nrule will increase w, in this case, because (t - o),\\n\\uf068 , and xi are all positive. For example, if xi = .8, \\n\\uf068  = 0.1, \\nt = 1, and o = - 1, then the weight update will be \\nwi\\uf044  = \\n\\uf068 (t - o)x i = O.1(1 - (-1))0.8 = 0.16. On the \\nother hand, if t = -1 and o = 1, then weights associated with positive xi will be decreased rather than \\nincreased.',\n",
       " 'wi\\uf044  = \\n\\uf068 (t - o)x i = O.1(1 - (-1))0.8 = 0.16. On the \\nother hand, if t = -1 and o = 1, then weights associated with positive xi will be decreased rather than \\nincreased.  \\n \\nIn fact, the above learning procedure can be proven to converge within a finite number of applications \\nof the percep tron training rule to a weight vector that correctly classifies all training examples, provided \\nthe training examples are linearly separable and provided a sufficiently small \\n\\uf068  is used . If the data are \\nnot linearly separable, convergen ce is not assured.  \\n \\nGradient Descent and the Delta Rule  \\nAlthough the perceptron rule finds a successful weight vector when the training examples are \\nlinearly separable, it can fail to converge if the examples are not linearly separable. A second training',\n",
       " 'Although the perceptron rule finds a successful weight vector when the training examples are \\nlinearly separable, it can fail to converge if the examples are not linearly separable. A second training \\nrule, called the delta rule, is designed to overcome this difficulty. If the training examples are not \\nlinearly separable, the  delta rule converges toward a best -fit approximation to the target concept. The \\nkey idea behind the delta rule is  to use gradient descent to search the hypothesis space of possible \\nweight vectors to find the weights that best fit the training examples. This rule is important because \\ngradient descent provides the basis for the BACKPROPAGATION algorithm, which can lear n networks',\n",
       " 'gradient descent provides the basis for the BACKPROPAGATION algorithm, which can lear n networks \\nwith many interconnected units. It is also important because gradient descent can serve as the basis for \\nlearning algorithms that must search through hypothesis spaces containing many different types of \\ncontinuously parameterized hypotheses.  \\nThe delta training rule is best understood by considering the task of training an unthresholded \\nperceptron; that is, a linear unit for which the output o is given by  \\n \\n \\n \\nThus, a linear unit corresponds to the first stage of a perceptron, without the thresh old. \\nIn order to derive a weight learning rule for linear units, let us begin by specifying a measure for the',\n",
       " 'Thus, a linear unit corresponds to the first stage of a perceptron, without the thresh old. \\nIn order to derive a weight learning rule for linear units, let us begin by specifying a measure for the \\ntraining error of a hypothesis (weight vector), relative to the training examples. Although there are \\nmany ways to define this error, one common m easure that will turn out to be especially convenient is  \\n \\nwhere D is the set of training examples, td is the target output for training example d, and od is the \\noutput of the linear unit for training example d. By this definition, \\n\\uf028\\uf029\\uf0ae\\nwE  is simply half the squared \\n48 \\n difference between the target output td and the hear unit output od, summed over all training \\nexamples. Here we characterize E as a function of \\n\\uf028\\uf029\\uf0ae',\n",
       " '\\uf028\\uf029\\uf0ae\\nwE  is simply half the squared \\n48 \\n difference between the target output td and the hear unit output od, summed over all training \\nexamples. Here we characterize E as a function of \\n\\uf028\\uf029\\uf0ae\\nw , because the linear unit output o depends on this \\nweight vecto r. Of course E also depends on the particular set of training examples, but we assume these \\nare fixed during training, so we do not bother to write E as an explicit function of these. In particular, \\nthere we show that under certain conditions the hypothesis that minimizes E is also the most probable \\nhypothesis in H given the training data.  \\n \\n2.3.2. Multi -layer Perceptron  \\n \\nMulti -layer Perceptron (MLP)  is a supervised learning algorithm that learns a',\n",
       " 'hypothesis in H given the training data.  \\n \\n2.3.2. Multi -layer Perceptron  \\n \\nMulti -layer Perceptron (MLP)  is a supervised learning algorithm that learns a \\nfunction  f(⋅):Rm→Ro by training on a dataset, where  m is the number of dimensions for input and  o is the \\nnumber of dimensions for output. Given a set of features  X=x1,x2,...,xm and a target  y, it can learn a non -\\nlinear function approximator for either classification or regression. It is different from  logistic \\nregression, in that between the input and the output layer, there can be one or more non -linear layers, \\ncalled hidden layers. Figure  shows a one hidden layer MLP with scalar output.  \\n \\nThe leftmost layer, known as the input layer, consists of a s et of neurons  {xi|x1,x2,...,x m} representing the',\n",
       " 'called hidden layers. Figure  shows a one hidden layer MLP with scalar output.  \\n \\nThe leftmost layer, known as the input layer, consists of a s et of neurons  {xi|x1,x2,...,x m} representing the \\ninput features. Each neuron in the hidden layer transforms the values from the previous layer with a \\nweighted linear summation  w1x1+w 2x2+...+w mxm, followed by a non -linear activation function  g(⋅):R→R  - \\nlike the hyperbolic tan function. The output layer receives the values from the last hidden layer and \\ntransforms them into output values.  \\nThe module contains the public attributes  coefs_  and intercepts_ . coefs_  is a list of weight matrices, \\nwhere weight matrix  at index  i represents the weights between layer  i and layer  i+1. intercepts_  is a list',\n",
       " 'where weight matrix  at index  i represents the weights between layer  i and layer  i+1. intercepts_  is a list \\nof bias vectors, where the vector at index  i represents the bias values added to layer  i+1. \\nThe advantages of Multi -layer Perceptron are:  \\n\\uf0b7 Capability to learn non -linear  models.  \\n\\uf0b7 Capability to learn models in real -time (on -line learning) using  partial_fit . \\nThe disadvantages of Multi -layer Perceptron (MLP) include:  \\n\\uf0b7 MLP with hidden layers have a non -convex loss function where there exists more than one local \\nminimum. Therefore different random weight initializations can lead to different validation \\naccuracy.  \\n\\uf0b7 MLP requires tuning a number of hyperparameters such as the number of hidden neurons, \\nlayers, and iterations.',\n",
       " 'accuracy.  \\n\\uf0b7 MLP requires tuning a number of hyperparameters such as the number of hidden neurons, \\nlayers, and iterations.  \\n\\uf0b7 MLP is sensitive to feature scaling.  \\n \\n49 \\n 2.4. Support Vector Machines  \\n \\nSupport Vector Machine or SVM is one of the most popular Supervised Learning algorithms, \\nwhich is used for Classification as well as Regression problems. However, primarily, it is used for \\nClassification problems in Machine L earning. The goal of the SVM algorithm is to create the best line or \\ndecision boundary that can segregate n -dimensional space into classes so that we can easily put the \\nnew data point in the correct category in the future. This best decision boundary is ca lled a hyperplane.',\n",
       " 'new data point in the correct category in the future. This best decision boundary is ca lled a hyperplane.  \\nSVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are \\ncalled as support vectors, and hence algorithm is termed as Support Vector Machine. Consider the \\nbelow diagram in which there are two di fferent categories that are classified using a decision boundary \\nor hyperplane:  \\n \\n \\nExample:  SVM can be understood with the example that we have used in the KNN classifier. Suppose \\nwe see a strange cat that also has some features of dogs, so if we want a m odel that can accurately \\nidentify whether it is a cat or dog, so such a model can be created by using the SVM algorithm. We will',\n",
       " 'identify whether it is a cat or dog, so such a model can be created by using the SVM algorithm. We will \\nfirst train our model with lots of images of cats and dogs so that it can learn about different features of \\ncats and dogs, and then we test it with this strange creature. So as support vector creates a decision \\nboundary between these two data (cat and dog) and choose extreme cases (support vectors), it will see \\nthe extreme case of cat and dog. On the basis of the support vectors, it will classify it as a cat. Consider \\nthe below diagram:  \\n \\n \\nSVM algorithm can be used for  Face detection, image classification, text categorization,  etc. \\nTypes of SVM  \\nSVM can be of two types:  \\n50 \\n o Linear SVM:  Linear SVM is used for linearly separable data, which means if a dataset can be',\n",
       " 'Types of SVM  \\nSVM can be of two types:  \\n50 \\n o Linear SVM:  Linear SVM is used for linearly separable data, which means if a dataset can be \\nclassified into two classes by using a single straight line, then such data is termed as linearly \\nseparable data, and classifier is used called as Linear SVM classifier.  \\no Non -linear SVM:  Non -Linear SVM is used for non -linearl y separated data, which means if a \\ndataset cannot be classified by using a straight line, then such data is termed as non -linear data \\nand classifier used is called as Non -linear SVM classifier.  \\nHyperplane and Support Vectors in the SVM algorithm : \\n Hyperpla ne: There can be multiple lines/decision boundaries to segregate the classes in n -',\n",
       " 'Hyperplane and Support Vectors in the SVM algorithm : \\n Hyperpla ne: There can be multiple lines/decision boundaries to segregate the classes in n -\\ndimensional space, but we need to find out the best decision boundary that helps to classify the data \\npoints. This best boundary is known as the hyperplane of SVM.  \\nThe dimens ions of the hyperplane depend on the features present in the dataset, which means if there \\nare 2 features (as shown in image), then hyperplane will be a straight line. And if there are 3 features, \\nthen hyperplane will be a 2 -dimension plane.  \\nWe always create a hyperplane that has a maximum margin, which means the maximum distance \\nbetween the data points.  \\n \\nSupport Vectors:',\n",
       " 'then hyperplane will be a 2 -dimension plane.  \\nWe always create a hyperplane that has a maximum margin, which means the maximum distance \\nbetween the data points.  \\n \\nSupport Vectors:  \\nThe data points or vectors that are the closest to the hyperplane and which affect the position of the \\nhyperplane are terme d as Support Vector. Since these vectors support the hyperplane, hence called a \\nSupport vector.  How does SVM works?  \\n \\n2.4.1. Linear SVM:  \\nThe working of the SVM algorithm can be understood by using an example. Suppose we have a dataset \\nthat has two tags (gre en and blue), and the dataset has two features x1 and x2. We want a classifier \\nthat can classify the pair(x1, x2) of coordinates in either green or blue. Consider the below image:',\n",
       " 'that can classify the pair(x1, x2) of coordinates in either green or blue. Consider the below image:  \\n \\nSo as it is 2 -d space so by just using a straight line, we can easily separate these two classes. But there \\ncan be multiple lines that can separate these classes. Consider the below image:  \\n \\n51 \\n  \\n \\nHence, the SVM algorithm helps to find the best line or decision boundary; this best boundary or region \\nis called as a  hyperplane . SVM algorithm finds the closest point of the lines from both the classes. These \\npoints are called support vectors. The distance between the vectors and the hyperplane is called \\nas margin . And the goal of SVM is to maximize this margin. The  hyperplane  with ma ximum margin is \\ncalled the  optimal hyperplane .  \\n \\n2.4.2. Non -Linear SVM:',\n",
       " 'as margin . And the goal of SVM is to maximize this margin. The  hyperplane  with ma ximum margin is \\ncalled the  optimal hyperplane .  \\n \\n2.4.2. Non -Linear SVM:  \\n           If data is linearly arranged, then we can separate it by using a straight line, but for non -linear \\ndata, we cannot draw a single straight line. Consider the below image:  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nSo to separate these data points, we need to add one more dimension. For linear data, we have used two dimensions \\nx and y, so for non -linear data, we will add a third dimension z. It can be calculated as:  \\nz=x2 +y2 \\nBy adding the third dimension, the sample space will become as below image:  \\n52 \\n  \\nSo now, SVM will divide the datasets into classes in the following way. Consider the below image:',\n",
       " 'z=x2 +y2 \\nBy adding the third dimension, the sample space will become as below image:  \\n52 \\n  \\nSo now, SVM will divide the datasets into classes in the following way. Consider the below image:  \\n \\n \\nSince we are in 3 -d Space, hence it is looking like a plane parallel to the x -axis. If we convert it in 2d \\nspace with z=1, then it will become as:  \\n \\n \\nHence we get a circumference of radius 1 in case of non -linear data.  \\n \\n2.4.3. SVM Kernels  \\nIn practice, SVM algorithm is implemented with kernel that transforms an input data space into \\n53 \\n the required form. SVM uses a  technique called the kernel trick in which kernel takes a low dimensional \\ninput space and transforms it into a higher dimensional space. In simple words, kernel converts non -',\n",
       " 'input space and transforms it into a higher dimensional space. In simple words, kernel converts non -\\nseparable problems into separable problems by adding more dimensions to it. It ma kes SVM more \\npowerful, flexible and accurate. The following are some of the types of kernels used by SVM.  \\nLinear Kernel  \\nIt can be used as a dot product between any two observations. The formul a of linear kernel is as below  \\n \\nK(x,xi)=sum (x∗xi) \\n \\nFrom the abo ve formula, we can see that the product between two vectors say 𝑥 & 𝑥𝑖 is the sum of the \\nmultiplication of each pair of input values.  \\n \\n2.5. Unsupervised Machine Learning:  \\n2.5.1. Introduction to clustering  \\n \\nAs the name suggests, unsupervised learning is a machine learning technique in which models',\n",
       " '2.5. Unsupervised Machine Learning:  \\n2.5.1. Introduction to clustering  \\n \\nAs the name suggests, unsupervised learning is a machine learning technique in which models \\nare not supervised using training dataset. Instead, models itself find the hidden patterns and insights \\nfrom the given data. It can be compared to learning which takes place in the human brain while learning \\nnew  things. It can be defined as:  \\n \\n“Unsupervised learning is a type of machine learning in which models are trained using \\nunlabeled dataset and are allowed to act on that data without any supervision. ” \\n \\nUnsupervised learning cannot be directly applied to a re gression or classification problem \\nbecause unlike supervised learning, we have the input data but no corresponding output data. The goal',\n",
       " 'Unsupervised learning cannot be directly applied to a re gression or classification problem \\nbecause unlike supervised learning, we have the input data but no corresponding output data. The goal \\nof unsupervised learning is to  find the underlying structure of dataset, group that data according to \\nsimilarities, and  represent that dataset in a compressed format  \\n \\nExample:  Suppose the unsupervised learning algorithm is given an input dataset containing images of \\ndifferent types of cats and dogs. The algorithm is never trained upon the given dataset, which means it \\ndoes not have any idea about the features of the dataset. Th e task of the unsupervised learning \\nalgorithm is to identify the image features on their own. Unsupervised learning algorithm will perform',\n",
       " 'algorithm is to identify the image features on their own. Unsupervised learning algorithm will perform \\nthis task by clustering the image dataset into the groups according to similarities between images.  \\n \\n \\nWhy use Unsupe rvised Learning?  \\n54 \\n Below are some main reasons which describe the importance of Unsupervised Learning:  \\no Unsupervised learning is helpful for finding useful insights from the data.  \\no Unsupervised learning is much similar as a human learns to think by their own e xperiences, \\nwhich makes it closer to the real AI.  \\no Unsupervised learning works on unlabeled and uncategorized data which make unsupervised \\nlearning more important.  \\no In real -world, we do not always have input data with the corresponding output so to solve suc h',\n",
       " 'learning more important.  \\no In real -world, we do not always have input data with the corresponding output so to solve suc h \\ncases, we need unsupervised learning.  \\nWorking of Unsupervised Learning  \\n \\nWorking of unsupervised learning can be understood by the below diagram:  \\n \\n \\nHere, we have taken an unlabeled input data, which means it is not categorized and \\ncorresponding outputs are also not given. Now, this unlabeled input data is fed to the machine learning \\nmodel in order to train it. Firstly, it will interpret the raw data to find the hidden patterns from the data \\nand then will apply suitable algorithms such as k -means clusteri ng, Decision tree, etc.  \\nOnce it applies the suitable algorithm, the algorithm divides the data objects into groups according to',\n",
       " 'and then will apply suitable algorithms such as k -means clusteri ng, Decision tree, etc.  \\nOnce it applies the suitable algorithm, the algorithm divides the data objects into groups according to \\nthe similarities and difference between the objects.  \\nTypes of Unsupervised Learning Algorithm:  \\nThe unsupervised learning algorit hm can be further categorized into two types of problems:  \\n \\n55 \\n o Clustering : Clustering is a method of grouping the objects into clusters such that objects with \\nmost similarities remains into a group and has less or no similarities with the objects of another \\ngroup. Cluster analysis finds the commonalities between the data objects and categorizes them \\nas per the presence and absence of those commonalities.',\n",
       " 'group. Cluster analysis finds the commonalities between the data objects and categorizes them \\nas per the presence and absence of those commonalities.  \\no Association : An association rule is an unsupervised learning method which is used for finding \\nthe relations hips between variables in the large database. It determines the set of items that \\noccurs together in the dataset. Association rule makes marketing strategy more effective. Such \\nas people who buy X item (suppose a bread) are also tend to purchase Y (Butter/ Jam) item. A \\ntypical example of Association rule is Market Basket Analysis.  \\n \\n \\nUnsupervised Learning algorithms:  \\n \\nBelow is the list of some popular unsupervised learning algorithms:  \\no K-means clustering  \\no KNN (k -nearest neighbors)  \\no Hierarchal clustering',\n",
       " \"Unsupervised Learning algorithms:  \\n \\nBelow is the list of some popular unsupervised learning algorithms:  \\no K-means clustering  \\no KNN (k -nearest neighbors)  \\no Hierarchal clustering  \\no Anomaly detection  \\no Neural Networks  \\no Principle Component Analysis  \\no Independent Component Analysis  \\no Apriori algorithm  \\no Singular value decomposition  \\nAdvantages of Unsupervised Learning  \\no Unsupervised learning is used for more complex tasks as compared to supervised learning  \\nbecause, in unsupervised learning, we don't have labeled input data.  \\no Unsupervised learning is preferable as it is easy to get unlabeled data in comparison to labeled \\ndata.  \\nDisadvantages of Unsupervised Learning\",\n",
       " 'o Unsupervised learning is preferable as it is easy to get unlabeled data in comparison to labeled \\ndata.  \\nDisadvantages of Unsupervised Learning  \\no Unsupervised learning is intrinsically more difficult than supervised learning as it does not have \\ncorresponding output.  \\no The result of the unsupervised learning algorithm might be less accurate as input data is not \\nlabeled, and algorithms do not know the exact output in advance.  56 \\n Supervised Learning  Unsupervised Learning  \\nSupervised learning algorithms are trained using labeled data.  Unsupervised learning algorithms are trained using unlabeled data.  \\nSupervised learning model takes direct feedback to check if it is predicting \\ncorrect output or not.  Unsupervised learning model does not take any feedback.',\n",
       " 'Supervised learning model takes direct feedback to check if it is predicting \\ncorrect output or not.  Unsupervised learning model does not take any feedback.  \\nSupervised learning model predicts the output.  Unsupervised learning model finds the hidden patterns in data.  \\nIn supervised learning, input data is provided to the model along with the \\noutput.  In unsupervised learning, only input data is provided to the model.  \\nThe goal of supervised learning is to train the model so that it can predict \\nthe output when it is given new data.  The goal of unsupervised learning is to find the hidden patterns and \\nuseful  insights from the unknown dataset.  \\nSupervised learning needs supervision to train the model.  Unsupervised learning does not need any supervision to train the \\nmodel.',\n",
       " 'useful  insights from the unknown dataset.  \\nSupervised learning needs supervision to train the model.  Unsupervised learning does not need any supervision to train the \\nmodel.  \\nSupervised learning can be categorized \\nin Classification  and Regression  problems.  Unsupervised Learning can be classified \\nin Clustering  and Associations  problems.  \\nSupervised learning can be used for those cases where we know the input \\nas well as corresponding outputs.  Unsupervised learning can be used for those cases where we have \\nonly input data and no corresponding output data.  \\nSupervised learning model produces an accurate result.  Unsupervised learning model may give less accurate result as \\ncompared to supervised learning.',\n",
       " 'Supervised learning model produces an accurate result.  Unsupervised learning model may give less accurate result as \\ncompared to supervised learning.  \\nSupervised learning is not close to true Artificial intelligence as in this, we \\nfirst train the model for each data, and then only it can predict the correct \\noutput.  Unsupervised learning is more close to the true Artificial \\nIntelligence as it learns similarly as a child learns daily routine \\nthings by his e xperiences.  \\nIt includes various algorithms such as Linear Regression, Logistic \\nRegression, Support Vector Machine, Multi -class Classification, Decision \\ntree, Bayesian Logic, etc.  It includes various algorithms such as Clustering, KNN, and Apriori  \\nalgorithm.  \\n \\n2.5.2. K -Mean Clustering  \\n \\nk-means clustering algorithm',\n",
       " 'tree, Bayesian Logic, etc.  It includes various algorithms such as Clustering, KNN, and Apriori  \\nalgorithm.  \\n \\n2.5.2. K -Mean Clustering  \\n \\nk-means clustering algorithm  \\nOne of the most used clustering algorithm is  k-means . It allows to group the data according to the \\nexisting similarities among them in  k clusters, given as input to the algorithm. I’ll start with a simple \\nexample.  \\nLet’s imagine we have 5 objects (say 5 people) and for each of them we know two features (height and \\nweight). We want to group them into  k=2 clusters.  \\n \\nOur dataset will look like this:  57 \\n  \\nFirst of all, we have to initialize the value of the centroids for our clusters. For instance, let’s choose \\nPerson 2 and Person 3 as the two centroids  c1 and c2, so that  c1=(120,32)  and c2=(113,33) .',\n",
       " 'Person 2 and Person 3 as the two centroids  c1 and c2, so that  c1=(120,32)  and c2=(113,33) . \\nNow we compute the euclidian distance between each of the two centroids and each point in the dat a. \\nIf you did all the calculations, you should have come up with the following numbers:  \\n \\n Distance of object from  c1 Distance of object from  c2 \\nPerson 1  52.3 58.3 \\nPerson 2  0 7.1 \\nPerson 3  7.1 0 \\nPerson 4  70.4 75.4 \\nPerson 5  13.9 9.4 \\n \\nAt this point, we will assign each object  to the cluster it is closer to (that is taking the minimum between \\nthe two computed distances for each object).  \\n \\nWe can then arrange the points as follows:  \\n \\nPerson 1 → cluster 1  \\nPerson 2 → cluster 1  \\nPerson 3 → cluster 2  \\nPerson  4 → cluster 1  \\nPerson 5→ cluster 2',\n",
       " 'We can then arrange the points as follows:  \\n \\nPerson 1 → cluster 1  \\nPerson 2 → cluster 1  \\nPerson 3 → cluster 2  \\nPerson  4 → cluster 1  \\nPerson 5→ cluster 2  \\n \\nLet’s iterate, which means  to redefine the centroids by calculating the mean of the members of each of \\nthe two clusters.  \\n \\nSo c’1 = ((167+120+175)/3, (55+32+76)/3) = (154, 54.3) and  c’2 = ((113+108)/2, (33+25)/2) = (110.5, 29)  \\n \\nThen, we calculate the distances again and re -assign the points to the new centroids.  \\n \\nWe repeat this process until the centroids don’t move anymore (or the difference between them is \\nunder a certain small threshold).  \\n \\nIn our case, the result we get is gi ven in the figure below.  You can see the two different clusters labelled',\n",
       " 'under a certain small threshold).  \\n \\nIn our case, the result we get is gi ven in the figure below.  You can see the two different clusters labelled \\nwith two different colours and the position of the centroids, given by the crosses.  \\n \\n58 \\n  \\nHow to apply k -means?  \\nAs you probably already know, I’m using Python libraries to analyze my data. The  k-means  algorithm is \\nimplemented in the  scikit -learn  package. To use it, you will just need the following line in your script:  \\n \\nWhat if our data is… non -numerical?  \\n \\nAt this point, you will maybe have noticed something. The basic concept of  k-mean s stands on \\nmathematical calculations (means, euclidian distances). But what if our data is non -numerical or, in',\n",
       " 'At this point, you will maybe have noticed something. The basic concept of  k-mean s stands on \\nmathematical calculations (means, euclidian distances). But what if our data is non -numerical or, in \\nother words,  categorical ? Imagine, for instance, to have the ID code and date of birth of the five people \\nof the previous example, instead of t heir heights and weights.  \\nWe could think of transforming our categorical values in numerical values and eventually apply  k-\\nmeans . But beware:  k-means  uses numerical distances, so it could consider close two really distant \\nobjects that merely have been assi gned two close numbers.  \\n \\n \\n \\n \\n \\nk-modes  is an extension of  k-means . Instead of distances it uses  dissimilarities  (that is,',\n",
       " 'objects that merely have been assi gned two close numbers.  \\n \\n \\n \\n \\n \\nk-modes  is an extension of  k-means . Instead of distances it uses  dissimilarities  (that is, \\nquantification of the total mismatches between two objects: the smaller this number, the more similar \\nthe two objects). And instead of means, it uses  modes.  A mode is a vector of elements that minimizes \\nthe dissimilarities between the vector itself  and each object of the data. We will have as many modes as \\nthe number of clusters we required, since they act  as centroids.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n59 \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUnit III  \\nEnsemble and Probabilistic Learning  \\n \\nEnsemble   Learning:  Model Combination Schemes, Voting, Error -Correcting Output Codes,',\n",
       " 'Unit III  \\nEnsemble and Probabilistic Learning  \\n \\nEnsemble   Learning:  Model Combination Schemes, Voting, Error -Correcting Output Codes, \\nBagging: Random Forest Trees, Boosting: Adaboost, Stacking.  \\nProbabilistic Learning:  Gaussian mixture models - The Expectation -Maximization (EM) \\nAlgorithm, Information Criteria, Nearest neighbour methods - Nearest Neighbour Smoothing, \\nEfficient Distance Computations: the KD -Tree, Distance Measures.  \\n \\n3. Introduction:      \\n \\nEnsemble Learnin g \\nEnsemble learning usually produces more accurate solutions than a single model would. 60 \\n Ensemble Learning is a technique that create multiple models and then combine them them to produce',\n",
       " 'Ensemble learning usually produces more accurate solutions than a single model would. 60 \\n Ensemble Learning is a technique that create multiple models and then combine them them to produce \\nimproved results. Ensemble learning usually produces more accurate so lutions than a single model \\nwould.  \\n\\uf0b7 Ensemble learning methods is applied to regression as well as classification.  \\no Ensemble learning for regression creates multiple repressors i.e. multiple regression \\nmodels such as linear, polynomial, etc.  \\no Ensemble learn ing for classification creates multiple classifiers i.e. multiple classification \\nmodels such as logistic, decision tress, KNN, SVM, etc.  \\n \\nFigure 1: Ensemble learning view  \\nWhich components to combine?  \\n• different learning algorithms',\n",
       " 'models such as logistic, decision tress, KNN, SVM, etc.  \\n \\nFigure 1: Ensemble learning view  \\nWhich components to combine?  \\n• different learning algorithms  \\n• same learning algorithm trained in different ways  \\n• same learning algorithm trained the same way  \\n \\nThere are two steps in ensemble learning:  \\nMultiples machine learning models were generated using same or different machine learning \\nalgorithm. These are called “base models”.  The prediction perform on the basis of base models.  \\nTechniques/Methods in ensemble learning  \\nVoting, Error -Correcting Output Codes, Bagging: Random Forest Trees, Boosting: Adaboost, Stacking.  \\n3.1 Model Combination Schemes - Combining Multiple Learners  \\nWe discus sed many different learning algorithms in the previous chapters. Though these are',\n",
       " '3.1 Model Combination Schemes - Combining Multiple Learners  \\nWe discus sed many different learning algorithms in the previous chapters. Though these are \\ngenerally successful, no one single algorithm is always the most accurate. Now, we are going to discuss \\nmodels composed of multiple learners that complement each other so tha t by combining them, we \\nattain higher accuracy.  \\n \\nThere are also different ways the multiple base -learners are combined to generate the final \\noutput:  \\n \\nFigure2: General Idea - Combining Multiple Learners  \\n61 \\n  \\n \\nMultiexpert combination   \\nMultiexpert combination  methods have base -learners that work in parallel . These methods can \\nin turn be divided into two:',\n",
       " '61 \\n  \\n \\nMultiexpert combination   \\nMultiexpert combination  methods have base -learners that work in parallel . These methods can \\nin turn be divided into two:  \\n \\n\\uf0b7 In the global approach, also called learner fusion , given an input, all base -learners generate an \\noutput and all these outputs are used.  \\nExamples are voting  and stacking . \\n\\uf0b7 In the local approach, or learner selection , for example, in mixture of experts , there is a gating \\nmodel, which looks at the input and chooses one (or very few) of the learners as responsible for \\ngenerating the output.  \\n \\nMultistage combination  \\nMultistage combination methods use a serial approach where the next base -learner is trained \\nwith or tested on only the instances where the previous base -learners are not accurate enough. The',\n",
       " 'Multistage combination methods use a serial approach where the next base -learner is trained \\nwith or tested on only the instances where the previous base -learners are not accurate enough. The \\nidea is that the base -learners (or the different representations they use) are sorted in increasing \\ncomplexit y so that a complex base -learner is not used (or its complex representation is not extracted) \\nunless the preceding simpler base -learners are not confident.  \\nAn example is cascading . \\n \\nLet us say that we have L base -learners. We denote by dj(x) the predictio n of base -learner Mj given the \\narbitrary dimensional input x. In the case of multiple representations, each Mj uses a different input \\nrepresentation xj . The final prediction is calculated from the predictions of',\n",
       " 'arbitrary dimensional input x. In the case of multiple representations, each Mj uses a different input \\nrepresentation xj . The final prediction is calculated from the predictions of  \\nthe base -learners:  \\n \\ny = f (d 1, d2, . . . , dL |Φ) \\n \\nwhere f (·) is the combining function with Φ denoting its parameters.  \\n62 \\n  \\nFigure 1: Base -learners are dj and their outputs are combined using f (·). This is for a single \\noutput; in the case of classification, each base -learner has K outputs that are separately used to \\ncalculate yi, and then we choose the maximum.  Note that here all learners observe the same input; it \\nmay be the case that different learners observe different representations of the same input object or \\nevent.',\n",
       " 'may be the case that different learners observe different representations of the same input object or \\nevent.  \\n \\nWhen th ere are K outputs, for each learner there are dji(x), i = 1, . . . , K,  \\nj = 1, . . . , L , and, combining them, we also generate K values, yi, i = 1, . . . , K and then for example in \\nclassification, we choose the class with  \\nthe maximum yi value:  \\n                                    \\n \\n3.2 Voting  \\nThe simplest way to combine multiple classifiers is by voting , which corresponds to taking a \\nlinear combination of the learn  \\n \\ners, Refer figure 1.  \\n \\n \\nThis is also known as ensembles and linear opinion pools . In the sim plest case, all learners are \\ngiven equal weight and we have simple voting that corresponds to taking an average. Still, taking a',\n",
       " 'This is also known as ensembles and linear opinion pools . In the sim plest case, all learners are \\ngiven equal weight and we have simple voting that corresponds to taking an average. Still, taking a \\n(weighted) sum is only one of the possibilities and there are also other combination rules, as shown in \\ntable 1. I f the outputs are not posterior probabilities, these rules require that outputs be normalized to \\nthe same scale  \\n \\nTable 1 - Classifier combination rules  \\n \\n \\n63 \\n An example of the use of these rules is shown in table 2, which demonstrates the effects of \\ndifferent  rules. Sum rule is the most intuitive and is the most widely used in practice. Median rule is \\nmore robust to outliers; minimum and maximum rules are pessimistic and optimistic, respectively. With',\n",
       " 'more robust to outliers; minimum and maximum rules are pessimistic and optimistic, respectively. With \\nthe product rule, each learner has veto power; regardless o f the other ones, if one learner has an \\noutput of 0, the overall output goes to 0. Note that after the combination rules, yi do not necessarily \\nsum up to 1.  \\n \\nTable 2: Example of combination rules on three learners and three classes  \\n \\nIn weighted sum, dji is the vote of learner j for class Ci and wj is the weight of its vote. Simple voting is a \\nspecial case where all voters have equal weight, namely, wj = 1/L. In classification, this is called plurality \\nvoting where the class having the maximum number of vo tes is the winner.',\n",
       " 'special case where all voters have equal weight, namely, wj = 1/L. In classification, this is called plurality \\nvoting where the class having the maximum number of vo tes is the winner.  \\n \\nWhen there are two classes, this is majority voting where the winning class gets more than half of the \\nvotes. If the voters can also supply the additional information of how much they vote for each class \\n(e.g., by the posterior probabil ity), then after normalization, these can be used as weights in a weighted \\nvoting scheme. Equivalently, if dji are the class posterior probabilities, P(Ci | x,Mj ), then we can just sum \\nthem up ( wj = 1/L) and choose the class with maximum yi . \\n \\nIn the case  of regression, simple or weighted averaging or median can be used to fuse the outputs of',\n",
       " 'them up ( wj = 1/L) and choose the class with maximum yi . \\n \\nIn the case  of regression, simple or weighted averaging or median can be used to fuse the outputs of \\nbase -regressors. Median is more robust to noise than the average.  \\n \\nAnother possible way to find wj is to assess the accuracies of the learners (regressor or classifie r) on a \\nseparate validation set and use that information to compute the weights, so that we give more weights \\nto more accurate learners.  \\n \\nVoting schemes can be seen as approximations under a Bayesian framework with weights \\napproximating prior model probabi lities, and model decisions approximating model -conditional \\nlikelihoods.   \\n \\n \\n \\nSimple voting corresponds to a uniform prior. If we have a prior distribution preferring simpler models,',\n",
       " 'likelihoods.   \\n \\n \\n \\nSimple voting corresponds to a uniform prior. If we have a prior distribution preferring simpler models, \\nthis would give larger weights to them. We cannot integrate over all mode ls; we only choose a subset \\nfor which we believe P(Mj ) is high, or we can have another Bayesian step and calculate P(Ci | x,Mj ), the \\nprobability of a model given the sample, and sample high probable models from this density.  \\n \\nLet us assume that dj are ii d with expected value E[d j] and variance Var (dj ), then when we take a simple \\n64 \\n average with wj = 1/L, the expected value and variance of the output are  \\n \\n \\nWe see that the expected value does not change, so the bias does not change. But variance, and',\n",
       " '64 \\n average with wj = 1/L, the expected value and variance of the output are  \\n \\n \\nWe see that the expected value does not change, so the bias does not change. But variance, and \\ntherefore mean square error, decreases as the number of independent voters, L, increases. In the \\ngeneral case,  \\n \\n \\n \\nwhich implies that if learners are positively correlated, variance (and error) increase. We can thus view \\nusing different algor ithms and input features as efforts to decrease, if not completely eliminate, the \\npositive correlation.  \\n \\n3.3 Error -Correcting Output Codes  \\nThe Error -Correcting  Output  Codes  method is a technique that allows a multi -class classification \\nproblem to be reframed as multiple binary classification problems, allowing the use of native binary',\n",
       " 'problem to be reframed as multiple binary classification problems, allowing the use of native binary \\nclassification models to be used directly.  \\n \\nUnlike  one-vs-rest and one-vs-one methods  that offer a similar solution by dividing a multi -class \\nclassification problem into a fixed number of binary classification problems, the error -correcting output \\ncodes technique allows each class to be encoded as an arbitrary numbe r of binary classification \\nproblems. When an overdetermined representation is used, it allows the extra models to act as “error -\\ncorrection” predictions that can result in better predictive performance.  \\n \\nIn error -correcting output codes (ECOC), the main cla ssification task is defined in terms of a',\n",
       " 'correction” predictions that can result in better predictive performance.  \\n \\nIn error -correcting output codes (ECOC), the main cla ssification task is defined in terms of a \\nnumber of subtasks that are implemented by the base -learners. The idea is that the original task of \\nseparating one class from all other classes may be a difficult problem. Instead, we want to define a set \\nof simple r classification problems, each specializing in one aspect of the task, and combining these \\nsimpler classifiers, we get the final classifier.  \\n \\nBase -learners are binary classifiers having output −1/ + 1, and there is a code matrix W of K × L whose K \\nrows are the binary codes of classes in terms of the L base -learners dj. For example, if the second row of',\n",
       " 'rows are the binary codes of classes in terms of the L base -learners dj. For example, if the second row of \\nW is [−1,+1,+1,−1], this means that for us to say an instance belongs to C2, the instance should be on the \\nnegative side of d1 and d4, and on the pos itive side of d2 and d3. Similarly, the columns of the code \\nmatrix defines the task of the base -learners. For example, if the third column is [−1,+1,+1]T , we \\nunderstand that the task of the third base -learner, d3, is to separate the instances of C1 from t he \\ninstances of C2 and C3 combined. This is how we form the training set of the base -learners. For example \\nin this case, all instances labeled with C2 and C3 form X+\\n3 and instances labeled with C1 form X−\\n3, and d3 is \\ntrained so that xt ∈ X+\\n3 give output +1 and xt ∈ X−',\n",
       " 'in this case, all instances labeled with C2 and C3 form X+\\n3 and instances labeled with C1 form X−\\n3, and d3 is \\ntrained so that xt ∈ X+\\n3 give output +1 and xt ∈ X−\\n3 give output −1. \\n \\nThe code matrix thus allows us to define a polychotomy ( K > 2 classification problem) in terms of \\ndichotomies ( K = 2 classification problem), and it is a method that is applicable using any learning \\n65 \\n algorithm to implement the dichotomizer base -learners —for example, linear or multilayer perceptrons \\n(with a single output), decision trees, or SVMs whose original definit ion is for two -class problems.  \\nThe typical one discriminant per class setting corresponds to the diagonal code matrix where L = K. For \\nexample, for K = 4, \\n \\n \\nwe have',\n",
       " 'The typical one discriminant per class setting corresponds to the diagonal code matrix where L = K. For \\nexample, for K = 4, \\n \\n \\nwe have  \\n                     \\nThe problem here is that if there is an error with one of the baselea rners, there may be a \\nmisclassification because the class code words are so similar. So the approach in error -correcting codes \\nis to have L > K and increase the Hamming distance between the code words. One possibility is pairwise \\nseparation of classes wher e there is a separate baselearner to separate C i from C j, for i < j. In this case, L \\n= K(K − 1)/2 and with K = 4, the code matrix is  \\n \\n \\nwhere a 0 entry denotes “don’t care.” That is, d1 is trained to separate C 1 from C 2 and does not use the',\n",
       " '= K(K − 1)/2 and with K = 4, the code matrix is  \\n \\n \\nwhere a 0 entry denotes “don’t care.” That is, d1 is trained to separate C 1 from C 2 and does not use the \\ntraining instances belonging to the other classes. Similarly, we say that an instance belongs to C 2 if d1 = \\n−1 and d4 = d5 = +1, and we do not consider the values of d2, d3, and d6. The problem here is that L is \\nO(K2), and for large K pairwise separation may not be feasible.  \\n \\nIf we can have L high, we can just randomly generate the code matrix with −1 / + 1 and this will work \\nfine, but if we want to keep L low, we need to optimize W. The approach is to set L beforehand and \\nthen find W such that the distances between rows, and at the same time the distances between',\n",
       " 'fine, but if we want to keep L low, we need to optimize W. The approach is to set L beforehand and \\nthen find W such that the distances between rows, and at the same time the distances between \\ncolumns, are as large as possible, in terms of Hamming distance. With K classes, there are 2(K-1) − 1 \\npossible columns, namely, two -class problems. This is because K bits can b e written in 2 K different ways \\nand complements (e.g., “0101” and “1010,” from our point of view, define the same discriminant) \\ndividing the possible combinations by 2 and then subtracting 1 because a column of all 0s (or 1s) is \\nuseless. For example, when K = 4, we have  \\n \\n \\n \\nWhen K is large, for a given value of L, we look for L columns out of the 2(K-1)−1. We would like these',\n",
       " 'useless. For example, when K = 4, we have  \\n \\n \\n \\nWhen K is large, for a given value of L, we look for L columns out of the 2(K-1)−1. We would like these \\ncolumns of W to be as different as possible so that the tasks to be learned by the base -learners are as \\ndifferent from each other as possible. At the same time, we would like the rows of W to be as different \\nas possible so that we can have maximum error correction in c ase one or more base -learners fail.  \\n \\nECOC can be written as a voting scheme where the entries of W, wij , are considered as vote weights:  \\n66 \\n  \\nand then we choose the class with the highest yi . Taking a weighted sum and then choosing the \\nmaximum instead of ch ecking for an exact match allows dj to no longer need to be binary but to take a',\n",
       " 'and then we choose the class with the highest yi . Taking a weighted sum and then choosing the \\nmaximum instead of ch ecking for an exact match allows dj to no longer need to be binary but to take a \\nvalue between −1 and +1, carrying soft certainties instead of hard decisions. Note that a value pj \\nbetween 0 and 1, for example, a posterior probability, can be converted to a  value dj between −1 and +1 \\nsimply as  \\n \\nOne problem with ECOC is that because the code matrix W is set a priori, there is no guarantee that the \\nsubtasks as defined by the columns of W will be simple.  \\n \\n3.4 Bagging  \\nBootstrap aggregating, often abbreviated as  bagging, involves having each model in the \\nensemble vote with equal weight. In order to promote model variance, bagging trains each model in the',\n",
       " 'Bootstrap aggregating, often abbreviated as  bagging, involves having each model in the \\nensemble vote with equal weight. In order to promote model variance, bagging trains each model in the \\nensemble using a randomly drawn subset of the training set. As an e xample, the  random \\nforest  algorithm combines random decision trees with bagging to achieve very high classification \\naccuracy.  \\nThe simplest method of combining classifiers is known  as bagging, which stands for bootstrap \\naggregating, the statistical description of the method. This is fine if you know what a bootstrap is, but \\nfairly useless if you don’t. A bootstrap sample is a sample taken from the original dataset with \\nreplacement, so that we may get some data several times and others not at all. The bootstrap sample is',\n",
       " 'replacement, so that we may get some data several times and others not at all. The bootstrap sample is \\nthe same size as the original, and lots and lots of these samples are taken: B of them, where B is at least \\n50, and could even be in the thousands. The name bootstrap  is more popular in computer science than \\nanywhere else, since there is also a bootstrap loader, which is the first program to run when a computer \\nis turned on. It comes from the nonsensical idea of ‘picking yourself up by your bootstraps,’ which \\nmeans lif ting yourself up by your shoelaces, and is meant to imply starting from nothing.  \\nBootstrap sampling seems like a very strange thing to do. We’ve taken a perfectly good dataset,',\n",
       " 'means lif ting yourself up by your shoelaces, and is meant to imply starting from nothing.  \\nBootstrap sampling seems like a very strange thing to do. We’ve taken a perfectly good dataset, \\nmucked it up by sampling from it, which might be good if we had made a smaller dataset (since it would \\nbe faster), but we still ended up with a dataset the same size. Worse, we’ve done it lots of times. Surely \\nthis is just a way to burn up computer time without gaining anything. The benefit of it is that we will get \\nlots of learners that perform slightly differently, which is exactly what we want for an ensemble \\nmethod. Another benefit is that estimates of the accuracy of the classification function can be made',\n",
       " 'lots of learners that perform slightly differently, which is exactly what we want for an ensemble \\nmethod. Another benefit is that estimates of the accuracy of the classification function can be made \\nwithout complicated analytic work, by throwing computer resources at the p roblem (technically, \\nbagging is a variance reducing algorithm; the meaning of this will become clearer when we talk about \\nbias and variance). Having taken a set of bootstrap samples, the bagging method simply requires that \\nwe fit a model to each dataset, a nd then combine them by taking the output to be the majority vote of \\nall the classifiers. A NumPy implementation is shown next, and then we will look at a simple example.  \\n \\n# Compute bootstrap samples',\n",
       " 'all the classifiers. A NumPy implementation is shown next, and then we will look at a simple example.  \\n \\n# Compute bootstrap samples  \\nsamplePoints = np.random .randint( 0,nPoints ,(nPoints ,nSamples )) \\nclassifiers = [] \\n \\nfor i in range( nSamples ): \\nsample = [] \\nsampleTarget = [] \\nfor j in range( nPoints ): \\nsample .append( data [samplePoints [j,i]]) \\n67 \\n sampleTarget .append( targets [samplePoints [j,i]]) \\n# Train classifiers  \\nclassifiers .append( self.tree.make_tree( sample ,sampleTarget ,features )) \\n \\nThe example consists of taking the party data that was used to demonstrate the decision tree, and \\nrestricting the trees to stumps, so that they can make a classification based on just one variable',\n",
       " 'restricting the trees to stumps, so that they can make a classification based on just one variable  \\n \\nWhen we want to construct the decision tree to decide what to do in the evening, we start by listing \\neverything that we’ve done for the past few days to get a suitable dataset (here, the last ten days):  \\n \\n \\nThe output of a decision tree that uses the whole dataset for this is not su rprising: it takes the two \\nlargest classes, and separates them. However, using just stumps of trees and 20 samples, bagging can \\nseparate the data perfectly, as this output shows:  \\n \\n \\n \\n3.4.1 RANDOM FORESTS  \\nA random forest is an ensemble learning method where  multiple decision trees are constructed \\nand then they are merged to get a more accurate prediction.',\n",
       " '3.4.1 RANDOM FORESTS  \\nA random forest is an ensemble learning method where  multiple decision trees are constructed \\nand then they are merged to get a more accurate prediction.  \\nIf there is one method in machine learning that has grown in popularity over the last few years, \\nthen it is the idea of random forests. The concept has bee n around for longer than that, with several \\ndifferent people inventing variations, but the name that is most strongly attached to it is that of \\nBreiman, who also described the CART algorithm in unit 2.  \\n \\nFigure 3: Example of random forest with majority voting  \\n68 \\n  \\n \\nThe idea is largely that if one tree is good, then many trees (a forest) should be better, provided',\n",
       " 'Figure 3: Example of random forest with majority voting  \\n68 \\n  \\n \\nThe idea is largely that if one tree is good, then many trees (a forest) should be better, provided \\nthat there is enough variety between them. The most interesting thing about a random forest is the \\nways that it creates randomness from a standard  dataset. The first of the methods that it uses is the \\none that we have just seen: bagging. If we wish to create a forest then we can make the trees different \\nby training them on slightly different data, so we take bootstrap samples from the dataset for ea ch tree. \\nHowever, this isn’t enough randomness yet. The other obvious place where it is possible to add \\nrandomness is to limit the choices that the decision tree can make. At each node, a random subset of',\n",
       " 'However, this isn’t enough randomness yet. The other obvious place where it is possible to add \\nrandomness is to limit the choices that the decision tree can make. At each node, a random subset of \\nthe features is given to the tree, and it can only pick from that subset rather than from the whole set.  \\nAs well as increasing the randomness in the training of each tree, it also speeds up the training, \\nsince there are fewer features to search over at each stage. Of course, it does introduce a new \\nparamet er (how many features to consider), but the random forest does not seem to be very sensitive \\nto this parameter; in practice, a subset size that is the square root of the number of features seems to',\n",
       " 'to this parameter; in practice, a subset size that is the square root of the number of features seems to \\nbe common. The effect of these two forms of randomness is to reduce the variance without effecting \\nthe bias. Another benefit of this is that there is no need to prune the trees. There is another parameter \\nthat we don’t know how to choose yet, which is the number of trees to put into the forest. However, \\nthis is f airly easy to pick if we want optimal results: we can keep on building trees until the error stops \\ndecreasing.  \\nOnce the set of trees are trained, the output of the forest is the majority vote for classification, \\nas with the other committee methods that we  have seen, or the mean response for regression. And',\n",
       " 'Once the set of trees are trained, the output of the forest is the majority vote for classification, \\nas with the other committee methods that we  have seen, or the mean response for regression. And \\nthose are pretty much the main features needed for creating a random forest. The algorithm is given \\nnext before we see some results of using the random forest.  \\n \\nAlgorithm  \\nHere is an outline of the random  forest algorithm.  \\n1. The random forests algorithm generates many classification trees. Each tree is generated as \\nfollows:  \\na) If the number of examples in the training set is N, take a sample of N examples at \\nrandom - but with replacement, from the original data . This sample will be the training \\nset for generating the tree.',\n",
       " 'random - but with replacement, from the original data . This sample will be the training \\nset for generating the tree.  \\nb) If there are M input variables, a number m is specified such that at each node, m \\nvariables are selected at random out of the M and the best split on these m is used to \\n69 \\n split the node. The val ue of m is held constant during the generation of the various \\ntrees in the forest.  \\nc) Each tree is grown to the largest extent possible.  \\n2. To classify a new object from an input vector, put the input vector down each of the trees in the \\nforest. Each tree gives a classification, and we say the tree “votes” for that class. The forest \\nchooses the classification',\n",
       " 'forest. Each tree gives a classification, and we say the tree “votes” for that class. The forest \\nchooses the classification  \\n \\nThe implementation of this is very easy: we modify the decision to take an extra parameter, which is m, \\nthe number of features that should be used in the selection set at each stage. We will look at an \\nexample of using it shortly as a comparison to boosting.  \\n \\nLooking at the algorithm you might be able to see that it is a very unusual machine learning \\nmethod because it is embarrassingly parallel: since the t rees do not depend upon each other, you can \\nboth create and get decisions from different trees on different individual processors if you have them. \\nThis means that the random forest can run on as many processors as you have available with nearly',\n",
       " 'This means that the random forest can run on as many processors as you have available with nearly \\nlinear spe edup.  \\n \\nThere is one more nice thing to mention about random forests, which is that with a little bit of \\nprogramming effort they come with built -in test data: the bootstrap sample will miss out about 35% of \\nthe data on average, the so -called out -of-bootstra p examples. If we keep track of these datapoints then \\nthey can be used as novel samples for that particular tree, giving an estimated test error that we get \\nwithout having to use any extra datapoints.  \\n \\nThis avoids the need for cross -validation.  \\n \\nAs a brief  example of using the random forest, we start by demonstrating that the random',\n",
       " 'without having to use any extra datapoints.  \\n \\nThis avoids the need for cross -validation.  \\n \\nAs a brief  example of using the random forest, we start by demonstrating that the random \\nforest gets the correct results on the Party example that has been used in both this and the previous \\nchapters, based on 10 trees, each trained on 7 samples, and with just two l evels allowed in each tree:  \\n \\n \\n \\nAs a rather more involved example, the car evaluation dataset in the UCI Repository contains 1,728 \\nexamples aiming to classify whether or not a car is a good purchase based on six attributes. The \\nfollowing results compare a single decision tree, bagging, and a random forest with 50 trees, each based',\n",
       " 'following results compare a single decision tree, bagging, and a random forest with 50 trees, each based \\non 100 samples, and with a maximum depth of five for each tree. It can be seen that the random forest \\nis the most accurate of the three methods.  \\n \\n70 \\n  \\n \\n \\nStrengths and weaknesses  \\n \\nStrengths  \\nThe following are some of the important strengths of random forests.  \\n• It runs efficiently on large data bases.  \\n• It can handle thousands of input variables without variable deletion.  \\n• It gives estimates of what variables are important in the classifi cation.  \\n• It has an effective method for estimating missing data and maintains accuracy when a large \\nproportion of the data are missing.  \\n• Generated forests can be saved for future use on other data.',\n",
       " '• It has an effective method for estimating missing data and maintains accuracy when a large \\nproportion of the data are missing.  \\n• Generated forests can be saved for future use on other data.  \\n• Prototypes are computed that give information about the rela tion between the variables and the \\nclassification.  \\n• The capabilities of the above can be extended to unlabeled data, leading to unsupervised \\nclustering, data views and outlier detection.  \\n• It offers an experimental method for detecting variable interactions.  \\n• Random forest run times are quite fast, and they are able to deal with unbalanced and missing \\ndata.  \\n• They can handle binary features, categorical features, numerical features without any need for \\nscaling.  \\nWeaknesses',\n",
       " 'data.  \\n• They can handle binary features, categorical features, numerical features without any need for \\nscaling.  \\nWeaknesses  \\n• A weakness of random forest algorithms is  that when used for regression they cannot predict \\nbeyond the range in the training data, and that they may over -fit data sets that are particularly \\nnoisy.  \\n71 \\n • The sizes of the models created by random forests may be very large. It may take hundreds of \\nmegabyt es of memory and may be slow to evaluate.  \\n• Random forest models are black boxes that are very hard to interpret.  \\n3.5 Boosting  \\n\\uf0a7 Boosting: train next learner on mistakes made by previous learner(s)  \\n \\nIn bagging, generating complementary base -learners is left to  chance and to the unstability of the',\n",
       " '3.5 Boosting  \\n\\uf0a7 Boosting: train next learner on mistakes made by previous learner(s)  \\n \\nIn bagging, generating complementary base -learners is left to  chance and to the unstability of the \\nlearning method. In boosting, we actively try to generate complementary base -learners by training the \\nnext learner on the mistakes of the previous learners. The original boosting algorithm combines three \\nweak learners to generate a strong learner. A weak learner has error probability less than 1/2, which \\nmakes it better than random guessing on a two -class problem, and a strong learner has arbitrarily small \\nerror probability.  \\n \\nOriginal Boosting Concept  \\nGiven a large trai ning set, we randomly divide it into three. We use X1 and train d1. We then take X2',\n",
       " 'error probability.  \\n \\nOriginal Boosting Concept  \\nGiven a large trai ning set, we randomly divide it into three. We use X1 and train d1. We then take X2 \\nand feed it to d1. We take all instances misclassified by d1 and also as many instances on which d1 is \\ncorrect  \\nfrom X2, and these together form the training set of d2. We then take X3 and feed it to d1 and d2. The \\ninstances on which d1 and d2 disagree form the training set of d3. During testing, given an instance, we \\ngive it to d1 and d2; if they agree, that is the response, otherwise the response of d3 is taken as the \\noutput.  \\n \\n1. Split data X into {X 1, X2, X3} \\n2. Train d1 on X 1 \\n\\uf0a7 Test d 1 on X 2 \\n3. Train d 2 on d 1’s mistakes on X 2 (plus some right)  \\n\\uf0a7 Test d 1 and d 2 on X 3',\n",
       " 'output.  \\n \\n1. Split data X into {X 1, X2, X3} \\n2. Train d1 on X 1 \\n\\uf0a7 Test d 1 on X 2 \\n3. Train d 2 on d 1’s mistakes on X 2 (plus some right)  \\n\\uf0a7 Test d 1 and d 2 on X 3 \\n4. Train d 3 on disagreements between d 1 and d 2 \\n\\uf0a7 Testing: apply d 1 and d 2; if disagree, use d 3 \\n\\uf0a7 Drawback: need large X  \\n \\noverall system has reduced error rate, and the error rate can arbitrarily be reduced by using such \\nsystems recursively, that is, a boosting system of three models used as dj in a higher system.  \\n \\nThough it is quite successful, the disadvantage of the origin al boosting method is that it requires a very \\nlarge training sample. The sample should be divided into three and furthermore, the second and third',\n",
       " 'large training sample. The sample should be divided into three and furthermore, the second and third \\nclassifiers are only trained on a subset on which the previous ones err. So unless one has a quite large \\ntraining set, d2 and d3 will not have training  \\nsets of reasonable size.  \\n \\n3.5.1AdaBoost  \\n         Freund and Schapire (1996) proposed a variant, named AdaBoost , short for adaptive boosting, \\nthat uses the same training set over and over and thus need not be large , but the classifiers should be \\nsimple so that they do not overfit. AdaBoost can also combine an arbitrary number of baselearners, not \\nthree.  \\n \\nAdaBoost algorithm  72 \\n  \\n \\nThe idea is to modify the probabilities of drawing the instances as a function of the error . Let us say pt\\nj',\n",
       " 'three.  \\n \\nAdaBoost algorithm  72 \\n  \\n \\nThe idea is to modify the probabilities of drawing the instances as a function of the error . Let us say pt\\nj \\ndenotes the probability that the instance pair (xt, rt) is drawn to train the jth base -learner.  \\nInitially, all pt\\n1 = 1/N. Then we add new base -learners as follows, starting from j = 1:  Є j denotes the \\nerror rate of dj .  \\nAdaBoost  requires that learners are weak, that is,  Є j < 1/2,∀j;  if  not, we stop adding new base -\\nlearners. Note that this error rate is not on the original problem but on the dataset used at step j. We \\ndefine  βj = Є j /(1 −Є j) < 1, and we set pt\\nj+1 = βj pt\\nj if dj correctly classifies xt ; otherwise, pt\\nj +1 = pt\\nj.   \\nBecause pt\\nj+1  should be probabilities, there is a normalization where we divide pt',\n",
       " 'j+1 = βj pt\\nj if dj correctly classifies xt ; otherwise, pt\\nj +1 = pt\\nj.   \\nBecause pt\\nj+1  should be probabilities, there is a normalization where we divide pt\\nj+1 by  𝑡 pt\\nj+1 , so that \\nthey sum up to 1. This has the effect that the probability of a correctly classified ins tance is decreased, \\nand the probability of a misclassified instance increases. Then a new sample of the same size is drawn \\nfrom the original sample according to these modified probabilities, pt\\nj+1 with replacement, and is used to \\ntrain d j+1. \\n  \\nThis has the  effect that dj+1 focuses more on instances misclassified by dj ; that is why the base -learners \\nare chosen to be simple and not accurate, since otherwise the next training sample would contain only',\n",
       " 'are chosen to be simple and not accurate, since otherwise the next training sample would contain only \\na few outlier and noisy instances repeated many times over. For example, with decision trees, decision \\nstumps , which are t rees grown only one or two levels, are used. So it is clear that these would have bias \\nbut the decrease in variance is larger and the overall error decreases. An algorithm like the linear \\ndiscriminant has low variance, and we cannot gain by AdaBoosting lin ear discriminants.  \\n. \\n3.5.2 Stacking  - Stacked Generalization  \\nStacked generalization is a technique proposed by Wolpert (1992) that extends voting in that \\nthe way the output of the base -learners is combined need not be linear but is learned through a',\n",
       " 'Stacked generalization is a technique proposed by Wolpert (1992) that extends voting in that \\nthe way the output of the base -learners is combined need not be linear but is learned through a \\ncombin er system, f (·|Φ), which is another learner, whose parameters Φ are also trained. (see the \\nbelow given figure)  \\n \\n73 \\n  \\nFigure: In stacked generalization, the combiner is another learner and is not restricted to being a linear \\ncombination as in voting.  \\n \\ny = f (d1, d2, . . . , d L |Φ) \\n \\nThe combiner learns what the correct output is when the base -learners give a certain output \\ncombination. We cannot train the combiner function on the training data because the base -learners \\nmay be memorizing the training set; the combiner system should act ually learn how the baselearners',\n",
       " 'combination. We cannot train the combiner function on the training data because the base -learners \\nmay be memorizing the training set; the combiner system should act ually learn how the baselearners \\nmake errors. Stacking is a means of estimating and correcting for the biases of the base -learners. \\nTherefore, the combiner should be trained on data unused in training the base -learners.  \\n \\nIf f (·|w1, . . . , wL) is a linear  model with constraints, wi ≥ 0,  𝑗Wj = 1, the optimal weights can be found \\nby constrained regression, but of course we do not need to enforce this; in stacking, there is no \\nrestriction on the combiner function and unlike voting, f (·) can be nonlinear. Fo r example, it may be \\nimplemented as a multilayer perceptron with Φ its connection weights.',\n",
       " 'restriction on the combiner function and unlike voting, f (·) can be nonlinear. Fo r example, it may be \\nimplemented as a multilayer perceptron with Φ its connection weights.  \\n \\nThe outputs of the base -learners dj define a new L-dimensional space in which the output \\ndiscriminant/regression function is learned by the combiner function.  \\n \\nIn stacked generalization, we would like the base -learners to be as different as possible so that they will \\ncomplement each other, and, for this, it is best if they are based on different learning algorithms. If we \\nare combining classifiers that can generate continuous outputs, for example, posterior probabilities, it is \\nbetter that they be the combined rather than hard decisions.',\n",
       " 'are combining classifiers that can generate continuous outputs, for example, posterior probabilities, it is \\nbetter that they be the combined rather than hard decisions.  \\nWhen we compare a trained combiner as we have in stacking, with a fixed rule such as in \\nvoting, we see that both have their advanta ges: A trained rule is more flexible and may have less bias, \\nbut adds extra parameters, risks introducing variance, and needs extra time and data for training. Note \\nalso that there is no need to normalize classifier outputs before stacking.  \\n \\n3.6 Probabilis tic Learning  \\nIn machine learning , a probabilistic classifier  is a classifier  that is able to predict, given an \\nobservation of an input, a  probability distribution  over a  set of classes, rather than only outputting the',\n",
       " 'observation of an input, a  probability distribution  over a  set of classes, rather than only outputting the \\nmost likely class that the observation should belong to. Probabilistic classifiers provide classification \\nthat can be useful in its own right  or when combining classifiers into  ensembles . \\n \\nOne criticism that is often made of neural networks —especially the MLP —is that it is not clear exactly \\nwhat it is doing: while we can go and have a look at th e activations of the neurons and the weights, they \\ndon’t tell us much.  \\n74 \\n In this topic ( probabilistic classifier  ) we are going to look at methods that are based on statistics, and \\nthat are therefore more transparent, in that we can always extract and look a t the probabilities and see',\n",
       " 'that are therefore more transparent, in that we can always extract and look a t the probabilities and see \\nwhat they are, rather than having to worry about weights that have no obvious meaning.  \\n \\n \\n3.7 GAUSSIAN MIXTURE MODELS  \\nHowever, suppose that we have the same data, but without target labels. This requires \\nunsupervised learning,  Suppose that the different classes each come from their own Gaussian \\ndistribution. This is known as multi -modal data, since there is one distribution (mode) for each different \\nclass. We can’t fit one Gaussian to the data, because it doesn’t look Gaussian o verall.  \\n \\nThere is, however, something we can do. If we know how many classes there are in the data,',\n",
       " 'class. We can’t fit one Gaussian to the data, because it doesn’t look Gaussian o verall.  \\n \\nThere is, however, something we can do. If we know how many classes there are in the data, \\nthen we can try to estimate the parameters for that many Gaussians, all at once. If we don’t know, then \\nwe can try different numbers and see which one works  best. We will talk about this issue more for a \\ndifferent method (the k-means algorithm) in Unit 2. It is perfectly possible to use any other probability \\ndistribution instead of a Gaussian, but Gaussians are by far the most common choice. Then the output \\nfor any particular datapoint that is input to the algorithm will be the sum of the values expected by all \\nof the M Gaussians:',\n",
       " 'for any particular datapoint that is input to the algorithm will be the sum of the values expected by all \\nof the M Gaussians:  \\n \\n \\nwhere _(x ; μm,  𝑚) is a Gaussian function with mean μm and covariance matrix  𝑚, and the  αm are \\nweights with the constraint that  𝛼𝑚 𝑀\\n𝑚=1  =1. \\n \\nThe given figures 4 shows two examples, where the data (shown by the histograms) comes from two \\ndifferent Gaussians, and the model is computed as a sum or mixture of the two Gaussians together.  \\n \\n \\nFIGU RE 4: Histograms of training data from a mixture of two Gaussians and two fitted models, shown as \\nthe line plot. The model shown on the left fits well, but the one on the right produces two Gaussians \\nright on top of each other that do not fit the data well .',\n",
       " 'the line plot. The model shown on the left fits well, but the one on the right produces two Gaussians \\nright on top of each other that do not fit the data well . \\n \\n \\nThe figure also gives you some idea of how to use the mixture model once it has been created. The \\nprobability that input xi belongs to class m can be written as (where a hat on a variable (ˆ·) means that \\nwe are estimating the value of that variable):  \\n \\nThe problem is how to choose the weights αm. The common approach is to aim for the maximum \\n75 \\n likelihood solution (the likelihood is the conditional probability of the data given the model, and the \\nmaximum likelihood solution varies the model to maximise thi s conditional probability). In fact, it is',\n",
       " 'maximum likelihood solution varies the model to maximise thi s conditional probability). In fact, it is \\ncommon to compute the log likelihood and then to maximise that; it is guaranteed to be negative, since \\nprobabilities are all less than 1, and the logarithm spreads out the values, making the optimisation more \\neffective. The algorithm that is used is an example of a very general one known as the expectation -\\nmaximisation (or more compactly, EM) algorithm.  \\n \\n3.8 The Expectation -Maximisation (EM) Algorithm  \\nThe basic idea of the EM algorithm is that sometimes it is easie r to add extra variables that are \\nnot actually known (called hidden or latent variables) and then to maximise the function over those',\n",
       " 'The basic idea of the EM algorithm is that sometimes it is easie r to add extra variables that are \\nnot actually known (called hidden or latent variables) and then to maximise the function over those \\nvariables. This might seem to be making a problem much more complicated than it needs to be, but it \\nturns out for many pro blems that it makes finding the solution significantly easier.  \\nIn order to see how it works, we will consider the simplest interesting case of the Gaussian \\nmixture model: a combination of just two Gaussian mixtures. The assumption now is that sample from \\nthat Gaussian. If the probability of picking Gaussian one is p, then the entire model looks like this \\n(where N( μ, σ2) specifies a Gaussian distribution with mean μ and standard deviation σ):',\n",
       " 'that Gaussian. If the probability of picking Gaussian one is p, then the entire model looks like this \\n(where N( μ, σ2) specifies a Gaussian distribution with mean μ and standard deviation σ): \\n \\n \\n \\nIf the probability distribution of p is written as π, then the  probability density is:  \\n \\n \\n \\nFinding the maximum likelihood solution (actually the maximum log likelihood) to this problem \\nis then a case of computing the sum of the logarithm of Equation  over all of the training data, and \\ndifferentiating it, which would be rather difficult. Fortunately, there is a way around it. The key insight \\nthat we need is that if we knew which of the two Gaussian components the datapoint came from, then \\nthe computation would be easy. The mean and standard deviation for each component  could be',\n",
       " 'that we need is that if we knew which of the two Gaussian components the datapoint came from, then \\nthe computation would be easy. The mean and standard deviation for each component  could be \\ncomputed from the datapoints that belong to that component, and there would not be a problem. \\nAlthough we don’t know which component each datapoint came from, we can pretend we do, by \\nintroducing a new variable f. If f = 0 then the data came from  Gaussian one, if f = 1 then it came from  \\nGaussian two.  \\n \\nThis is the typical initial step of an EM algorithm: adding latent variables. Now we just need to \\nwork out how to optimise over them. This is the time when the reason for the algorithm being called',\n",
       " 'This is the typical initial step of an EM algorithm: adding latent variables. Now we just need to \\nwork out how to optimise over them. This is the time when the reason for the algorithm being called \\nexpectation -maximisation becomes clear.We don’t know much about variable f (hardly surprising, since \\nwe invented it), but we can compute its expectation (that is, the value that we ‘expect’ to see, which is \\nthe mean average) from the data:  \\n \\n \\n \\nwhere D deno tes the data. Note that since we have set f = 1 this means that we are choosing Gaussian \\ntwo.  \\n76 \\n  \\nComputing the value of this expectation is known as the E -step. Then this estimate of the \\nexpectation is maximised over the model parameters (the parameters of t he two Gaussians and the',\n",
       " '76 \\n  \\nComputing the value of this expectation is known as the E -step. Then this estimate of the \\nexpectation is maximised over the model parameters (the parameters of t he two Gaussians and the \\nmixing parameter π), the M -step. This requires differentiating the expectation with respect to each of \\nthe model parameters. These two steps are simply iterated until the algorithm converges. Note that the \\nestimate never gets any s maller, and it turns out that EM algorithms are guaranteed to reach a local \\nmaxima. To see how this looks for the two -component Gaussian mixture, we’ll take a closer look at the \\nalgorithm:  \\n \\n \\nThe trick with applying EM algorithms to problems is in identify ing the correct latent variables',\n",
       " 'algorithm:  \\n \\n \\nThe trick with applying EM algorithms to problems is in identify ing the correct latent variables \\nto include, and then simply working through the steps. They are very powerful methods for a wide \\nvariety of statistical learning problems. We are now going to turn our attention to something much \\nsimpler, which is how we ca n use information about nearby datapoints to decide on classification \\noutput. For this we don’t use a model of the data at all, but directly use the data that is available.  \\n \\n3.9 Information Criteria  \\nwe introduced the idea of a validation set, or using cross -validation if there was not enough \\ndata. However, this replaces data with computation time, as many models are trained on different \\ndatasets.',\n",
       " 'data. However, this replaces data with computation time, as many models are trained on different \\ndatasets.  \\n \\nAn alternative idea is to identify some measure that tel ls us about how well we can expect this \\ntrained model to perform. Probabilistic model selection (or “ information criteria ”) provides an \\nanalytical technique for scoring and choosing among candidate models. Models are scored both on \\ntheir performance on the  training dataset and based on the complexity of the model.There are two \\nsuch information criteria that are commonly used:  \\n \\n \\n \\nIn these equations, k is the number of parameters in the model, N is the number of training \\nexamples, and L is the best (largest)  likelihood of the model. In both cases, based on the way that they',\n",
       " 'In these equations, k is the number of parameters in the model, N is the number of training \\nexamples, and L is the best (largest)  likelihood of the model. In both cases, based on the way that they \\nare written here, the model with the largest value is taken.  \\n \\n3.10 Nearest neighbour methods  \\n77 \\n Suppose that you are in a nightclub and decide to dance. It is unlikely that you will know the \\ndance moves for the particular song that is playing, so you will probably try to work out what to do by \\nlooking at what the people close to you are doing. The first thing you could do would be just to pick the \\nperson closest to you and copy them. However, since most of the people who are in the nightclub are',\n",
       " 'person closest to you and copy them. However, since most of the people who are in the nightclub are \\nalso unlikely to know all the moves, you might decide to look at a few more people and do what most of \\nthem are doing. This is pretty much exactly the idea behind nearest neighbour methods: if we don’t \\nhave a model that describes the data, then the best thing to do is to look at similar data and choose to \\nbe in the same class as them.  \\n \\nWe have the datapoints  positioned within input space, so we just need to work out which of the \\ntraining data are close to it. This requires computing the distance to each datapoint in the training set, \\nwhich is relatively expensive: if we are in normal Euclidean space, then we have to compute d',\n",
       " 'which is relatively expensive: if we are in normal Euclidean space, then we have to compute d \\nsubtractions and d squarings (we can ignore the square root since we only want to know which points \\nare the closest, not the actual distance) and this has to be done O( N2) times. We can then identify the k \\nnearest neighbours  to the test point, and then set the class of the test point to be the most common \\none out of those for the nearest neighbours. The choice of k is not trivial. Make it too small and nearest \\nneighbour methods are sensitive to noise, too large and the accura cy reduces as points that are too far \\naway are considered. Some possible effects of changing the size of k on the decision boundary are \\nshown in below Figure 5.',\n",
       " 'away are considered. Some possible effects of changing the size of k on the decision boundary are \\nshown in below Figure 5.  \\n \\n \\nFIGURE 5:  The nearest neighbours decision boundary with left: one neighbour and right:  \\ntwo n eighbours . \\n \\nThis method suffers from the curse of dimensionality. First, as shown above, the computational \\ncosts get higher as the number of dimensions grows. This is not as bad as it might appear at first: there \\nare sets of methods such as KD -Trees (will discuss in u pcoming topics) that compute this in O( N log N) \\ntime. However, more importantly, as the number of dimensions increases, so the distance to other \\ndatapoints tends to increase. In addition, they can be far away in a variety of different directions —there',\n",
       " 'datapoints tends to increase. In addition, they can be far away in a variety of different directions —there \\nmigh t be points that are relatively close in some dimensions, but a long way in others. There are \\nmethods for dealing with these problems, known as adaptive nearest neighbour methods, and there is a \\nreference to them in the Further Reading section at the end o f the chapter.  \\n \\nThe only part of this that requires any care during the implementation is what to do when there \\nis more than one class found in the closest points, but even with that the implementation is nice and \\nsimple:  \\n \\n78 \\n  \\n \\nWe are going to look next at h ow we can use these methods for regression, before we turn to the',\n",
       " 'simple:  \\n \\n78 \\n  \\n \\nWe are going to look next at h ow we can use these methods for regression, before we turn to the \\nquestion of how to perform the distance calculations as efficiently as possible, something that is done \\nsimply but inefficiently in the code above. We will then consider briefly whether or n ot the Euclidean \\ndistance is always the most useful way to calculate distances, and what alternatives there are.  \\n \\nFor the k-nearest neighbours algorithm the bias -variance decomposition can be computed as:  \\n \\n \\nThe way to interpret this is that when k is smal l, so that there are few neighbours considered, the model \\nhas flexibility and can represent the underlying model well, but that it makes mistakes (has high',\n",
       " 'has flexibility and can represent the underlying model well, but that it makes mistakes (has high \\nvariance) because there is relatively little data. As k increases, the variance decreases, but at th e cost of \\nless flexibility and so more bias.  \\n \\n3.11 Nearest Neighbour Smoothing  \\nNearest neighbour methods can also be used for regression by returning the average value of the \\nneighbours to a point, or a spline or similar fit as the new value. The most comm on methods are known \\nas kernel smoothers, and they use a kernel (a weighting function between pairs of points) that decides \\nhow much emphasis (weight) to put onto the contribution from each datapoint according to its distance \\nfrom the input.',\n",
       " 'how much emphasis (weight) to put onto the contribution from each datapoint according to its distance \\nfrom the input.  \\n \\nHere we shall simply use two kernels that are used for smoothing. Both of these kernels are \\ndesigned to give more weight to points that are closer to the current input, with the weights decreasing \\nsmoothly to zero as they pass out of the range of the current input , with the range specified by a \\nparameter λ.  \\n \\nThey are the Epanechnikov quadratic kernel:  \\n \\n79 \\n and the tricube kernel:  \\n  \\nThe results of using these kernels are shown in below Figure 6 on a dataset that consists of the time \\nbetween eruptions (technically known as the repose) and the duration of the eruptions of Mount',\n",
       " 'The results of using these kernels are shown in below Figure 6 on a dataset that consists of the time \\nbetween eruptions (technically known as the repose) and the duration of the eruptions of Mount \\nRuapehu, the large volcano in the centre of New Zealand’s n orth island. Values of λ  of 2 and 4 were \\nused here. Picking λ  requires experimentation. Large values average over more datapoints, and \\ntherefore produce lower variance, but at the cost of higher bias.  \\n \\n \\n80 \\n  \\nFIGURE 6:  Output of the nearest neighbour method and two kernel smoothers on the data of duration \\nand repose of eruptions of Mount Ruapehu 1860 –2006.  \\n \\n3.12 Efficient Distance Computations: the KD -Tree  \\n \\nAs was mentioned above, computing the distances between all pairs of points is very',\n",
       " 'and repose of eruptions of Mount Ruapehu 1860 –2006.  \\n \\n3.12 Efficient Distance Computations: the KD -Tree  \\n \\nAs was mentioned above, computing the distances between all pairs of points is very \\ncomputationally exp ensive. Fortunately, as with many problems in computer science, designing an \\nefficient data structure can reduce the computational overhead a lot. For the problem of finding \\nnearest neighbours the data structure of choice is the KD -Tree. It has been around  since the late 1970s, \\nwhen it was devised by Friedman and Bentley, and it reduces the cost of finding a nearest neighbour to \\nO(log N) for O( N) storage. The construction of the tree is O( N log2 N), with much of the computational',\n",
       " 'O(log N) for O( N) storage. The construction of the tree is O( N log2 N), with much of the computational \\ncost being in the computation of the median, which with a naïve algorithm requires a sort and is \\ntherefore O( N log N), or can be computed with a randomised algorithm in O( N) time.  \\n \\nThe idea behind the KD -tree is very simple. You create a binary tree by choosing one dimensio n \\nat a time to split into two, and placing the line through the median of the point coordinates of that \\ndimension. The points themselves end up as leaves of the tree. Making the tree follows pretty much the \\nsame steps as usual for constructing a binary tre e: we identify a place to split into two choices, left and',\n",
       " 'same steps as usual for constructing a binary tre e: we identify a place to split into two choices, left and \\nright, and then carry on down the tree. This makes it natural to write the algorithm recursively. The \\nchoice of what to split and where is what makes the KD -tree special. Just one dimension is spli t in each \\nstep, and the position of the split is found by computing the median of the points that are to be split in \\nthat one dimension, and putting the line there. In general, the choice of which dimension to split \\n81 \\n alternates through the different choices , or it can be made randomly. The algorithm below cycles \\nthrough the possible dimensions based on the depth of the tree so far, so that in two dimensions it \\nalternates horizontal and vertical splits.',\n",
       " 'through the possible dimensions based on the depth of the tree so far, so that in two dimensions it \\nalternates horizontal and vertical splits.  \\nThe centre of the construction method is simply a recur sive function that picks the axis to split \\non, finds the median value on that axis, and separates the points according to that value, which in \\nPython can be written as:  \\n \\n \\n \\nSuppose that we had seven two -dimensional points to make a tree from: (5 , 4), (1, 6), (6, 1), (7, 5), (2, \\n7), (2, 2), (5, 8) (as plotted in Figure 7).  \\n \\nFIGURE 7:  The initial set of 2D data.  \\n \\nThe algorithm will pick the first coordinate to split on initially, and the median point here is 5, so the \\nsplit is through x = 5. Of those on th e left of the line, the median y coordinate is 6, and for those on the',\n",
       " 'split is through x = 5. Of those on th e left of the line, the median y coordinate is 6, and for those on the \\nright it is 5. At this point we have separated all the points, and so the algorithm terminates with the split \\nshown in Figure 8 and the tree shown in Figure 9.  \\n82 \\n  \\nFIGURE 8:  The splits and leaf points found by  the KD -tree. \\n \\n \\nFIGURE 9:  The KD -tree that made the splits.  \\n \\nSearching the tree is the same as any other binary tree; we are more interested in finding the nearest \\nneighbours of a test point. This is fairly easy: starti ng at the root of the tree you recurse down through \\nthe tree comparing just one dimension at a time until you find a leaf node that is in the region',\n",
       " 'the tree comparing just one dimension at a time until you find a leaf node that is in the region \\ncontaining the test point. Using the tree shown in Figure 9 we introduce the test point (3 , 5), which \\nfinds  (2, 2) as the leaf for the box that (3 , 5) is in. However, looking at Figure 10 we see that this is not \\nthe closest point at all, so we need to do some more work.  \\n \\n \\nFIGURE 10 Two test points for the example KD -tree. \\n \\n \\n83 \\n 3.13 Distance Measures  \\n \\nWe have computed the distance between points as the Euclidean distance, which is something \\nthat you learnt about in high school. However, it is not the only option, nor is it necessarily the most \\nuseful. In this section we will look at the underlying idea behind d istance calculations and possible \\nalternatives.',\n",
       " 'useful. In this section we will look at the underlying idea behind d istance calculations and possible \\nalternatives.  \\n \\nIf I were to ask you to find the distance between my house and the nearest shop, then your first \\nguess might involve taking a map of my town, locating my house and the shop, and using a ruler to \\nmeasure the distance between them. By careful application of the map scale you can now tell me how \\nfar it is. However, when I set out to buy some milk I’m liable to find that I have to walk rather further \\nthan you’ve told me, since the direct line that you measured wo uld involve walking through (or over) \\nseveral houses, and some serious fence -scaling. Your ‘as the crow flies’ distance is the shortest possible',\n",
       " 'several houses, and some serious fence -scaling. Your ‘as the crow flies’ distance is the shortest possible \\npath, and it is the straight -line, or Euclidean, distance. You can measure it on the map by just using a \\nruler,  but it essentially consists of measuring the distance in one direction (we’ll call it  \\nnorth -south) and then the distance in another direction that is perpendicular to the first (let’s call it \\neast -west) and then squaring them, adding them together, and th en taking the square root of that. \\nWriting that out, the Euclidean distance that we are all used to is:  \\n \\n \\nwhere ( x1, y1) is the location of my house in some coordinate system (say by using a GPS tracker) and \\n(x2, y2) is the location of the shop.',\n",
       " 'where ( x1, y1) is the location of my house in some coordinate system (say by using a GPS tracker) and \\n(x2, y2) is the location of the shop.  \\n \\nIf I told you that my town was laid out on a grid block system, as is common in towns that were \\nbuilt in the interval between the invention of the motor car and the invention of innovative town \\nplanners, then you would probably use a different measure. You woul d measure the distance between \\nmy house and the shop in the ‘north -south’ direction and the distance in the ‘east -west’ direction, and \\nthen add the two distances together. This would correspond to the distance I actually had to walk. It is \\noften known as t he city -block or Manhattan distance and looks like:',\n",
       " 'then add the two distances together. This would correspond to the distance I actually had to walk. It is \\noften known as t he city -block or Manhattan distance and looks like:  \\n \\nThe point of this discussion is to show that there is more than one way to measure a distance, \\nand that they can provide radically different answers. These two different distances can be seen in \\nFigure 11. Mathematically, these distance measures are known as metrics. A metric function or norm \\ntakes two inputs and gives a scalar (the distance) back, which is positive, and 0 if and only if the two \\npoints are the same, symmetric (so that the distance  \\nto the  shop is the same as the distance back), and obeys the triangle inequality, which says that the',\n",
       " 'points are the same, symmetric (so that the distance  \\nto the  shop is the same as the distance back), and obeys the triangle inequality, which says that the \\ndistance from a to b plus the distance from b to c should not be less than the direct distance from a to c. \\n \\n84 \\n  \\nFIGURE 10:  The Euclidean and city -block distances  between two points.  \\n \\nMost of the data that we are going to have to analyse lives in rather more than two dimensions. \\nFortunately, the Euclidean distance that we know about generalises very well to higher dimensions (and \\nso does the city -block metric). In fact, these two measures are both instances of a class of metrics that \\nwork in any number of dimensions. The general measure is the Minkowski metric and it is written as:',\n",
       " 'work in any number of dimensions. The general measure is the Minkowski metric and it is written as:  \\n \\n \\nIf we put k = 1 then we get the city -block distance (Equation (7.12)), and k = 2 gives the \\nEuclidean distance (Equation (7.11)). Thus, you might possibly see the Euclidean metric written as the L2 \\nnorm and the city -block distance as the L1 norm. These norms have another interesting feature. \\nRemember that we can define different ave rages of a set of numbers. If we define the average as the \\npoint that minimises the sum of the distance to every datapoint, then it turns out that the mean \\nminimises the Euclidean distance (the sum -of-squares distance), and the median minimises the L1 \\nmetr ic. \\n \\nThere are plenty of other possible metrics to choose, depending upon the dataspace. We',\n",
       " 'minimises the Euclidean distance (the sum -of-squares distance), and the median minimises the L1 \\nmetr ic. \\n \\nThere are plenty of other possible metrics to choose, depending upon the dataspace. We \\ngenerally assume that the space is flat (if it isn’t, then none of these techniques work, and we don’t \\nwant to worry about that). However, it can still be beneficia l to look at other metrics. Suppose that we \\nwant our classifier to be able to recognise images, for example of faces. We take a set of digital photos \\nof faces and use the pixel values as features. Then we use the nearest neighbour algorithm that we’ve \\njust seen to identify each face. Even if we ensure that all of the photos are taken fully face -on, there are',\n",
       " 'just seen to identify each face. Even if we ensure that all of the photos are taken fully face -on, there are \\nstill a few things that will get in the way of this method. One is that slight variations in the angle of the \\nhead (or the camera) could make a differe nce; another is that different distances between the face and \\nthe camera (scaling) will change the results; and another is that different lighting conditions will make a \\ndifference. We can try to fix all of these things in preprocessing, but there is also another alternative: \\nuse a different metric that is invariant to these changes, i.e., it does not vary as they do. The idea of \\ninvariant metrics is to find measures that ignore changes that you don’t want. So if you want to be able',\n",
       " 'invariant metrics is to find measures that ignore changes that you don’t want. So if you want to be able \\nto rotate shapes around and still recognize them, you need a metric that is invariant to rotation.  \\n \\nA common invariant metric in use for images is the tangent distance, which is an approximation \\nto the Taylor expansion in first derivatives, and works very well for small rotations  and scalings; for \\nexample, it was used to halve the final error rate on nearest neighbor classification of a set of \\nhandwritten letters. Invariant metrics are an interesting topic for further study, and there is a reference \\nfor them in the Further Reading  section if you are interested.  \\n \\n85 \\n Unit IV  \\nReinforcement Learning  and Evaluating Hypotheses',\n",
       " 'for them in the Further Reading  section if you are interested.  \\n \\n85 \\n Unit IV  \\nReinforcement Learning  and Evaluating Hypotheses  \\n \\nIntroduction, Learning Task, Q Learning, Non deterministic Rewards and actions, temporal -\\ndifference learning, Relationship to Dynamic Programming, Active reinforcement learning, \\nGeneralization in reinforcement learning.  \\nMotivation, Basics of Sampling Theory: Error Estimation and Estimating Binomial Proportions, \\nThe Binomial Distribution, Estimators, Bias, and Variance    \\n \\nReinforcement learning addresses the question of how an autonomous agent that senses and acts in its \\nenvironment can learn to choose optimal actions to achieve its goals.  \\n \\n4.1. Introduction',\n",
       " 'Reinforcement learning addresses the question of how an autonomous agent that senses and acts in its \\nenvironment can learn to choose optimal actions to achieve its goals.  \\n \\n4.1. Introduction  \\n\\uf0b7 Consider building a learning robot . The robot, or agent , has a set  of sensors to observe the state of \\nits environment, and a set of actions it can perform to alter this state.  \\n\\uf0b7 Its task is to learn a control strategy, or policy , for choosing actions that achieve its goals.  \\n\\uf0b7 The goals of the agent can be defined by a rewa rd function that assigns a numerical value to each \\ndistinct action the agent may take from each distinct state.  \\n\\uf0b7 This reward function may be built into the robot, or known only to an external teacher who',\n",
       " 'distinct action the agent may take from each distinct state.  \\n\\uf0b7 This reward function may be built into the robot, or known only to an external teacher who \\nprovides the reward value for each action performed by the robot.  \\n\\uf0b7 The task of the robot is to perform sequences of actions, observe their consequences, and learn a \\ncontrol policy.  \\n\\uf0b7 The control policy is one that, from any initial state, chooses actions that maximize the reward \\naccumulated over time by the agent.  \\nExample:  \\n\\uf0b7 A mobile robot may have sensors such as a camera and sonars, and actions such as \"move \\nforward\" and \"turn.\"  \\n\\uf0b7 The robot may have a goal of docking onto its battery charger whenever its battery level is low.',\n",
       " 'forward\" and \"turn.\"  \\n\\uf0b7 The robot may have a goal of docking onto its battery charger whenever its battery level is low.  \\n\\uf0b7 The goal of docking to the batt ery charger can be captured by assigning a positive reward (Eg., \\n+100) to state -action transitions that immediately result in a connection to the charger and a \\nreward of zero to every other state -action transition.  \\n \\nReinforcement Learning Problem  \\n\\uf0b7 An agent interacting with its environment. The agent exists in an environment described by some \\nset of possible states S.  \\n\\uf0b7 Agent perform any of a set of possible actions A. Each time it performs an action a, in some state st \\nthe agent receives a real -value d reward r, that indicates the immediate value of this state -action',\n",
       " \"the agent receives a real -value d reward r, that indicates the immediate value of this state -action \\ntransition. This produces a sequence of states si, actions ai, and immediate rewards ri as shown in \\nthe figure.  \\n\\uf0b7 The agent's task is to learn a control policy, 𝝅: S → A , that  maximizes the expected sum of these \\nrewards, with future rewards discounted exponentially by their delay.   86 \\n  \\n \\n \\nReinforcement learning problem characteristics  \\n \\n1. Delayed reward : The task of the agent is to learn a target function 𝜋 that maps from the cur rent state \\ns to the optimal action a = 𝜋 (s). In reinforcement learning, training information is not available in (s, 𝜋 \\n(s)). Instead, the trainer provides only a sequence of immediate reward values as the agent executes its\",\n",
       " '(s)). Instead, the trainer provides only a sequence of immediate reward values as the agent executes its \\nsequence of actions. The agen t, therefore, faces the problem of temporal credit assignment : \\ndetermining which of the actions in its sequence are to be credited with producing the eventual \\nrewards.  \\n \\n2. Exploration: In reinforcement learning, the agent influences the distribution of training examples by \\nthe action sequence it chooses. This raises the question of which experimentation strategy produces \\nmost effective learning. The learner faces a trade -off in choosing whether to favor exploration of \\nunknown states and actions, or exploitation of states and actions that it has already learned will yield \\nhigh reward.',\n",
       " \"unknown states and actions, or exploitation of states and actions that it has already learned will yield \\nhigh reward.  \\n \\n3. Partially observable states: The agent's sensors can perceive the entire state of the environment at \\neach time step, in many practical situations sensors provide only partial information. In such cases, the \\nagent needs to consider its previous observations together with its current sensor data when choosing \\nactions, and the best policy may be one that cho oses actions specifically to improve the observability of \\nthe environment.  \\n \\n4. Life -long learning: Robot requires to learn several related tasks within the same environment, using \\nthe same sensors. For example, a mobile robot may need to learn how to dock  on its battery charger,\",\n",
       " 'the same sensors. For example, a mobile robot may need to learn how to dock  on its battery charger, \\nhow to navigate through narrow corridors, and how to pick up output from laser printers. This setting \\nraises the possibility of using previously obtained experience or knowledge to reduce sample complexity \\nwhen learning new tasks.  \\n \\n4.2. Learning Task  \\n\\uf0b7 Consider Markov decision process (MDP) where the agent can perceive a set S of distinct states of \\nits environment and has a set A of actions that it can perform.  \\n\\uf0b7 At each discrete time step t, the agent senses the current state st , chooses a current action at, and \\nperforms it.  \\n87 \\n \\uf0b7 The environment responds by giving the agent a reward rt = r(st, at) and by producing the',\n",
       " 'performs it.  \\n87 \\n \\uf0b7 The environment responds by giving the agent a reward rt = r(st, at) and by producing the \\nsucceeding state st+l = δ(st, at). Here the functions δ(st, at) and r(st, at) depend only on the current \\nstate and ac tion, and not on earlier states or actions.  \\n \\nThe task of the agent is to learn a policy, 𝝅: S → A , for selecting its next action a, based on the current \\nobserved state st; that is, (st) = at .  \\n \\nHow shall we specify precisely which policy π we would like the agent to learn?  \\n \\n1. One approach is to require the policy that produces the greatest possible cumulative reward for the \\nrobot over time.  \\n\\uf0b7 To state this requirement more precisely, define the cumulative value Vπ (st ) achieved by following',\n",
       " 'robot over time.  \\n\\uf0b7 To state this requirement more precisely, define the cumulative value Vπ (st ) achieved by following \\nan arbitrary policy π from an arbitrary initial state st as follows:  \\n \\n \\n\\uf0b7 Where, the sequence of rewards rt+i is generated by beginning at state st and by repeatedly using \\nthe policy π to select actions.  \\n\\uf0b7 Here 0 ≤ γ ≤ 1 is a constant that determines the relative value of delayed versus immediate \\nrewards. if we set γ = 0, only the immediate reward is considered. As we set γ closer to 1, future \\nrewards are given greater emphasis relative to the immediate rewa rd.  \\n\\uf0b7 The quantity Vπ (st) is called the discounted cumulative reward achieved by policy π from initial',\n",
       " 'rewards are given greater emphasis relative to the immediate rewa rd.  \\n\\uf0b7 The quantity Vπ (st) is called the discounted cumulative reward achieved by policy π from initial \\nstate s. It is reasonable to discount future rewards relative to immediate rewards because, in many \\ncases, we prefer to obtain the reward sooner rather t han later.  \\n \\n2. Other definitions of total reward is finite horizon reward,  \\n \\n \\nConsiders the undiscounted sum of rewards over a finite number h of steps  \\n \\n3. Another approach is average reward  \\n \\n \\nConsiders the average reward per time step over the entire  lifetime of the agent.  \\n \\nWe require that the agent learn a policy π that maximizes Vπ (st) for all states s. such a policy is called \\nan optimal policy and denote it by π*  \\n88',\n",
       " 'We require that the agent learn a policy π that maximizes Vπ (st) for all states s. such a policy is called \\nan optimal policy and denote it by π*  \\n88 \\n  \\n \\nRefer the value function Vπ *(s) an optimal policy as V*(s). V*(s) gives the maximum discounted \\ncumulative reward that the agent can obtain starting from state s. \\n \\nExample:  \\nA simple grid -world environment is depicted in the diagram  \\n \\n\\uf0b7 The six grid squares in this diagram represent six possible states, or locations, for the agent.  \\n\\uf0b7 Each arrow in the diagram represents a possible action the agent can take to move from one state \\nto another.  \\n\\uf0b7 The number associated with each arrow represents  the immediate reward r(s, a) the agent receives \\nif it executes the corresponding state -action transition',\n",
       " 'to another.  \\n\\uf0b7 The number associated with each arrow represents  the immediate reward r(s, a) the agent receives \\nif it executes the corresponding state -action transition  \\n\\uf0b7 The immediate reward in this environment is defined to be zero for all state -action transitions \\nexcept for those leading into the state labelled G. T he state G as the goal state, and the agent can \\nreceive reward by entering this state.  \\n \\nOnce the states, actions, and immediate rewards are defined, choose a value for the discount factor γ, \\ndetermine the optimal policy π * and its value function V*(s).  \\n \\nLet’s choose γ = 0.9. The diagram at the bottom of the figure shows one optimal policy for this setting.  \\n \\n89',\n",
       " 'determine the optimal policy π * and its value function V*(s).  \\n \\nLet’s choose γ = 0.9. The diagram at the bottom of the figure shows one optimal policy for this setting.  \\n \\n89 \\n  \\n \\nValues of V*(s) and Q(s, a) follow from r(s, a), and the discount factor γ = 0.9. An optimal policy, \\ncorresponding to actions with maximal Q values, is also shown.  \\n \\nThe discounted future reward from the bottom centre state is  \\n0+ γ 100+ γ2 0+ γ3 0+... = 9 0 \\n4.1.3. Q LEARNING  \\nHow can an agent learn an optimal policy π * for an arbitrary environment?  \\nThe training information available to the learner is the sequence of immediate rewards r(si,ai) \\nfor i  = 0, 1,2, . . . . Given this kind of training information it is easier to learn a numerical evaluation',\n",
       " 'for i  = 0, 1,2, . . . . Given this kind of training information it is easier to learn a numerical evaluation \\nfunction defined over states and actions, then implement the optimal policy in terms of this evaluation \\nfunction.  \\n \\nWhat evaluation function should the agent attempt to learn?  \\nOne obvious choice is V*. The agent should prefer state sl over state s2 whenever V*(sl) > V*(s2), \\nbecause the cumulative future reward will be greater from sl  \\nThe optimal action in state s is the action a that maximizes the sum o f the immediate reward r(s, a) plus \\nthe value V* of the immediate successor state, discounted by γ.  \\n \\n \\n \\n4.1.3.1. The Q Function  \\nThe value of Evaluation function Q(s, a) is the reward received immediately upon executing',\n",
       " 'the value V* of the immediate successor state, discounted by γ.  \\n \\n \\n \\n4.1.3.1. The Q Function  \\nThe value of Evaluation function Q(s, a) is the reward received immediately upon executing \\naction a from state s, plus the value (discounted by γ ) of following the optimal policy thereafter  \\n \\n \\n90 \\n  \\nRewrite Equation (3) in terms of Q(s, a) as  \\n \\n \\nEquation (5) makes clear, it need only consider each available action a in its current state s and choose \\nthe action that maximizes Q(s, a) .  \\n \\n4.1.3.2. An Algorithm for Learning Q  \\n\\uf0b7 Learning the Q function corresponds to learning the optimal policy .  \\n\\uf0b7 The key problem is finding a reliable way to estimate training values for Q, given only a sequence of',\n",
       " \"\\uf0b7 Learning the Q function corresponds to learning the optimal policy .  \\n\\uf0b7 The key problem is finding a reliable way to estimate training values for Q, given only a sequence of \\nimmediate rewards r spread out over time. This can be accomplished through iterative \\napproximation  \\n \\nRewriting Equation  \\n \\n \\n \\n \\n \\n \\n \\n \\nQ learning algorithm:  \\n \\n \\n\\uf0b7 Q learning algorithm assuming deterministic rewards and actions. The discount factor γ may be any \\nconstant such that 0 ≤ γ < 1  \\n91 \\n \\uf0b7 𝑄̂ to refer to the learner's estimate, or hypothesis, of the actual Q function  \\n \\n4.1.3.2. An Illustrative Example  \\n \\n\\uf0b7 To illustrate the operation of the Q learning algorithm, consider a single action taken by an agent, \\nand the corresponding refinement to 𝑄̂ shown in below figure\",\n",
       " '\\uf0b7 To illustrate the operation of the Q learning algorithm, consider a single action taken by an agent, \\nand the corresponding refinement to 𝑄̂ shown in below figure  \\n \\n\\uf0b7 The agent moves one cell to the right in its grid world and receives an immediate reward of zero for \\nthis transition.  \\n\\uf0b7 Apply the training rule of Equation  \\n \\nto refine its estimate Q for the state -action transition it just executed.  \\n \\n\\uf0b7 According to the trai ning rule, the new 𝑄̂ estimate for this transition is the sum of the received \\nreward (zero) and the highest 𝑄̂ value associated with the resulting state (100), discounted by γ (.9).   \\n \\n \\n4.1.3.3. Convergence  \\nWill the Q Learning Algorithm converge toward a Q equal to the true Q function?  \\nYes, under certain conditions.',\n",
       " '4.1.3.3. Convergence  \\nWill the Q Learning Algorithm converge toward a Q equal to the true Q function?  \\nYes, under certain conditions.  \\n1. Assume the system is a deterministic MDP.  \\n2. Assume the immediate reward values are bounded; that is, there exists some positive constant c such \\nthat for all states s and actions a, | r(s, a)| < c  \\n3. Assume the agent selects actions in such a fashion that it visits every possible state -action pair \\ninfinitely often  \\n \\n92 \\n  \\n \\n \\n \\n \\n \\n \\n \\n93 \\n  \\n \\n \\n4.1.3.4. Experimentation Strategies  \\nThe Q learning algorithm does not specify how actions are chosen by the agent.  \\n\\uf0b7 One obvious strategy would be for the agent in state s to select the action a that maximizes 𝑄̂(s, a), \\nthereby exploiting its current approximation 𝑄̂.',\n",
       " '\\uf0b7 One obvious strategy would be for the agent in state s to select the action a that maximizes 𝑄̂(s, a), \\nthereby exploiting its current approximation 𝑄̂.  \\n\\uf0b7 However, with this strategy the agent runs the risk that it will overcommit to actions that are \\nfound during early training to have high Q values, while failing to explore other actions that have \\neven higher values.  \\n\\uf0b7 For this reason, Q learning uses a prob abilistic approach to selecting actions. Actions with higher 𝑄̂ \\nvalues are assigned higher probabilities, but every action is assigned a nonzero probability.  \\n\\uf0b7 One way to assign such probabilities is   \\n \\n \\n\\uf0b7 Where, P(ai |s) is the probability of selecting action ai, given that the agent is in state s, and k > 0 is',\n",
       " '\\uf0b7 One way to assign such probabilities is   \\n \\n \\n\\uf0b7 Where, P(ai |s) is the probability of selecting action ai, given that the agent is in state s, and k > 0 is \\na constant that determines how strongly the selection favors actions with high 𝑄̂ values  \\n4.2. Evaluating Hypotheses  \\n4.2.1. Motivation  \\nIt is important to evaluate the performance of learned hypotheses as precisely as possible.  \\n\\uf0b7 One reason is simply to understand whether to use the hypothesis.  \\n\\uf0b7 A second reason is that evaluating hypotheses is an integral component of many learning \\nmethods.  \\n \\nTwo key difficulties aris e while learning a hypothesis and estimating its future accuracy given only a \\nlimited set of data:',\n",
       " 'methods.  \\n \\nTwo key difficulties aris e while learning a hypothesis and estimating its future accuracy given only a \\nlimited set of data:  \\n \\n1. Bias in the estimate . The observed accuracy of the learned hypothesis over the training examples is \\noften a poor estimator of its accuracy over future e xamples. Because the learned hypothesis was \\nderived from these examples, they will typically provide an optimistically biased estimate of hypothesis \\naccuracy over future examples. This is especially likely when the learner considers a very rich hypothesis \\nspace, enabling it to overfit the training examples. To obtain an unbiased estimate of future accuracy, \\ntest the hypothesis on some set of test examples chosen independently of the training examples and \\n94 \\n the hypothesis.',\n",
       " 'test the hypothesis on some set of test examples chosen independently of the training examples and \\n94 \\n the hypothesis.  \\n \\n2. Variance in the estimate. Even i f the hypothesis accuracy is measured over an unbiased set of test \\nexamples independent of the training examples, the measured accuracy can still vary from the true \\naccuracy, depending on the makeup of the particular set of test examples. The smaller the s et of test \\nexamples, the greater the expected variance.  \\n \\n4.2.2. Estimating Hypothesis Accuracy  \\n \\nSample Error –  \\nThe sample error of a hypothesis with respect to some sample S of instances drawn from X is the \\nfraction of S that it misclassifies.  \\n \\nDefinition: The sample error ( errors(h) ) of hypothesis h with respect to target function f and data \\nsample S is',\n",
       " 'fraction of S that it misclassifies.  \\n \\nDefinition: The sample error ( errors(h) ) of hypothesis h with respect to target function f and data \\nsample S is  \\n \\n \\n \\nWhere n is the number of examples in S, and the quantity δ(f(x), h(x)) is 1 if f (x) ≠ h(x), and 0 \\notherwise.  \\nTrue Error –  \\nThe true error of a hypothesis is the probability that it will misclassify a single randomly drawn \\ninstance from the distribution D.  \\nDefinition: The true error (error D (h)) of hypothesis h with respect to target function f and \\ndistribution D, is the proba bility that h will misclassify an instance drawn at random according to D. \\n \\n \\nConfidence Intervals for Discrete -Valued Hypotheses  \\nSuppose we wish to estimate the true error for some discrete valued hypothesis h, based on its',\n",
       " 'Confidence Intervals for Discrete -Valued Hypotheses  \\nSuppose we wish to estimate the true error for some discrete valued hypothesis h, based on its \\nobserved sample error over a s ample S, where  \\n\\uf0b7 The sample S contains n examples drawn independent of one another, and independent of h, \\naccording to the probability distribution D  \\n\\uf0b7 n ≥ 30  \\n\\uf0b7 Hypothesis h commits r errors over these n examples (i.e., errors (h) = r/n).  \\n \\nUnder these condit ions, statistical theory allows to make the following assertions:  \\n1. Given no other information, the most probable value of error D (h) is errors(h)  \\n2. With approximately 95% probability , the true error error D (h) lies in the interval  \\n \\n95 \\n  \\nExample:',\n",
       " '1. Given no other information, the most probable value of error D (h) is errors(h)  \\n2. With approximately 95% probability , the true error error D (h) lies in the interval  \\n \\n95 \\n  \\nExample:  \\nSuppose the data sample S contains n = 40 examples and that hypothesis h commits r = 12 errors over \\nthis data.  \\n\\uf0b7 The sample error is errors(h) = r/n = 12/40 = 0.30  \\n\\uf0b7 Given no other information, true error is error D (h) = errors(h), i.e., error D (h) = 0.30  \\n\\uf0b7 With the 95% confidence interval estimate for error D (h).  \\n \\n \\n \\n= 0.30 ± (1.96 * 0.07)  \\n= 0.30 ± 0.14  \\n3. A different constant, ZN, is used to calculate the N% confidence interval . The general expression for \\napproximate N% confidence intervals for error D (h) is  \\n \\n \\n \\nWhere,',\n",
       " '= 0.30 ± 0.14  \\n3. A different constant, ZN, is used to calculate the N% confidence interval . The general expression for \\napproximate N% confidence intervals for error D (h) is  \\n \\n \\n \\nWhere,  \\n \\n \\nThe above equation describes how to calculate the confidence intervals, or error bars, for estimates of \\nerror D (h) that are based on errors(h)  \\n \\nExample:  \\nSuppose the data sample S contains n = 40 examples and that hypothesis h commits r = 12 errors over \\nthis data.  \\n\\uf0b7 The sample error is errors(h) = r/n = 12/40 = 0.30  \\n\\uf0b7 With the 68% confidence interval estimate for error D (h).  \\n \\n96 \\n  \\n= 0.30 ± (1.00 * 0.07)  \\n= 0.30  ± 0.07  \\n \\n4.2.3. Basics of Sampling Theory  \\n4.2.3.1. Error Estimation and Estimating Binomial Proportions',\n",
       " '96 \\n  \\n= 0.30 ± (1.00 * 0.07)  \\n= 0.30  ± 0.07  \\n \\n4.2.3. Basics of Sampling Theory  \\n4.2.3.1. Error Estimation and Estimating Binomial Proportions  \\n\\uf0b7 Collect a random sample S of n independently drawn instances from the distribution D, and then \\nmeasure the sample error errors( h). Repeat this experiment many times, each time drawing a \\ndifferent random sample Si of size n, we would expect to observe different values for the various \\nerrorsi(h), depending on random differences in the makeup of the various Si. We say that errorsi(h) , \\nthe outcome of the ith such experiment, is a random variable .  \\n\\uf0b7 Imagine that we were to run k random experiments, measuring the random variables errors1(h), \\nerrors2(h) . . . errorssk(h) and plotted a histogram displaying the frequency with which each',\n",
       " '\\uf0b7 Imagine that we were to run k random experiments, measuring the random variables errors1(h), \\nerrors2(h) . . . errorssk(h) and plotted a histogram displaying the frequency with which each \\nposs ible error value is observed.  \\n\\uf0b7 As k grows, the histogram would approach a particular probability distribution called the Binomial \\ndistribution which is shown in below figure.  \\n \\nA Binomial distribution is defined by the probability function  \\n \\n \\n \\nIf the random variable X follows a Binomial distribution, then:  \\n\\uf0b7 The probability Pr(X = r) that X will take on the value r is given by P(r)  \\n \\n97 \\n  \\n \\n4.2.3.2. The Binomial Distribution  \\nConsider the following problem for better understanding of Binomial Distribution',\n",
       " '97 \\n  \\n \\n4.2.3.2. The Binomial Distribution  \\nConsider the following problem for better understanding of Binomial Distribution  \\n\\uf0b7 Given a worn and bent coin and estimate the probability that the coin will turn up heads when \\ntossed.  \\n\\uf0b7 Unknown probability of heads p. Toss the coin n times and record the nu mber of times r that it \\nturns up heads.  \\nEstimate of p = r / n  \\n\\uf0b7 If the experiment were rerun , generating a new set of n coin tosses, we might expect the \\nnumber of heads r to vary somewhat from the value measured in the first experiment, yielding \\na somewhat different estimate for p.  \\n\\uf0b7 The Binomial distribution describes for each possible value of r (i.e., from 0 to n), the probability \\nof observing exactly r heads given a sample of n independent tosses of a coin whose true',\n",
       " '\\uf0b7 The Binomial distribution describes for each possible value of r (i.e., from 0 to n), the probability \\nof observing exactly r heads given a sample of n independent tosses of a coin whose true \\nprobability of heads is p.  \\n \\nThe general setting to which the Binomial distribution applies is:  \\n1. There is a base experiment (e.g., toss of the coin) whose outcome can be described by a random \\nvariable ‘Y’. The random variable Y can take on two possible values (e.g., Y = 1 if heads,  Y = 0 if tails).  \\n2. The probability that Y = 1 on any single trial of the base experiment is given by some constant p, \\nindependent of the outcome of any other experiment. The probability that Y = 0 is therefore (1 - p). \\nTypically, p is not known in advan ce, and the problem is to estimate it.',\n",
       " 'independent of the outcome of any other experiment. The probability that Y = 0 is therefore (1 - p). \\nTypically, p is not known in advan ce, and the problem is to estimate it.  \\n3. A series of n independent trials of the underlying experiment is performed (e.g., n independent coin \\ntosses), producing the sequence of independent, identically distributed random variables Y1, Y2, . . . , \\nYn. Let  R denote the number of trials for which Yi = 1 in this series of n experiments  \\n \\n4. The probability that the random variable R will take on a specific value r (e.g., the probability of \\nobserving exactly r heads) is given by the Binomial distribution  \\n \\n98 \\n  \\n \\nMean, Variance and Standard Deviation',\n",
       " 'observing exactly r heads) is given by the Binomial distribution  \\n \\n98 \\n  \\n \\nMean, Variance and Standard Deviation  \\nThe Mean (expected value) is the average of the values taken on by repeatedly sampling the random \\nvariable  \\n \\nDefinition: Consider a random variable Y that takes on the possible values y1, . . . yn. The expected \\nvalue (Mean) of Y, E[Y], is  \\n \\nThe Variance captures how far the random variable is expected to vary from its mean value.  \\nDefinition: The variance of a random variable Y, Var[Y], is  \\n \\nThe variance describes the expected squared error in using a single observat ion of Y to estimate its \\nmean E[Y].  \\n \\nThe square root of the variance is called the standard deviation of Y, denoted σy  \\n \\nDefinition: The standard deviation of a random variable Y, σy, is',\n",
       " 'mean E[Y].  \\n \\nThe square root of the variance is called the standard deviation of Y, denoted σy  \\n \\nDefinition: The standard deviation of a random variable Y, σy, is  \\n \\nIn case the random variable Y is governed by a Binomial distribution , then the Mean, Variance and standard \\ndeviation are given by  \\n \\n \\n4.2.3.3. Estimators, Bias, and Variance  \\nLet us describe errors(h) and errorD(h) using the terms in Equation (1) defining the Binomial \\ndistribution. We then have  \\n \\nWhere,  \\n\\uf0b7 n is the number of instances in the sample S,  \\n99 \\n \\uf0b7 r is the number of instances from S misclassified by h  \\n\\uf0b7 p is the probability of misclassifying a single instance drawn from D  \\n \\n\\uf0b7 Estimator:  \\n \\nerrors(h) an estimator for the true error errorD(h): An estimator is any random variable used to',\n",
       " '\\uf0b7 p is the probability of misclassifying a single instance drawn from D  \\n \\n\\uf0b7 Estimator:  \\n \\nerrors(h) an estimator for the true error errorD(h): An estimator is any random variable used to \\nestimate some parameter of the underlying population from which the sample is drawn  \\n\\uf0b7 Estimation bias: is the difference between the expected value of the estimator and the true value of \\nthe parameter.  \\n \\nDefinition: The estimation bias of an estimator Y for an arbitrary parameter p is  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nUNIT V  \\nGenetic Algorithms  \\n \\nMotivation, Genetic Algorithms: Representing Hypotheses, Genetic Operator, Fitness Function \\nand Selection, An Illustrative Example, Hypothesis Space Search, Genetic Programming, Models \\n100',\n",
       " 'Motivation, Genetic Algorithms: Representing Hypotheses, Genetic Operator, Fitness Function \\nand Selection, An Illustrative Example, Hypothesis Space Search, Genetic Programming, Models \\n100 \\n of Evolution and Learning: Lamarkian Evolution, Baldwin Effect, Paralle lizing Genetic \\nAlgorithms.  \\n \\n5.1. Motivation  \\nGenetic algorithms (GAS) provide a learning method motivated by an analogy to biological \\nevolution. Rather than search from general -to-specific hypotheses, or from simple -to-complex, GAS \\ngenerate successor hypotheses by repeatedly mutating and recombining parts of the best currently \\nknown hypotheses. At each step, a collection of hypotheses called the current population is updated by',\n",
       " 'known hypotheses. At each step, a collection of hypotheses called the current population is updated by \\nreplacing some fraction of the population by offspring of the most fit current hypotheses. The process \\nforms a generate -and-test beam -search of hypotheses, in which variants of the best current hypotheses \\nare most likely to be considered next. The popularity of GAS is motivated by a number of factors \\nincluding:  \\n\\uf0b7 Evolution is known to be a successful, robust method for adaptation within biological systems.  \\n\\uf0b7 GAS can search spaces of hypotheses containing complex interacting parts, where the impact of \\neach part on overall hypothesis fitness may be difficult to model.  \\n\\uf0b7 Genetic algorithms are easily parallelized and can take advantage of  the decreasing costs of',\n",
       " 'each part on overall hypothesis fitness may be difficult to model.  \\n\\uf0b7 Genetic algorithms are easily parallelized and can take advantage of  the decreasing costs of \\npowerful computer hardware.  \\n \\n9.2 Genetic Algorithms  \\nThe problem addressed by GAS is to search a space of candidate hypotheses to identify the best \\nhypothesis. In GAS the \"best hypothesis\" is defined as the one that optimizes a pred efined numerical \\nmeasure for the problem at hand, called b the hypothesis fitness . For example, if the learning task is the \\nproblem of approximating an unknown function given training examples of its input and output, then \\nfitness could be defined as the a ccuracy of the hypothesis over this training data. If the task is to learn a',\n",
       " 'fitness could be defined as the a ccuracy of the hypothesis over this training data. If the task is to learn a \\nstrategy for playing chess, fitness could be defined as the number of games won by the individual when \\nplaying against other individuals in the current population.  \\nAlthough differ ent implementations of genetic algorithms vary in their details, they typically share the \\nfollowing structure: The algorithm operates by iteratively updating a pool of hypotheses, called the \\npopulation. On each iteration, all members of the population are evaluated according to the fitness \\nfunction. A new population is then generated by probabilistically selecting the fit individuals from the \\ncurrent population. Some of these selected individuals are carried forward into the next generation',\n",
       " 'current population. Some of these selected individuals are carried forward into the next generation \\npopulation intac t. Others are used as the basis for creating new offspring individuals by applying genetic \\noperations such as crossover and mutation.  101 \\n  \\n \\n \\nThe inputs to this algorithm include the fitness function for ranking candidate hypotheses, a \\nthreshold defining an acceptable level of fitness for terminating the algorithm, the size of the \\npopulation to be maintained, and parameters that determine how su ccessor populations are to be \\ngenerated: the fraction of the population to be replaced at each generation and the mutation rate. \\nNotice in this algorithm each iteration through the main loop produces a new generation of hypotheses',\n",
       " 'Notice in this algorithm each iteration through the main loop produces a new generation of hypotheses \\nbased on the current popu lation. First, a certain number of hypotheses from the current population are \\nselected for inclusion in the next generation. These are selected probabilistically, where the probability \\nof selecting hypothesis hi is given by  \\n \\nThus, the probability that a hypothesis will be selected is proportional to its own fitness and is \\ninversely proportional to the fitness of the other competing hypotheses in the current population.  \\nOnce these members of the current generation have been selected for inclusion in the ne xt generation \\npopulation, additional members are generated using a crossover operation. Crossover, defined in detail',\n",
       " 'population, additional members are generated using a crossover operation. Crossover, defined in detail \\nin the next section, takes two parent hypotheses from the current generation and creates two offspring \\nhypotheses  \\nby recombining portions o f both parents. The parent hypotheses are chosen probabilistically from the \\ncurrent population, again using the probability function given by Equation (9.1). After new members \\nhave been created by this crossover operation, the new generation population now  contains the \\n102 \\n desired number of members. At this point, a certain fraction m of these members are chosen at \\nrandom, and  random mutations all performed to alter these members.  \\n \\n \\nThis GA algorithm thus performs a randomized, parallel beam search for hypothes es that perform well',\n",
       " 'random, and  random mutations all performed to alter these members.  \\n \\n \\nThis GA algorithm thus performs a randomized, parallel beam search for hypothes es that perform well \\naccording to the fitness function. In the following subsections, we describe in more detail the \\nrepresentation of hypotheses and genetic operators used in this algorithm.  \\n \\nRepresenting Hypotheses  \\nHypotheses in GAS are often represented  by bit strings, so that they can be easily manipulated \\nby genetic operators such as mutation and crossover. The hypotheses represented by these bit strings \\ncan be quite complex. For example, sets of if -then rules can easily be represented in this way, by \\nchoosing an encoding of rules that allocates specific substrings for each rule precondition and \\npostcondition.',\n",
       " 'choosing an encoding of rules that allocates specific substrings for each rule precondition and \\npostcondition.  \\nTo see how if -then rules can be encoded by bit strings, .first consider how we might use a bit \\nstring to describe a constraint on the value of a single attribute. To pick an example, consider the \\nattribute Outlook, which can take on any of the three values Sunny, Overcast, or Rain. One obvious way \\nto represent a constraint on Outlook is to use a bit string of length three, in which each bit position \\ncorresponds to one of its three possible values. Placing a 1 in some position indicates that the attribute \\nis allowed to take on the corresponding value. For example, the string 010 represe nts the constraint',\n",
       " \"is allowed to take on the corresponding value. For example, the string 010 represe nts the constraint \\nthat Outlook must take on the second of these values, or Outlook = Overcast. Similarly, the string 011 \\nrepresents the more general constraint that allows two possible values, or (Outlook = Overcast v Rain). \\nNote 11 1 represents the most general possible constraint, indicating that we don't care which of its \\npossible values the attribute takes on.  \\nGiven this method for representing constraints on a single attribute, conjunctions of constraints \\non multiple attributes can easily be represent ed by concatenating the corresponding bit strings. For \\nexample, consider a second attribute, Wind, that can take on the value Strong or Weak. A rule \\nprecondition such as\",\n",
       " 'example, consider a second attribute, Wind, that can take on the value Strong or Weak. A rule \\nprecondition such as  \\n \\n(Outlook = Overcast ^Rain) A (Wind = Strong)  \\n \\ncan then be represented by the followi ng bit string of length five:  \\n \\n    Outlook    Wind  \\n01 1       10 \\n \\nRule postconditions (such as PlayTennis = yes) can be represented in a similar fashion. Thus, an entire \\nrule can be described by concatenating the bit strings describing the rule precondition s, together with \\nthe bit string describing the rule postcondition. For example, the rule  \\n \\nIF Wind = Strong THEN PlayTennis = yes \\n \\nwould be represented by the string  \\n \\n    Outlook     Wind     PlayTennis  \\n111      10     10',\n",
       " 'IF Wind = Strong THEN PlayTennis = yes \\n \\nwould be represented by the string  \\n \\n    Outlook     Wind     PlayTennis  \\n111      10     10 \\n \\nwhere the first three bits describe the \"don\\'t care\" constraint on Outlook, the next two bits describe the \\nconstraint on Wind, and the final two bits describe the rule postcondition (here we assume PlayTennis 103 \\n can take on the values Yes or No). Note the bit  string representing the rule contains a substring for each \\nattribute in the hypothesis space, even if that attribute is not constrained by the rule preconditions. This \\nyields a fixed length bit -string representation for rules, in which substrings at speci fic locations describe \\nconstraints on specific attributes. Given this representation for single rules, we can represent sets of',\n",
       " 'constraints on specific attributes. Given this representation for single rules, we can represent sets of \\nrules by similarly concatenating the bit string representations of the individual rules.  \\n \\n \\nIn designing a bit string encoding fo r some hypothesis space, it is useful to arrange for every \\nsyntactically legal bit string to represent a well -defined hypothesis. To illustrate, note in the rule \\nencoding in the above paragraph the bit string 11 1 10 11 represents a rule whose postconditio n does \\nnot constrain the target attribute PlayTennis. If we wish to avoid considering this hypothesis, we may \\nemploy a different encoding (e.g., allocate just one bit to the PlayTennis postcondition to indicate',\n",
       " 'employ a different encoding (e.g., allocate just one bit to the PlayTennis postcondition to indicate \\nwhether the value is Yes or No), alter the ge netic operators so that they explicitly avoid constructing \\nsuch bit strings, or simply assign a very  low fitness to such bit strings.  \\n \\nIn some GAS, hypotheses are represented by symbolic descriptions rather than bit strings.  \\n \\nGenetic Operators  \\nThe generat ion of successors in a GA is determined by a set of operators that recombine and \\nmutate selected members of the current population. These operators correspond to idealized versions \\nof the genetic operations found in biological evolution. The two most commo n operators are crossover \\nand mutation.',\n",
       " 'of the genetic operations found in biological evolution. The two most commo n operators are crossover \\nand mutation.  \\nThe crossover operator produces two new offspring from two parent strings, by copying \\nselected bits from each parent. The bit at position i in each offspring is copied from the bit at position i \\nin one of the two par ents. The choice of which parent contributes the bit for position i is determined by \\nan additional string called the crossover mask. To illustrate, consider the single -point crossover \\noperator at the top of Table Consider the topmost of the two offspring i n this case. This offspring takes \\nits first five bits from the first parent and its remaining six bits from the second parent, because the',\n",
       " 'its first five bits from the first parent and its remaining six bits from the second parent, because the \\ncrossover mask 11 11 1000000 specifies these choices for each of the bit positions. The second offspring \\nuses the sam e crossover mask, but switches the roles of the two parents. Therefore, it contains the bits \\nthat were not used by the first offspring. In single -point crossover, the crossover mask is always \\nconstructed so that it begins with a string containing n contigu ous Is, followed by the necessary number \\nof 0s to complete the string. This results in offspring in which the first n bits are contributed by one \\nparent and the remaining bits by the second parent. Each time the single -point crossover operator is',\n",
       " 'parent and the remaining bits by the second parent. Each time the single -point crossover operator is \\napplied t he crossover point n is chosen at random, and the crossover mask is then created and applied.  \\n  104 \\n  \\n \\n \\n \\n \\n \\n \\nIn two-point crossover, offspring are created by substituting intermediate segments of one parent into \\nthe middle of the second parent string. Put another way, the crossover mask is a string beginning with \\nno zeros, followed by a contiguous string of nl ones, followed by the nece ssary number of zeros to \\ncomplete the string. Each time the two -point crossover operator is applied, a mask is generated by \\nrandomly choosing the integers no and nl.  \\n \\nFitness Function and Selection',\n",
       " 'complete the string. Each time the two -point crossover operator is applied, a mask is generated by \\nrandomly choosing the integers no and nl.  \\n \\nFitness Function and Selection  \\nThe fitness function defines the criterion for ranking po tential hypotheses and for \\nprobabilistically selecting them for inclusion in the next generation population. If the task is to learn \\nclassification rules, then the fitness function typically has a component that scores the classification \\naccuracy of the ru le over a set of provided training examples. Often other criteria may be included as \\nwell, such as the complexity or generality of the rule. More generally, when the bit -string hypothesis is',\n",
       " 'well, such as the complexity or generality of the rule. More generally, when the bit -string hypothesis is \\ninterpreted as a complex procedure (e.g., when the bit string rep resents a collection of if -then rules that \\nwill be chained together to control a robotic device), the fitness function may measure the overall \\nperformance of the resulting procedure rather than performance of individual rules.  \\nIn our prototypical GA shown in above Table , the probability that a hypothesis will be selected is given \\nby the ratio of its fitness to the fitness of other members of the current population as seen in Equation \\nabove . This method is sometimes called fitness proportionate selection, or roulette wheel selection.',\n",
       " 'above . This method is sometimes called fitness proportionate selection, or roulette wheel selection. \\nOther methods for using fitness to select hypotheses have also been proposed. For example, in \\ntournament selection, two hypotheses are first chosen at random from the current population. With \\nsome predefined probability p the m ore fit of these two is then selected, and with probability (1 - p) the \\nless fit hypothesis is selected. Tournament selection often yields a more diverse population than fitness \\nproportionate selection. In another method called rank selection, the hypothes es in the current \\npopulation are first sorted by fitness. The probability that a hypothesis will be selected is then \\nproportional to its rank in this sorted list, rather than its fitness.  \\n105',\n",
       " 'population are first sorted by fitness. The probability that a hypothesis will be selected is then \\nproportional to its rank in this sorted list, rather than its fitness.  \\n105 \\n  \\n5.3. An Illustrative Example  \\nA genetic algorithm can be viewed as a  general optimization method that searches a large space \\nof candidate objects seeking one that performs best according to the fitness function. Although not \\nguaranteed to find an optimal object, GAS often succeed in finding an object with high fitness. GAS  have \\nbeen applied to a number of optimization problems outside machine learning, including problems such \\nas circuit layout and job -shop scheduling. Within machine learning, they have been applied both to',\n",
       " 'as circuit layout and job -shop scheduling. Within machine learning, they have been applied both to \\nfunction -approximation problems and to tasks such a s choosing the network topology for artificial \\nneural network learning systems.  \\nTo illustrate the use of GAS for concept learning, we briefly summarize the GABIL system \\ndescribed by DeJong et al. (1993). GABIL uses a GA to learn boolean concepts represente d by a \\ndisjunctive set of propositional rules. In experiments over several concept learning problems, GABIL was \\nfound to be roughly comparable in generalization accuracy to other learning algorithms such as the \\ndecision tree learning algorithm C4.5 and the  rule learning system AQ14. The learning tasks in this study',\n",
       " \"decision tree learning algorithm C4.5 and the  rule learning system AQ14. The learning tasks in this study \\nincluded both artificial learning tasks designed to explore the systems' generalization accuracy and the \\nreal world problem of breast cancer diagnosis.  \\nThe specific instantiation of the GA algori thm in GABIL can be summarized as follows:  \\nRepresentation. Each hypothesis in GABIL corresponds to a disjunctive set of propositional rules, \\nencoded as described in Section 9.2.1. In particular, the hypothesis space of rule preconditions consists \\nof a conj unction of constraints on a fixed set of attributes, as described in that earlier section. To \\nrepresent a set of rules, the bit -string representations of individual rules are concatenated. To illustrate,\",\n",
       " 'represent a set of rules, the bit -string representations of individual rules are concatenated. To illustrate, \\nconsider a hypothesis space in which rule preconditi ons are conjunctions of constraints over two \\nBoolean attributes, a1 and a2.The rule postcondition is described by a single bit that indicates the \\npredicted  value of the target attribute c. Thus, the hypothesis consisting of the two rules  \\n \\nIF a l=T^a 2=F THEN  c=T; IF a 2=T THEN c=F  \\n \\n \\nwould be represented by the string  \\n \\na1   a2   c   a1   a2   c \\n10  01  1  11  01 0  \\n \\nNote the length of the bit string grows with the number of rules in the hypothesis. This variable bit -\\nstring length requires a slight modification to  the crossover operator, as described below.',\n",
       " 'Note the length of the bit string grows with the number of rules in the hypothesis. This variable bit -\\nstring length requires a slight modification to  the crossover operator, as described below.  \\n \\nGenetic operators. GABIL uses the standard mutation operator of  above  Table in which a single \\nbit is chosen at random and replaced by its complement. The crossover operator that it uses is a fairly \\nstandard extension to the two -point crossover operator described in Table 9.2. In particular, to \\naccommodate the variable -lengt h bit strings that encode rule sets, and to constrain the system so that \\ncrossover occurs only between like sections of the bit strings that encode rules, the following approach',\n",
       " 'crossover occurs only between like sections of the bit strings that encode rules, the following approach \\nis taken. To perform a crossover operation on two parents, two crossover point s are first chosen at \\nrandom in the first parent string. Let dl (dz) denote the distance from the leftmost (rightmost) of these \\ntwo crossover points to the rule boundary immediately to its left. The crossover points in the second \\nparent are now randomly ch osen, subject to the constraint that they must have the same dl and d2 \\nvalue. For example, if the two parent strings are  106 \\n  \\n \\nand the crossover points chosen for the first parent are the points following bit positions 1 and 8,  \\n \\n \\n \\nwhere \"[\" and \"1\" indicate crossover points, then dl = 1 and dz = 3. Hence the allowed pairs of crossover',\n",
       " 'where \"[\" and \"1\" indicate crossover points, then dl = 1 and dz = 3. Hence the allowed pairs of crossover \\npoints for the second parent include the pairs of bit positions (1,3), (1,8), and (6,8). If the pair (1,3) \\nhappens to  be chosen,  \\n \\n \\n \\nthen the two resulting offspring will be  \\n \\n \\n \\nAs this example illustrates, this crossover operation enables offspring to contain a different number of \\nrules than their parents, while assuring that all bit strings generated in this fashion represent well -\\ndefined rule sets.  \\nFitness function. The fitne ss of each hypothesized rule set is based on its classification accuracy over \\nthe training data. In particular, the function used to measure fitness is',\n",
       " 'Fitness function. The fitne ss of each hypothesized rule set is based on its classification accuracy over \\nthe training data. In particular, the function used to measure fitness is  \\n \\n \\nwhere correct (h) is the percent of all training examples correctly classified by hypothesis h. \\n \\nIn experiments comparing the behavior of GABIL to decision tree learning algorithms such as \\nC4.5 and ID5R, and to the rule learning algorithm AQ14report roughly comparable performance among \\nthese systems, tested on a variety of learning problems. For example,  over a set of 12 synthetic \\nproblems, GABIL achieved an average generalization accuracy of 92.1 %, whereas the performance of \\nthe other systems ranged from 91.2 % to 96.6 %.  \\n \\nExtensions  \\n107',\n",
       " 'problems, GABIL achieved an average generalization accuracy of 92.1 %, whereas the performance of \\nthe other systems ranged from 91.2 % to 96.6 %.  \\n \\nExtensions  \\n107 \\n In one set of experiments they explored the addition of two new geneti c operators that were \\nmotivated by the generalization operators common in many symbolic learning methods. The first of \\nthese operators,  AddAlternative, generalizes the constraint on a specific attribute by changing a 0 to a \\n1 in the substring correspondin g to the attribute. For example, if the constraint on an attribute is \\nrepresented by the string 10010, this operator might change it to 101 10. This operator was applied \\nwith probability .O1 to selected members of the population on each generation. The sec ond operator,',\n",
       " 'with probability .O1 to selected members of the population on each generation. The sec ond operator, \\nDropcondition performs a more drastic generalization step, by replacing all bits for a particular attribute \\nby a 1. This operator corresponds to generalizing the rule by completely dropping the constraint on the \\nattribute, and was applied on each generation with probability .60. The authors report this revised \\nsystem achieved an average performance of 95.2% over the above set of synthetic learning tasks, \\ncompared to 92.1% for the basic GA algorithm.  \\n \\nIn the above experiment, the two new operat ors were applied with the same probability to each \\nhypothesis in the population on each generation. In a second experiment, the bit -string representation',\n",
       " 'In the above experiment, the two new operat ors were applied with the same probability to each \\nhypothesis in the population on each generation. In a second experiment, the bit -string representation \\nfor hypotheses was extended to include two bits that determine which of these operators may be \\napplied  to the hypothesis. In this extended representation, the bit string for a typical rule set hypothesis \\nwould be  \\n \\n \\n \\nwhere the final two bits indicate in this case that the AddAlternative operator may be applied to this bit \\nstring, but that the Dropcondition operator may not. These two new bits define part of the search \\nstrategy used by the GA and are themselves altered and evolved using the same crossover and mutation',\n",
       " 'string, but that the Dropcondition operator may not. These two new bits define part of the search \\nstrategy used by the GA and are themselves altered and evolved using the same crossover and mutation \\noperators that operate on other bits in the string. While the authors report m ixed results with this \\napproach (i.e., improved performance on some problems, decreased performance on others), it \\nprovides an interesting illustration of how GAS might in principle be used to evolve their own \\nhypothesis search methods.  \\n \\n5.4 Hypothesis Spa ce Search  \\nAs illustrated above, GAS employ a randomized beam search method to seek a maximally fit hypothesis. \\nThis search is quite different from that of other learning methods we have considered in this book. To',\n",
       " 'This search is quite different from that of other learning methods we have considered in this book. To \\ncontrast the hypothesis space search of GA S with that of neural network BACKPROPAGATION, for \\nexample, the radiant descent search in BACKPROPAGATION moves smoothly from one hypothesis to a \\nnew hypothesis that is very similar. In contrast, the GA search can move much more abruptly, replacing \\na paren t hypothesis by an offspring that may be radically different from the parent. Note the GA search \\nis therefore less likely to fall into the same kind of local minima that can plague gradient descent \\nmethods.  \\nOne practical difficulty in some GA applications  is the problem of crowding. Crowding is a phenomenon',\n",
       " 'methods.  \\nOne practical difficulty in some GA applications  is the problem of crowding. Crowding is a phenomenon \\nin which some individual that is more highly fit than others in the population quickly reproduces, so that \\ncopies of this individual and very similar individuals take over a large fraction of the popula tion. The \\nnegative impact of crowding is that it reduces the diversity of the population, thereby slowing further \\nprogress by the GA. Several strategies have been explored for reducing crowding. One approach is to \\nalter the selection function, using criter ia such as tournament selection or rank selection in place of \\nfitness proportionate roulette wheel selection. A related strategy is \"fitness sharing,\" in which the',\n",
       " 'fitness proportionate roulette wheel selection. A related strategy is \"fitness sharing,\" in which the \\nmeasured fitness of an individual is reduced by the presence of other, similar individuals i n the \\npopulation. A third approach is to restrict the kinds of individuals allowed to recombine to form \\noffspring. For example, by allowing only the most similar individuals to recombine, we can encourage \\n108 \\n the formation of clusters of similar individuals, o r multiple \"subspecies\" within the population. A related \\napproach is to spatially distribute individuals and allow only nearby individuals to recombine. Many of \\nthese techniques are inspired by the analogy to biological evolution.  \\n \\nPopulation Evolution and  the Schema Theorem',\n",
       " 'these techniques are inspired by the analogy to biological evolution.  \\n \\nPopulation Evolution and  the Schema Theorem  \\nIt is interesting to ask whether one can mathematically characterize the evolution over time of the \\npopulation within a GA. The schema theorem provides one such characterization. It is based on the \\nconcept of schemas, or patterns that d escribe sets of bit strings. To be precise, a schema is any string \\ncomposed of 0s, 1s, and *\\'s. Each schema represents the set of bit strings containing the indicated 0s \\nand 1s, with each “*” interpreted as a \"don\\'t care.\" For example, the schema 0*10 repr esents the set of \\nbit strings that includes exactly 0010 and 01 10.  \\nAn individual bit string can be viewed as a representative of each of the different schemas that it',\n",
       " 'bit strings that includes exactly 0010 and 01 10.  \\nAn individual bit string can be viewed as a representative of each of the different schemas that it \\nmatches. For example, the bit string 0010 can be thought of as a representative of 24 distinct schemas \\nincluding 00**, 0* 10, ****, etc. Similarly, a population of bit strings can be viewed in terms of the set \\nof schemas that it represents and the number of individuals associated with each of these schema.  \\nThe schema theorem characterizes th e evolution of the population within a GA in terms of the number \\nof instances representing each schema. Let m(s, t) denote the number of instances of schema s in the \\npopulation at time t (i.e., during the tth generation). The schema theorem describes the e xpected value',\n",
       " 'population at time t (i.e., during the tth generation). The schema theorem describes the e xpected value \\nof m(s,t+1 ) in terms of m(s, t) and other properties of the schema, population, and GA algorithm \\nparameters.  \\nThe evolution of the population in the GA depends on the selection step, the recombination step, and \\nthe mutation step. Let us start by considering just the effect of the selection step. Let f (h) denote the \\nfitness of the individual bit string h and \\n\\uf028\\uf029tf\\uf04b denote the average fitness of all individuals in the \\npopulation at time t. Let n be the total number of individuals in the population. Let \\nptsh\\uf0c7\\uf065 , indicate \\nthat the individual h is both a representative of schema s and a me mber of the population at time t. \\nFinally, let',\n",
       " 'ptsh\\uf0c7\\uf065 , indicate \\nthat the individual h is both a representative of schema s and a me mber of the population at time t. \\nFinally, let \\n\\uf028\\uf029tsu,\\uf0d9 denote the average fitness of instances of schema s in the population at time t.  \\nWe are interested in calculating the expected value of m(s,t+1 ), which we denote E[m(s,t+1 )]. We can \\ncalculate E[m(s,t+1 )] using the probability distribution for selection given in Equation, which can be \\nrestated using our current terminology as follows  \\n \\nNow if we select one member for the new population according to this probability distributio n, then the \\nprobability that we will select a representative of schema s is \\n  \\n109 \\n  \\nThe second step above follows from the fact that by definition,',\n",
       " 'probability that we will select a representative of schema s is \\n  \\n109 \\n  \\nThe second step above follows from the fact that by definition,  \\n \\n \\n \\nEquation  gives the probability that a single hypothesis selected by the GA will be an instance of schema \\ns. Therefore, the expected number of instances of s resulting from the n independent selection steps \\nthat create the entire new generation is just n times this probability.  \\n \\n \\n \\nEquation states that the expected number of instances of schema s at generation t+ 1 is proportional to \\nthe average fitness \\n\\uf028\\uf029tsu,\\uf0d9  of instances of this schema at time t , and inversely proportional to the \\naverage fitness \\n\\uf028\\uf029tf\\uf04b  of all members of the population at time t. Thus, we can expect schemas with',\n",
       " '\\uf028\\uf029tsu,\\uf0d9  of instances of this schema at time t , and inversely proportional to the \\naverage fitness \\n\\uf028\\uf029tf\\uf04b  of all members of the population at time t. Thus, we can expect schemas with \\nabove average fitness to be represented with increasing frequency on successive generations. If we \\nview the GA as performing a virtual parallel search through the space of possibl e schemas at the same \\ntime it performs its explicit parallel search through the space of individuals, then Equation indicates that \\nmore fit schemas will grow in influence over time.  \\nWhile the above analysis considered only the selection step of the GA, the crossover and mutation \\nsteps must be considered as well. The schema theorem considers only the possible negative influence',\n",
       " 'While the above analysis considered only the selection step of the GA, the crossover and mutation \\nsteps must be considered as well. The schema theorem considers only the possible negative influence \\nof these genetic operators (e.g., random mutation may decrease the number of representatives of s, \\nindependent of \\n\\uf028\\uf029tsu,\\uf0d9  and considers only the case of single -point crossover. The full schema theorem \\nthus provides a lower bound on the expected frequency of schema s, as follows:  \\n \\n \\nHere, pc is the probability that the single -point crossover operator will be applied  \\nto an arbitrary individual, and pm, is the probability that an arbitrary bit of an arbitrary individual will be \\nmutated by the mutation operator. o(s) is the number of defined bits  in schema s, where 0 and 1 are',\n",
       " 'mutated by the mutation operator. o(s) is the number of defined bits  in schema s, where 0 and 1 are \\ndefined bits, but * is not. d(s) is the distan ce between the leftmost and rightmost defined bits in s. \\nFinally, l is the length of the individual bit strings in the population. Notice the leftmost term in Equation \\nis identical to the term from Equation and describes the effect of the selection step. T he middle term \\ndescribes the effect of the single -point crossover operator -in particular, it describes the probability that \\nan arbitrary individual representing s will still represent s following application of this crossover \\noperator. The rightmost term d escribes the probability that an arbitrary individual representing schema',\n",
       " \"operator. The rightmost term d escribes the probability that an arbitrary individual representing schema \\ns will still represent schema s following application of the mutation operator. Note that the effects of \\nsingle -point crossover and mutation increase with the number of defined bits o(s) in the schema and \\nwith the distance d(s) between the defined bits. Thus, the schema theorem can be roughly interpreted \\nas stating that more fit schemas will tend to grow in influence, especially schemas containing a small \\nnumber of defined bits (i.e.,  containing a large number of *'s), and especially when these defined bits \\n110 \\n are near one another within the bit string. The schema theorem is perhaps the most widely cited\",\n",
       " '110 \\n are near one another within the bit string. The schema theorem is perhaps the most widely cited \\ncharacterization of population evolution within a GA. One way in which it is incomple te is that it fails to \\nconsider the (presumably) positive effects of crossover and mutation. Numerous more recent \\ntheoretical analyses have been proposed, including analyses based on Markov chain models and on \\nstatistical mechanics models.  \\n \\n5.5 GENETIC PRO GRAMMING  \\nGenetic programming (GP) is a form of evolutionary computation in which the individuals in the \\nevolving population are computer programs rather than bit strings. The basic genetic programming \\napproach and presents a broad range of simple programs that can be successfully learned by GP.  \\n \\nRepresenting Programs',\n",
       " 'approach and presents a broad range of simple programs that can be successfully learned by GP.  \\n \\nRepresenting Programs  \\nPrograms manipulated by a GP are typically represented by trees corresponding to the parse tree of the \\nprogram. Each function call is represented by a node in the tree, and the arguments to the functio n are \\ngiven by its descendant nodes. For example, below Figure illustrates this tree representation for the \\nfunction sin(x) +\\n\\uf028\\uf029yx\\uf02b2 . To apply genetic programming to a particular domain, the user must define \\nthe primitive functions to be con sidered (e.g., sin, cos, \\n , +, -, exponential~), as well as the terminals \\n(e.g., x, y, constants such as 2). The genetic programming algorithm then uses an evolutionary search to',\n",
       " ', +, -, exponential~), as well as the terminals \\n(e.g., x, y, constants such as 2). The genetic programming algorithm then uses an evolutionary search to \\nexplore the vast space of programs that can be described  using these primitives. As in a genetic \\nalgorithm, the prototypical genetic programming algorithm maintains a population of individuals (in this \\ncase, program trees). On each iteration, it produces a new generation of individuals using selection, \\ncrossove r, and mutation. The fitness of a given individual program in the population is typically \\ndetermined by executing the program on a set of training data. Crossover operations are performed by \\nreplacing a randomly chosen subtree of one parent program by a su btree from the other parent \\nprogram.  \\n \\n111',\n",
       " 'replacing a randomly chosen subtree of one parent program by a su btree from the other parent \\nprogram.  \\n \\n111 \\n  \\n \\nAbove Figure  illustrates  a typical crossover operation. It describes a set of experiments applying \\na GP to a number of applications. In his experiments, 10% of the current population, selected \\nprobabilistically according to fitness, is retained unchanged in the next generation. T he remainder of \\nthe new generation is created by applying crossover to pairs of programs from the current generation, \\nagain selected probabilistically according to their fitness. The mutation operator was not used in this \\nparticular set of experiments.  \\n \\nIllustrative Example  \\nOne illustrative example presented by Koza (1992) involves learning an algorithm for stacking the blocks',\n",
       " 'particular set of experiments.  \\n \\nIllustrative Example  \\nOne illustrative example presented by Koza (1992) involves learning an algorithm for stacking the blocks \\nshown in below Figure The task is to develop a general algorithm for stacking the blocks into a single \\nstack that spells the word \"universal,\" independent of the initial configuration of blocks in the world. \\nThe actions available for manipulating blocks allow moving only a single block at a time. In particular, \\nthe top block on the stack can be moved to the table surface, or a block on the table surface can be \\nmoved to the top of the stack.  \\n112 \\n  \\n \\nAs in most GP applications, the choice of problem representation has a significant impact on the',\n",
       " 'moved to the top of the stack.  \\n112 \\n  \\n \\nAs in most GP applications, the choice of problem representation has a significant impact on the \\nease of solving the problem. In Koza\\'s formulation, the primitive functions used to compose prog rams \\nfor this task include the following three terminal arguments:  \\n\\uf0b7 CS (current stack), which refers to the name of the top block on the stack, or F if there is no \\ncurrent stack.  \\n\\uf0b7 TB (top correct block), which refers to the name of the topmost block on the s tack, such that it \\nand those blocks beneath it are in the correct order.  \\n\\uf0b7 NN (next necessary), which refers to the name of the next block needed above TB in the stack, \\nin order to spell the word \"universal,\" or F if no more blocks are needed.',\n",
       " '\\uf0b7 NN (next necessary), which refers to the name of the next block needed above TB in the stack, \\nin order to spell the word \"universal,\" or F if no more blocks are needed.  \\n \\nAs can be se en, this particular choice of terminal arguments provides a natural representation for \\ndescribing programs for manipulating blocks for this task. Imagine, in contrast, the relative difficulty of \\nthe task if we were to instead define the terminal arguments to be the x and y coordinates of each \\nblock.  \\nIn addition to these terminal arguments, the program language in this application included the \\nfollowing primitive functions:  \\n\\uf0b7 (MS x) (move to stack), if block x is on the table, this operator moves x to the top  of the stack',\n",
       " 'following primitive functions:  \\n\\uf0b7 (MS x) (move to stack), if block x is on the table, this operator moves x to the top  of the stack \\nand returns the value T. Otherwise, it does nothing and returns the value F.  \\n\\uf0b7 (MT x) (move to table), if block x is somewhere in the stack, this moves the block at the top of \\nthe stack to the table and returns the value T. Otherwise, it return s the value F.  \\n\\uf0b7 (EQ x y) (equal), which returns T if x equals y, and returns F otherwise.  \\n\\uf0b7 (NOT x), which returns T if x = F, and returns F if x = T.  \\n\\uf0b7 (DU x y) (do until), which executes the expression x repeatedly until expressiony returns the \\nvalue T.  \\n \\nTo a llow the system to evaluate the fitness of any given program, Koza provided a set of 166 training',\n",
       " 'value T.  \\n \\nTo a llow the system to evaluate the fitness of any given program, Koza provided a set of 166 training \\nexample problems representing a broad variety of initial block configurations, including problems of \\ndiffering degrees of difficulty. The fitness of any given  program was taken to be the number of these \\nexamples solved by the algorithm. The population was initialized to a set of 300 random programs. \\nAfter 10 generations, the system discovered the following program, which solves all 166 problems.  \\n \\n(EQ (DU (MT CS )(NOT CS)) (DU (MS NN)(NOT NN)) )  \\n \\nNotice this program contains a sequence of two DU, or \"Do Until\" statements. The first repeatedly \\n113 \\n moves the current top of the stack onto the table, until the stack becomes empty. The second \"Do',\n",
       " '113 \\n moves the current top of the stack onto the table, until the stack becomes empty. The second \"Do \\nUntil\" statement then repe atedly moves the next necessary block from the table onto the stack. The \\nrole played by the top level EQ expression here is to provide a syntactically legal way to sequence these \\ntwo \"Do Until\" loops.  \\nSomewhat surprisingly, after only a few generations, th is GP was able to discover a program that solves \\nall 166 training problems. Of course the ability of the system to accomplish this depends strongly on the \\nprimitive arguments and functions provided, and on the set of training example cases used to evaluate  \\nfitness.  \\n \\nRemarks on Genetic Programming  \\nAs illustrated in the above example, genetic programming extends genetic algorithms to the',\n",
       " \"fitness.  \\n \\nRemarks on Genetic Programming  \\nAs illustrated in the above example, genetic programming extends genetic algorithms to the \\nevolution of complete computer programs. Despite the huge size of the hypothesis space it must \\nsearch, genetic programming has been demonstrated to produce intriguin g results in a number of \\napplications. A comparison of GP to other methods for searching through the space of computer \\nprograms, such as hillclimbing and simulated annealing, is given by O'Reilly and Oppacher (1994).  \\nWhile the above example of GP search is  fairly simple, Koza et al. (1996) summarize the use of a GP in \\nseveral more complex tasks such as designing electronic filter circuits and classifying segments of\",\n",
       " 'several more complex tasks such as designing electronic filter circuits and classifying segments of \\nprotein molecules. The filter circuit design problem provides an example of a considerably m ore \\ncomplex problem. Here, programs are evolved that transform a simple fixed seed circuit into a final \\ncircuit design. The primitive functions used by the GP to construct its programs are functions that edit \\nthe seed circuit by inserting or deleting circu it components and wiring connections. The fitness of each \\nprogram is calculated by simulating the circuit it outputs (using the SPICE circuit simulator) to determine \\nhow closely this circuit meets the design specifications for the desired filter. More prec isely, the fitness',\n",
       " 'how closely this circuit meets the design specifications for the desired filter. More prec isely, the fitness \\nscore is the sum of the magnitudes of errors between the desired and actual circuit output at 101 \\ndifferent input frequencies. In this case, a population of size 640,000 was maintained, with selection \\nproducing 10% of the successor popul ation, crossover producing 89%, and mutation producing 1%. The \\nsystem was executed on a 64 -node parallel processor. Within the first randomly generated population, \\nthe circuits produced were so unreasonable that the SPICE simulator could not even simulate the \\nbehavior of 98% of the circuits. The percentage of unsimulatable circuits dropped to 84.9% following',\n",
       " 'the circuits produced were so unreasonable that the SPICE simulator could not even simulate the \\nbehavior of 98% of the circuits. The percentage of unsimulatable circuits dropped to 84.9% following \\nthe first generation, to 75.0% following the second generation, and to an average of 9.6% over \\nsucceeding generations. The fitness score of the best ci rcuit in the initial population was 159, compared \\nto a score of 39 after 20 generations and a score of 0.8 after 137 generations. The best circuit, produced \\nafter 137 generations, exhibited performance very similar to the desired behavior.  \\nIn most cases, t he performance of genetic programming depends crucially on the choice of \\nrepresentation and on the choice of fitness function. For this reason, an active area of current research',\n",
       " 'In most cases, t he performance of genetic programming depends crucially on the choice of \\nrepresentation and on the choice of fitness function. For this reason, an active area of current research \\nis aimed at the automatic discovery and incorporation of subroutines that imp rove on the original set of \\nprimitive functions, thereby allowing the system to dynamically alter the primitives from which it \\nconstructs individuals. See, for example, Koza (1994).  \\n \\n5.6 Models of Evolution and Learning  \\nIn many natural systems, individual organisms learn to adapt significantly during their lifetime. \\nAt the same time, biological and social processes allow their species to adapt over a time frame of many',\n",
       " 'At the same time, biological and social processes allow their species to adapt over a time frame of many \\ngenerations. One interesting question regarding evolutionary systems is \"What is the rela tionship \\nbetween learning during the lifetime of a single individual, and the longer time frame species -level \\nlearning afforded by evolution?\\'  \\n \\nLamarckian Evolution  \\nLarnarck was a scientist who, in the late nineteenth century, proposed that evolution over many \\ngenerations was directly influenced by the experiences of individual organisms during their lifetime. In 114 \\n particular, he proposed that experiences of a single organism directly affected the genetic makeup of \\ntheir offspring: If an individual learned du ring its lifetime to avoid some toxic food, it could pass this',\n",
       " \"their offspring: If an individual learned du ring its lifetime to avoid some toxic food, it could pass this \\ntrait on genetically to its offspring, which therefore would not need to learn the trait. This is an \\nattractive conjecture, because it would presumably allow for more efficient evolutionary pro gress than \\na generate -and-test process (like that of GAS and GPs) that ignores the experience gained during an \\nindividual's lifetime. Despite the attractiveness of this theory, current scientific evidence \\noverwhelmingly contradicts Lamarck's model. The cur rently accepted view is that the genetic makeup \\nof an individual is, in fact, unaffected by the lifetime experience of one's biological parents. Despite this\",\n",
       " \"of an individual is, in fact, unaffected by the lifetime experience of one's biological parents. Despite this \\napparent biological fact, recent computer studies have shown that Lamarckian processes can \\nsometim es improve the effectiveness of computerized genetic algorithms (see Grefenstette 1991; \\nAckley and Littman 1994; and Hart and Belew 1995).  \\n \\nBaldwin Effect  \\nAlthough Lamarckian evolution is not an accepted model of biological evolution, other mechanisms \\nhave  been suggested by which individual learning can alter the course of evolution. One such \\nmechanism is called the Baldwin effect, after J. M. Baldwin (1896), who first suggested the idea. The \\nBaldwin effect is based on the following observations:\",\n",
       " 'mechanism is called the Baldwin effect, after J. M. Baldwin (1896), who first suggested the idea. The \\nBaldwin effect is based on the following observations:  \\n\\uf0b7 If a species is evolving in a changing environment, there will be evolutionary pressure to favor \\nindividuals with the capability to learn during their lifetime. For example, if a new predator \\nappears in the environment, then individuals capable of learning  to avoid the predator will be \\nmore successful than individuals who cannot learn. In effect, the ability to learn allows an \\nindividual to perform a small local search during its lifetime to maximize its fitness. In contrast, \\nnonlearning individuals whose f itness is fully determined by their genetic makeup will operate \\nat a relative disadvantage.',\n",
       " 'nonlearning individuals whose f itness is fully determined by their genetic makeup will operate \\nat a relative disadvantage.  \\n\\uf0b7 Those individuals who are able to learn many traits will rely less strongly on their genetic code \\nto \"hard -wire\" traits. As a result, these individuals can support a more diverse gene pool, relying \\non individual learning to overcome the \"missing\" or \"not quite optimized\" traits in the genetic \\ncode. This more diverse gene pool can, in turn, support more rapid evolutionary adaptation. \\nThus, the ability of individuals t o learn can have an indirect accelerating effect on the rate of \\nevolutionary adaptation for the entire population.  \\n \\nTo illustrate, imagine some new change in the environment of some species, such as a new',\n",
       " 'evolutionary adaptation for the entire population.  \\n \\nTo illustrate, imagine some new change in the environment of some species, such as a new \\npredator. Such a change will selectively favor indi viduals capable of learning to avoid the predator. As \\nthe proportion of such self -improving individuals in the population grows, the population will be able to \\nsupport a more diverse gene pool, allowing evolutionary processes (even non -Lamarckian generate -\\nand-test processes) to adapt more rapidly. This accelerated adaptation may in turn enable standard \\nevolutionary processes to more quickly evolve a genetic (nonlearned) trait to avoid the predator (e.g., \\nan instinctive fear of this animal). Thus, the Baldwi n effect provides an indirect mechanism for',\n",
       " 'an instinctive fear of this animal). Thus, the Baldwi n effect provides an indirect mechanism for \\nindividual learning to positively impact the rate of evolutionary progress. By increasing survivability and \\ngenetic diversity of the species, individual learning supports more rapid evolutionary progress, thereby  \\nincreasing the chance that the species will evolve genetic, nonlearned traits that better fit the new \\nenvironment.  \\nThere have been several attempts to develop computational models to study the Baldwin \\neffect. For example, Hinton and Nowlan (1987) experim ented with evolving a population of simple \\nneural networks, in which some network weights were fixed during the individual network \"lifetime,\"',\n",
       " 'effect. For example, Hinton and Nowlan (1987) experim ented with evolving a population of simple \\nneural networks, in which some network weights were fixed during the individual network \"lifetime,\" \\nwhile others were trainable. The genetic makeup of the individual determined which weights were 115 \\n trainable and whi ch were fixed. In their experiments, when no individual learning was allowed, the \\npopulation failed to improve its fitness over time. However, when individual learning was allowed, the \\npopulation quickly improved its fitness. During early generations of ev olution the population contained \\na greater proportion of individuals with many trainable weights. However, as evolution proceeded, the',\n",
       " 'a greater proportion of individuals with many trainable weights. However, as evolution proceeded, the \\nnumber of fixed, correct network weights tended to increase, as the population evolved toward \\ngenetically given weight va lues and toward less dependence on individual learning of weights. \\nAdditional computational studies of the Baldwin effect have been reported by Belew (1990), Harvey \\n(1993), and French and Messinger (1994). An excellent overview of this topic can be found i n Mitchell \\n(1996). A special issue of the journal Evolutionary Computation on this topic (Turney et al. 1997) \\ncontains several articles on the Baldwin effect.  \\n \\n5.7 Parallelizing Genetic Algorithms  \\nGAS are naturally suited to parallel implementation, and a number of approaches to',\n",
       " 'contains several articles on the Baldwin effect.  \\n \\n5.7 Parallelizing Genetic Algorithms  \\nGAS are naturally suited to parallel implementation, and a number of approaches to \\nparallelization have been explored. Coarse grain approaches to parallelization subdivide the population \\ninto somewhat distinct groups of individuals, called demes. Each deme is assigned to a different \\ncomputational node, and a stand ard GA search is performed at each node. Communication and cross -\\nfertilization between demes occurs on a less frequent basis than within demes. Transfer between \\ndemes occurs by a migration process, in which individuals from one deme are copied or transferr ed to \\nother demes. This process is modeled after the kind of cross -fertilization that might occur between',\n",
       " 'other demes. This process is modeled after the kind of cross -fertilization that might occur between \\nphysically separated subpopulations of biological species. One benefit of such approaches is that it \\nreduces the crowding problem often encountered in  nonparallel GAS, in which the system falls into a \\nlocal optimum due to the early appearance of a genotype that comes to dominate the entire \\npopulation. Examples of coarse -grained parallel GAS are described by Tanese (1989) and by Cohoon et \\nal. (1987).  \\nIn contrast to coarse -grained parallel implementations of GAS, fine -grained implementations \\ntypically assign one processor per individual in the population. Recombination then takes place among',\n",
       " 'In contrast to coarse -grained parallel implementations of GAS, fine -grained implementations \\ntypically assign one processor per individual in the population. Recombination then takes place among \\nneighboring individuals. Several different types of neighborhoods  have been proposed, ranging from \\nplanar grid to torus. Examples of such systems are described by Spiessens and Manderick (1991). An \\nedited collection of papers on parallel GAS is available in Stender (1993).']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embeddings from open ai\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_search = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2ab2813a040>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Unsupervised models do not need training in the traditional sense, as they do not have corresponding output data to learn from. Instead, they use various algorithms such as clustering and association to find patterns and insights in the input data.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How to train ussupervised models?\"\n",
    "docs = document_search.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
